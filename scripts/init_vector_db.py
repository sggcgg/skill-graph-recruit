"""
åˆå§‹åŒ–å‘é‡æ•°æ®åº“
å°†æ¸…æ´—åçš„æ‹›è˜æ•°æ®å‘é‡åŒ–å¹¶å­˜å…¥ChromaDB
"""
import json
import logging
import sys
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.rag.vector_db import VectorDB

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_cleaned_data(project_root: Path) -> list:
    """
    åŠ è½½æ¸…æ´—åçš„æ•°æ®
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
        
    Returns:
        æ‰€æœ‰å²—ä½æ•°æ®åˆ—è¡¨
    """
    all_jobs = []
    
    # æ•°æ®ç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
    data_dir = project_root / 'data' / 'cleaned'
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¸…æ´—åçš„JSONæ–‡ä»¶
    cleaned_files = list(data_dir.glob("boss_*_cleaned.json"))
    
    if not cleaned_files:
        logger.error(f"æœªæ‰¾åˆ°æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶: {data_dir}")
        logger.error("è¯·å…ˆè¿è¡Œæ•°æ®æ¸…æ´—: python src/data_processing/data_cleaner.py")
        return []
    
    logger.info(f"æ‰¾åˆ° {len(cleaned_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    for file_path in cleaned_files:
        logger.info(f"åŠ è½½: {file_path.name}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            jobs = json.load(f)
            all_jobs.extend(jobs)
            logger.info(f"  åŠ è½½äº† {len(jobs)} æ¡æ•°æ®")
    
    logger.info(f"æ€»è®¡åŠ è½½ {len(all_jobs)} æ¡å²—ä½æ•°æ®")
    return all_jobs


def init_vector_database(force_recreate: bool = False):
    """
    åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    
    Args:
        force_recreate: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼ˆæ¸…ç©ºå·²æœ‰æ•°æ®ï¼‰
    """
    print("="*80)
    print("ğŸ“¦ åˆå§‹åŒ–å‘é‡æ•°æ®åº“")
    print("="*80)
    
    # 1. åˆå§‹åŒ–VectorDB
    logger.info("\nã€æ­¥éª¤1: åˆå§‹åŒ–å‘é‡æ•°æ®åº“ã€‘")
    db = VectorDB()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    current_count = db.get_stats()['total_documents']
    logger.info(f"å½“å‰æ•°æ®åº“æ–‡æ¡£æ•°: {current_count}")
    
    if current_count > 0 and not force_recreate:
        logger.info("æ•°æ®åº“å·²æœ‰æ•°æ®")
        user_input = input("\næ˜¯å¦æ¸…ç©ºå¹¶é‡æ–°å¯¼å…¥? (y/n): ").strip().lower()
        if user_input != 'y':
            logger.info("å–æ¶ˆæ“ä½œ")
            return
        
        logger.info("æ¸…ç©ºæ•°æ®åº“...")
        db.clear()
    elif current_count > 0 and force_recreate:
        logger.info("å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼Œæ¸…ç©ºæ•°æ®åº“...")
        db.clear()
    
    # 2. åŠ è½½æ•°æ®
    logger.info("\nã€æ­¥éª¤2: åŠ è½½æ¸…æ´—åçš„æ•°æ®ã€‘")
    jobs = load_cleaned_data(project_root)
    
    if not jobs:
        logger.error("æ²¡æœ‰æ•°æ®å¯å¯¼å…¥")
        return
    
    # 3. å‘é‡åŒ–å¹¶æ·»åŠ åˆ°æ•°æ®åº“
    logger.info("\nã€æ­¥éª¤3: å‘é‡åŒ–å¹¶å¯¼å…¥ã€‘")
    logger.info(f"å¼€å§‹å‘é‡åŒ– {len(jobs)} æ¡æ•°æ®...")
    logger.info(f"è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # æ ¹æ®æ•°æ®é‡ä¼°ç®—æ—¶é—´
    estimated_minutes = len(jobs) / 1000  # å¤§çº¦æ¯1000æ¡éœ€è¦1åˆ†é’Ÿ
    logger.info(f"é¢„è®¡æ—¶é—´: {estimated_minutes:.1f} åˆ†é’Ÿ")
    
    try:
        # æ‰¹é‡æ·»åŠ ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        db.add_jobs(jobs, batch_size=50, show_progress=True)
        
        # 4. éªŒè¯
        logger.info("\nã€æ­¥éª¤4: éªŒè¯ã€‘")
        final_stats = db.get_stats()
        
        print("\n" + "="*80)
        print("âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ–‡æ¡£æ•°: {final_stats['total_documents']:,}")
        print(f"  å‘é‡ç»´åº¦: {final_stats['embedding_dim']}")
        print(f"  æ¨¡å‹: {final_stats['model_name']}")
        print(f"\nğŸ’¾ å­˜å‚¨ä½ç½®: data/vector_db/")
        
        # 5. æµ‹è¯•æœç´¢
        logger.info("\nã€æ­¥éª¤5: æµ‹è¯•æœç´¢ã€‘")
        test_queries = [
            "Pythonåç«¯å¼€å‘",
            "å‰ç«¯Reactå¼€å‘",
            "æ•°æ®åˆ†æå¸ˆ"
        ]
        
        print(f"\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½:")
        for query in test_queries:
            results = db.search(query, top_k=3)
            if results['metadatas'] and results['metadatas'][0]:
                print(f"\næŸ¥è¯¢: {query}")
                for i, meta in enumerate(results['metadatas'][0]):
                    distance = results['distances'][0][i]
                    similarity = 1 / (1 + max(0, distance))   # L2è·ç¦»è½¬ç›¸ä¼¼åº¦
                    print(f"  {i+1}. {meta['title']} | {meta['city']} (ç›¸ä¼¼åº¦: {similarity:.3f})")
        
        print("\n" + "="*80)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼ç°åœ¨å¯ä»¥ä½¿ç”¨RAGæ£€ç´¢åŠŸèƒ½äº†ï¼")
        print("="*80)
        print("\nä¸‹ä¸€æ­¥:")
        print("  - æµ‹è¯•RAGæœåŠ¡: python src/rag/rag_service.py")
        print("  - å¯åŠ¨APIæœåŠ¡: uvicorn src.api.main:app --reload")
        print()
        
    except Exception as e:
        logger.error(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        logger.error("\nå¯èƒ½çš„åŸå› :")
        logger.error("1. å†…å­˜ä¸è¶³ï¼ˆå‘é‡åŒ–éœ€è¦å¤§é‡å†…å­˜ï¼‰")
        logger.error("2. æ¨¡å‹æœªä¸‹è½½ï¼ˆè¯·å…ˆè¿è¡Œ python scripts/download_m3e_model.pyï¼‰")
        logger.error("3. ç£ç›˜ç©ºé—´ä¸è¶³")
        raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆå§‹åŒ–å‘é‡æ•°æ®åº“")
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°åˆ›å»ºï¼ˆæ¸…ç©ºå·²æœ‰æ•°æ®ï¼‰'
    )
    
    args = parser.parse_args()
    
    try:
        init_vector_database(force_recreate=args.force)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\næ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
