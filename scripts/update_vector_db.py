"""
å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“
å°†æ–°æŠ“å–çš„æ•°æ®å¢é‡æ·»åŠ åˆ°ChromaDBï¼ˆä¸æ¸…ç©ºå·²æœ‰æ•°æ®ï¼‰

ä½¿ç”¨åœºæ™¯ï¼š
1. å·²ç»åˆå§‹åŒ–è¿‡å‘é‡æ•°æ®åº“
2. æ–°æŠ“å–äº†æ•°æ®ï¼Œéœ€è¦æ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“
3. ä¸æƒ³é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®

ä¸ init_vector_db.py çš„åŒºåˆ«ï¼š
- init_vector_db.py: æ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºï¼ˆå…¨é‡ï¼‰
- update_vector_db.py: å¢é‡æ·»åŠ æ–°æ•°æ®ï¼ˆå¢é‡ï¼‰
"""
import json
import logging
import sys
from pathlib import Path
from tqdm import tqdm
from typing import List, Set

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.rag.vector_db import VectorDB

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_data_from_dir(data_dir: Path, pattern: str = "boss_*_cleaned.json") -> List[dict]:
    """
    ä»ç›®å½•åŠ è½½æ•°æ®
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        
    Returns:
        å²—ä½æ•°æ®åˆ—è¡¨
    """
    all_jobs = []
    data_files = list(data_dir.glob(pattern))
    
    if not data_files:
        logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_dir}/{pattern}")
        return []
    
    logger.info(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    for file_path in data_files:
        logger.info(f"åŠ è½½: {file_path.name}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            jobs = json.load(f)
            all_jobs.extend(jobs)
            logger.info(f"  åŠ è½½äº† {len(jobs)} æ¡æ•°æ®")
    
    logger.info(f"æ€»è®¡åŠ è½½ {len(all_jobs)} æ¡å²—ä½æ•°æ®")
    return all_jobs


def get_existing_job_ids(db: VectorDB) -> Set[str]:
    """
    è·å–æ•°æ®åº“ä¸­å·²æœ‰çš„ job_id
    
    Args:
        db: VectorDB å®ä¾‹
        
    Returns:
        å·²æœ‰çš„ job_id é›†åˆ
    """
    try:
        all_data = db.collection.get()
        existing_ids = set(all_data['ids'])
        logger.info(f"æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_ids)} æ¡æ•°æ®")
        return existing_ids
    except Exception as e:
        logger.error(f"è·å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
        return set()


def filter_new_jobs(jobs: List[dict], existing_ids: Set[str]) -> List[dict]:
    """
    è¿‡æ»¤å‡ºæ–°æ•°æ®ï¼ˆä¸åœ¨æ•°æ®åº“ä¸­çš„ï¼‰
    
    Args:
        jobs: æ‰€æœ‰å²—ä½æ•°æ®
        existing_ids: å·²æœ‰çš„ job_id é›†åˆ
        
    Returns:
        æ–°çš„å²—ä½æ•°æ®åˆ—è¡¨
    """
    new_jobs = []
    duplicate_count = 0
    
    for job in jobs:
        job_id = job.get('job_id')
        if not job_id:
            logger.warning("å‘ç°æ²¡æœ‰job_idçš„æ•°æ®ï¼Œè·³è¿‡")
            continue
        
        if job_id in existing_ids:
            duplicate_count += 1
        else:
            new_jobs.append(job)
    
    logger.info(f"æ€»æ•°æ®: {len(jobs)} æ¡")
    logger.info(f"å·²å­˜åœ¨: {duplicate_count} æ¡")
    logger.info(f"æ–°æ•°æ®: {len(new_jobs)} æ¡")
    
    return new_jobs


def update_vector_database(
    data_source: str = 'cleaned',
    skip_duplicates: bool = True,
    force_update: bool = False
):
    """
    å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“
    
    Args:
        data_source: æ•°æ®æºç±»å‹ ('cleaned' æˆ– 'enhanced')
        skip_duplicates: æ˜¯å¦è·³è¿‡é‡å¤æ•°æ®ï¼ˆTrue=åªæ·»åŠ æ–°æ•°æ®ï¼ŒFalse=æ›´æ–°æ‰€æœ‰æ•°æ®ï¼‰
        force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°å·²æœ‰æ•°æ®
    """
    print("="*80)
    print("ğŸ“¦ å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“")
    print("="*80)
    print()
    
    # 1. åˆå§‹åŒ–VectorDB
    logger.info("ã€æ­¥éª¤1: è¿æ¥å‘é‡æ•°æ®åº“ã€‘")
    db = VectorDB()
    
    # æ£€æŸ¥å½“å‰æ•°æ®é‡
    current_stats = db.get_stats()
    current_count = current_stats['total_documents']
    logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
    logger.info(f"  å½“å‰æ–‡æ¡£æ•°: {current_count:,}")
    logger.info(f"  æ¨¡å‹: {current_stats['model_name']}")
    
    if current_count == 0:
        logger.warning("\nâš ï¸  æ•°æ®åº“ä¸ºç©ºï¼")
        logger.warning("å»ºè®®ä½¿ç”¨ init_vector_db.py è¿›è¡Œé¦–æ¬¡åˆå§‹åŒ–")
        
        user_input = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        if user_input != 'y':
            logger.info("å–æ¶ˆæ“ä½œ")
            return
    
    # 2. åŠ è½½æ•°æ®
    logger.info("\nã€æ­¥éª¤2: åŠ è½½æ•°æ®ã€‘")
    
    if data_source == 'enhanced':
        data_dir = project_root / 'data' / 'enhanced'
        pattern = 'boss_*_enhanced.json'
        logger.info("æ•°æ®æº: LLMå¢å¼ºæ•°æ®")
    else:
        data_dir = project_root / 'data' / 'cleaned'
        pattern = 'boss_*_cleaned.json'
        logger.info("æ•°æ®æº: æ¸…æ´—æ•°æ®")
    
    jobs = load_data_from_dir(data_dir, pattern)
    
    if not jobs:
        logger.error("æ²¡æœ‰æ•°æ®å¯å¯¼å…¥")
        return
    
    # 3. è¿‡æ»¤é‡å¤æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    logger.info("\nã€æ­¥éª¤3: æ£€æŸ¥é‡å¤ã€‘")
    
    if skip_duplicates and not force_update:
        existing_ids = get_existing_job_ids(db)
        new_jobs = filter_new_jobs(jobs, existing_ids)
        
        if not new_jobs:
            print("\n" + "="*80)
            print("âœ… æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ·»åŠ ")
            print("="*80)
            print(f"\næ•°æ®åº“ä¸­å·²æœ‰å…¨éƒ¨ {len(jobs)} æ¡æ•°æ®")
            print("\nå¦‚æœéœ€è¦æ›´æ–°å·²æœ‰æ•°æ®ï¼Œè¯·ä½¿ç”¨:")
            print("  python scripts/update_vector_db.py --force-update")
            return
        
        jobs_to_add = new_jobs
        logger.info(f"å°†æ·»åŠ  {len(jobs_to_add)} æ¡æ–°æ•°æ®")
    else:
        jobs_to_add = jobs
        logger.info(f"å°†æ·»åŠ /æ›´æ–° {len(jobs_to_add)} æ¡æ•°æ®")
    
    # 4. è¯¢é—®ç¡®è®¤
    print(f"\nå‡†å¤‡æ·»åŠ  {len(jobs_to_add)} æ¡æ•°æ®åˆ°å‘é‡æ•°æ®åº“")
    
    # ä¼°ç®—æ—¶é—´å’Œç©ºé—´
    estimated_minutes = len(jobs_to_add) / 1000  # çº¦1000æ¡/åˆ†é’Ÿ
    estimated_size_mb = len(jobs_to_add) * 0.01  # çº¦10KB/æ¡
    
    print(f"é¢„è®¡è€—æ—¶: {estimated_minutes:.1f} åˆ†é’Ÿ")
    print(f"é¢„è®¡å ç”¨ç©ºé—´: {estimated_size_mb:.1f} MB")
    
    user_input = input("\nç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
    if user_input != 'y':
        logger.info("å–æ¶ˆæ“ä½œ")
        return
    
    # 5. å‘é‡åŒ–å¹¶æ·»åŠ 
    logger.info("\nã€æ­¥éª¤4: å‘é‡åŒ–å¹¶æ·»åŠ ã€‘")
    logger.info(f"å¼€å§‹å¤„ç† {len(jobs_to_add)} æ¡æ•°æ®...")
    
    try:
        # æ‰¹é‡æ·»åŠ ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        db.add_jobs(jobs_to_add, batch_size=50, show_progress=True)
        
        # 6. éªŒè¯
        logger.info("\nã€æ­¥éª¤5: éªŒè¯ã€‘")
        final_stats = db.get_stats()
        final_count = final_stats['total_documents']
        
        added_count = final_count - current_count
        
        print("\n" + "="*80)
        print("âœ… å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆï¼")
        print("="*80)
        print(f"\nğŸ“Š æ›´æ–°ç»Ÿè®¡:")
        print(f"  æ›´æ–°å‰: {current_count:,} æ¡")
        print(f"  æ›´æ–°å: {final_count:,} æ¡")
        print(f"  æ–°å¢: {added_count:,} æ¡")
        print(f"\nğŸ’¾ å­˜å‚¨ä½ç½®: data/vector_db/")
        
        # 7. æµ‹è¯•æœç´¢
        logger.info("\nã€æ­¥éª¤6: æµ‹è¯•æœç´¢ã€‘")
        test_queries = [
            "Pythonåç«¯å¼€å‘",
            "å‰ç«¯Reactå¼€å‘",
            "æ•°æ®åˆ†æå¸ˆ"
        ]
        
        print(f"\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½:")
        for query in test_queries:
            results = db.search(query, top_k=3)
            if results['metadatas'] and results['metadatas'][0]:
                print(f"\næŸ¥è¯¢: {query}")
                for i, meta in enumerate(results['metadatas'][0]):
                    distance = results['distances'][0][i]
                    similarity = 1 / (1 + max(0, distance))
                    print(f"  {i+1}. {meta['title']} | {meta['city']} (ç›¸ä¼¼åº¦: {similarity:.3f})")
        
        print("\n" + "="*80)
        print("ğŸ‰ æ›´æ–°å®Œæˆï¼")
        print("="*80)
        print("\nä¸‹ä¸€æ­¥:")
        print("  - å¯åŠ¨APIæœåŠ¡: uvicorn src.api.main:app --reload")
        print("  - æµ‹è¯•RAGæ£€ç´¢: python src/rag/rag_service.py")
        print()
        
    except Exception as e:
        logger.error(f"\nâŒ æ›´æ–°å¤±è´¥: {e}")
        logger.error("\nå¯èƒ½çš„åŸå› :")
        logger.error("1. å†…å­˜ä¸è¶³")
        logger.error("2. ç£ç›˜ç©ºé—´ä¸è¶³")
        logger.error("3. æ•°æ®æ ¼å¼é”™è¯¯")
        raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¢é‡æ·»åŠ æ¸…æ´—æ•°æ®ï¼ˆåªæ·»åŠ æ–°æ•°æ®ï¼‰
  python scripts/update_vector_db.py
  
  # å¢é‡æ·»åŠ LLMå¢å¼ºæ•°æ®
  python scripts/update_vector_db.py --source enhanced
  
  # å¼ºåˆ¶æ›´æ–°æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬å·²æœ‰æ•°æ®ï¼‰
  python scripts/update_vector_db.py --force-update
  
  # æ·»åŠ æ‰€æœ‰æ•°æ®ï¼ˆä¸è·³è¿‡é‡å¤ï¼‰
  python scripts/update_vector_db.py --no-skip-duplicates
        """
    )
    
    parser.add_argument(
        '--source',
        choices=['cleaned', 'enhanced'],
        default='cleaned',
        help='æ•°æ®æºç±»å‹ï¼ˆé»˜è®¤ï¼šcleanedï¼‰'
    )
    
    parser.add_argument(
        '--no-skip-duplicates',
        action='store_true',
        help='ä¸è·³è¿‡é‡å¤æ•°æ®ï¼ˆä¼šæ›´æ–°å·²æœ‰æ•°æ®ï¼‰'
    )
    
    parser.add_argument(
        '--force-update',
        action='store_true',
        help='å¼ºåˆ¶æ›´æ–°æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬å·²æœ‰æ•°æ®ï¼‰'
    )
    
    args = parser.parse_args()
    
    try:
        update_vector_database(
            data_source=args.source,
            skip_duplicates=not args.no_skip_duplicates,
            force_update=args.force_update
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\næ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
