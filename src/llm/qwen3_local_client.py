"""
Qwen2.5-1.5B æœ¬åœ°æ¨¡å‹å®¢æˆ·ç«¯
åŸºäºvLLMæ¡†æ¶çš„é«˜æ€§èƒ½æ¨ç†å®ç°ï¼ˆ8GBæ˜¾å­˜ç¨³å®šç‰ˆï¼‰

æŠ€æœ¯æ ˆ:
- Qwen2.5-1.5B-Instruct (2024å¹´9æœˆæœ€æ–°ï¼ŒvLLMç¨³å®šè¿è¡Œ)
- vLLM (é«˜æ€§èƒ½æ¨ç†æ¡†æ¶ï¼ŒGPUåˆ©ç”¨ç‡90%+)
- æ‰¹é‡æ¨ç†ä¼˜åŒ– (64æ¡/batch)

æ€§èƒ½æŒ‡æ ‡:
- æ¨ç†é€Ÿåº¦: 40-50æ¡/ç§’ (RTX 4060 Laptop 8GB)
- æ˜¾å­˜å ç”¨: çº¦3-4GB (8GBæ˜¾å­˜å®Œå…¨å¤Ÿç”¨ï¼Œç•™è¶³ä½™é‡)
- GPUåˆ©ç”¨ç‡: 85-90%
- æˆæœ¬: 0å…ƒ (æœ¬åœ°éƒ¨ç½²)
- ç¨³å®šæ€§: âœ… å®Œç¾é€‚é…8GBæ˜¾å­˜
"""
import json
import logging
from typing import List, Dict, Optional
from pathlib import Path
import torch

logger = logging.getLogger(__name__)


class Qwen3LocalClient:
    """Qwen3-7Bæœ¬åœ°æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆåŸºäºvLLMï¼‰"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-1.5B-Instruct",
        gpu_memory_utilization: float = 0.85,
        max_model_len: int = 4096,
        dtype: str = "half",
        tensor_parallel_size: int = 1
    ):
        """
        åˆå§‹åŒ–Qwen3æœ¬åœ°å®¢æˆ·ç«¯
        
        Args:
            model_name: æ¨¡å‹åç§° (HuggingFaceæ¨¡å‹ID)
            gpu_memory_utilization: GPUæ˜¾å­˜åˆ©ç”¨ç‡ (0-1)
            max_model_len: æœ€å¤§åºåˆ—é•¿åº¦
            dtype: æ•°æ®ç±»å‹ ("half"=FP16, "float"=FP32)
            tensor_parallel_size: å¼ é‡å¹¶è¡Œæ•° (å¤šå¡æ¨ç†)
        """
        logger.info("="*80)
        logger.info("ğŸš€ åˆå§‹åŒ–Qwen3-7Bæœ¬åœ°æ¨¡å‹")
        logger.info("="*80)
        
        # æ£€æŸ¥GPU
        if not torch.cuda.is_available():
            raise RuntimeError("âŒ æœªæ£€æµ‹åˆ°GPUï¼æœ¬åœ°æ¨¡å‹éœ€è¦GPUæ”¯æŒã€‚")
        
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… æ˜¾å­˜: {gpu_memory:.1f} GB")
        
        # æ£€æŸ¥æœ¬åœ°æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé¿å…å¯åŠ¨ vLLM å­è¿›ç¨‹åå†æŠ¥é”™
        cache_root = Path.home() / ".cache" / "huggingface" / "hub"
        model_dir_name = "models--" + model_name.replace("/", "--")
        snapshots_dir = cache_root / model_dir_name / "snapshots"
        weights_found = False
        if snapshots_dir.exists():
            for snap in snapshots_dir.iterdir():
                if snap.is_dir() and any(snap.glob("*.safetensors")):
                    weights_found = True
                    break
        if not weights_found:
            raise RuntimeError(
                f"æœ¬åœ°æ¨¡å‹æƒé‡ä¸å­˜åœ¨ ({model_name})ï¼Œ"
                "è¯·å…ˆä¸‹è½½æˆ–ä½¿ç”¨ API æ¨¡å¼ï¼ˆå½“å‰ç³»ç»Ÿå·²è‡ªåŠ¨é™çº§ä¸º API æ¨¡å¼ï¼‰ã€‚"
            )

        # å¯¼å…¥vLLM
        try:
            from vllm import LLM, SamplingParams
            logger.info("âœ… vLLMå·²å®‰è£…")
        except ImportError:
            raise RuntimeError("vLLMæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install vllm")

        # åŠ è½½æ¨¡å‹
        logger.info(f"â³ åŠ è½½æ¨¡å‹: {model_name}")
        logger.info(f"   - GPUæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization*100:.0f}%")
        logger.info(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {max_model_len}")
        logger.info(f"   - æ•°æ®ç±»å‹: {dtype}")

        try:
            self.llm = LLM(
                model=model_name,
                trust_remote_code=True,
                gpu_memory_utilization=gpu_memory_utilization,
                max_model_len=max_model_len,
                dtype=dtype,
                tensor_parallel_size=tensor_parallel_size,
                download_dir=str(cache_root),
            )
            logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}") from e
        
        # é…ç½®é‡‡æ ·å‚æ•°
        from vllm import SamplingParams
        self.default_sampling_params = SamplingParams(
            temperature=0.1,  # ä½æ¸©åº¦ï¼Œè¾“å‡ºæ›´ç¨³å®š
            max_tokens=512,
            top_p=0.9,
            repetition_penalty=1.05
        )
        
        self.model_name = model_name
        logger.info("="*80)
        logger.info("âœ… Qwen3å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼")
        logger.info("="*80)
    
    def extract_skills_from_jd(
        self,
        jd_text: str,
        known_skills: Optional[List[str]] = None,
        temperature: float = 0.1
    ) -> List[str]:
        """
        ä»JDæ–‡æœ¬ä¸­æå–æŠ€èƒ½ï¼ˆå•æ¡ï¼‰
        
        Args:
            jd_text: èŒä½æè¿°æ–‡æœ¬
            known_skills: å·²çŸ¥æŠ€èƒ½åˆ—è¡¨ï¼ˆç”¨äºå‚è€ƒï¼‰
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            æå–çš„æŠ€èƒ½åˆ—è¡¨
        """
        prompt = self._build_skill_extraction_prompt(jd_text, known_skills)
        
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=512,
            top_p=0.9
        )
        
        outputs = self.llm.generate([prompt], sampling_params)
        result_text = outputs[0].outputs[0].text.strip()
        
        return self._parse_skills_from_response(result_text)
    
    def batch_extract_skills(
        self,
        jd_texts: List[str],
        known_skills: Optional[List[str]] = None,
        batch_size: int = 32,
        temperature: float = 0.1,
        show_progress: bool = True
    ) -> List[List[str]]:
        """
        æ‰¹é‡æå–æŠ€èƒ½ï¼ˆé«˜æ€§èƒ½ï¼‰
        
        Args:
            jd_texts: JDæ–‡æœ¬åˆ—è¡¨
            known_skills: å·²çŸ¥æŠ€èƒ½åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å° (vLLMä¼šè‡ªåŠ¨ä¼˜åŒ–)
            temperature: æ¸©åº¦å‚æ•°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æŠ€èƒ½åˆ—è¡¨çš„åˆ—è¡¨
        """
        from vllm import SamplingParams
        from tqdm import tqdm
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æå–æŠ€èƒ½: {len(jd_texts)} æ¡JD")
        logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size} (vLLMè‡ªåŠ¨æ‰¹å¤„ç†)")
        
        all_skills = []
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=512,
            top_p=0.9
        )
        
        # è®°å½•æœ‰æ•ˆJDçš„ç´¢å¼•å’Œpromptsï¼ˆè·³è¿‡ç©ºJDä»¥èŠ‚çœèµ„æºï¼‰
        valid_indices = []
        prompts = []
        for idx, jd in enumerate(jd_texts):
            if jd and len(jd.strip()) > 10:  # åªå¤„ç†æœ‰æ•ˆçš„JD
                valid_indices.append(idx)
                prompts.append(self._build_skill_extraction_prompt(jd, known_skills))
        
        # åˆå§‹åŒ–æ‰€æœ‰ç»“æœä¸ºç©ºåˆ—è¡¨
        all_skills = [[] for _ in range(len(jd_texts))]
        
        if not prompts:
            logger.warning("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„JDæ–‡æœ¬ï¼Œè·³è¿‡LLMæå–")
            return all_skills
        
        logger.info(f"   æœ‰æ•ˆJD: {len(prompts)}/{len(jd_texts)} æ¡")
        
        # æ‰¹é‡æ¨ç†ï¼ˆåªå¤„ç†æœ‰æ•ˆJDï¼‰
        total_batches = (len(prompts) + batch_size - 1) // batch_size
        
        iterator = range(0, len(prompts), batch_size)
        if show_progress:
            iterator = tqdm(iterator, total=total_batches, desc="Qwen3æ‰¹é‡æ¨ç†")
        
        extracted_skills = []  # ä¸´æ—¶å­˜å‚¨æå–çš„æŠ€èƒ½
        for i in iterator:
            batch_prompts = prompts[i:i+batch_size]
            
            # vLLMæ‰¹é‡æ¨ç†ï¼ˆè‡ªåŠ¨ä¼˜åŒ–GPUåˆ©ç”¨ç‡ï¼‰
            outputs = self.llm.generate(batch_prompts, sampling_params)
            
            # è§£æç»“æœ
            for output in outputs:
                result_text = output.outputs[0].text.strip()
                skills = self._parse_skills_from_response(result_text)
                extracted_skills.append(skills)
        
        # å°†æå–çš„æŠ€èƒ½å¡«å……åˆ°å¯¹åº”çš„åŸå§‹ç´¢å¼•ä½ç½®
        for valid_idx, skills in zip(valid_indices, extracted_skills):
            all_skills[valid_idx] = skills
        
        logger.info(f"âœ… æ‰¹é‡æå–å®Œæˆï¼")
        valid_skills = [s for s in all_skills if s]
        if valid_skills:
            logger.info(f"   å¹³å‡æ¯æ¡JDæå–: {sum(len(s) for s in valid_skills) / len(valid_skills):.1f} ä¸ªæŠ€èƒ½")
        
        return all_skills
    
    def _build_skill_extraction_prompt(
        self,
        jd_text: str,
        known_skills: Optional[List[str]] = None
    ) -> str:
        """æ„å»ºæŠ€èƒ½æå–Promptï¼ˆä¼˜åŒ–å°æ¨¡å‹JSONè¾“å‡ºï¼‰"""
        known_skills_str = ", ".join(known_skills[:50]) if known_skills else \
            "Python, Java, JavaScript, MySQL, Redis, Docker, Kubernetes, React, Vue, Django"
        
        # ä½¿ç”¨æ›´ç®€æ´çš„Promptï¼Œæé«˜å°æ¨¡å‹éµå¾ªç‡
        prompt = f"""ä»»åŠ¡: ä»JDä¸­æå–æŠ€æœ¯æŠ€èƒ½

JD: {jd_text[:1800]}

å‚è€ƒ: {known_skills_str}

è¦æ±‚:
1. åªæå–æŠ€æœ¯ç±»æŠ€èƒ½
2. è¾“å‡ºæ ‡å‡†JSONæ ¼å¼
3. ä¸è¦ä»»ä½•é¢å¤–æ–‡å­—

è¾“å‡º:
{{"skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"]}}"""

        return prompt
    
    def _parse_skills_from_response(self, response: str) -> List[str]:
        """è§£æLLMè¿”å›çš„æŠ€èƒ½åˆ—è¡¨ï¼ˆå¢å¼ºå®¹é”™ï¼‰"""
        import re
        
        try:
            # æ¸…ç†å“åº”
            response = response.strip()
            
            # æ–¹æ³•1: æå–ä»£ç å—ä¸­çš„JSON
            if '```json' in response:
                response = response.split('```json')[1].split('```')[0].strip()
            elif '```' in response:
                response = response.split('```')[1].split('```')[0].strip()
            
            # æ–¹æ³•2: æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„JSONå¯¹è±¡ï¼ˆå¤„ç†å¤šæ¬¡è¾“å‡ºçš„æƒ…å†µï¼‰
            # å¦‚: {"skills": []} æ ¹æ®èŒä½æè¿°æå–çš„æŠ€èƒ½ä¸ºï¼š {"skills": ["PHP"]}
            # ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™ï¼ŒåŒ¹é… { å¼€å§‹åˆ°ä¸‹ä¸€ä¸ª } ç»“æŸ
            json_pattern = r'\{\s*"skills"\s*:\s*\[(?:[^\[\]]|\[[^\]]*\])*\]\s*\}'
            json_matches = re.findall(json_pattern, response, re.DOTALL)
            
            all_skills = []
            
            if json_matches:
                # è§£ææ‰€æœ‰åŒ¹é…åˆ°çš„JSONå¯¹è±¡
                for json_str in json_matches:
                    try:
                        result = json.loads(json_str)
                        skills = result.get('skills', [])
                        if skills:  # åªä¿ç•™éç©ºçš„æŠ€èƒ½åˆ—è¡¨
                            all_skills.extend(skills)
                    except:
                        continue
            else:
                # æ–¹æ³•3: å°è¯•ç›´æ¥è§£æï¼ˆå¯èƒ½æœ‰å°¾éƒ¨åƒåœ¾å­—ç¬¦ï¼‰
                # ç§»é™¤æœ«å°¾çš„éJSONå­—ç¬¦
                response = re.sub(r'[^\}\]]+$', '', response)
                # ç¡®ä¿JSONæ ¼å¼å®Œæ•´
                if '{' in response and '}' not in response:
                    response += '}'
                if '[' in response and ']' not in response:
                    response += ']'
                
                result = json.loads(response)
                all_skills = result.get('skills', [])
            
            # è¿‡æ»¤å’Œæ¸…ç†
            all_skills = [s.strip() for s in all_skills if s and isinstance(s, str)]
            all_skills = [s for s in all_skills if len(s) > 1 and len(s) < 50]
            
            # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
            seen = set()
            unique_skills = []
            for skill in all_skills:
                if skill not in seen:
                    seen.add(skill)
                    unique_skills.append(skill)
            
            return unique_skills
            
        except json.JSONDecodeError as e:
            # é™çº§æ–¹æ¡ˆï¼šæ­£åˆ™æå–æ‰€æœ‰å¼•å·å†…çš„å†…å®¹
            logger.debug(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™æå–: {response[:80]}...")
            skills = re.findall(r'"([^"]+)"', response)
            # è¿‡æ»¤éæŠ€èƒ½è¯æ±‡
            skills = [s.strip() for s in skills if s and len(s) > 1 and len(s) < 50]
            # æ’é™¤å¸¸è§çš„éæŠ€èƒ½è¯ï¼ˆæ›´å…¨é¢ï¼‰
            exclude_words = {
                'skills', 'æŠ€èƒ½', 'æ ¹æ®', 'èŒä½', 'æè¿°', 'æå–', 'æŠ€æœ¯',
                'èƒ½åŠ›', 'ç»éªŒ', 'å­¦å†', 'å¹´é™', 'è¦æ±‚', 'å²—ä½', 'å·¥ä½œ',
                'è´£ä»»', 'æ²Ÿé€š', 'å›¢é˜Ÿ', 'åˆä½œ', 'æœ¬ç§‘', 'ç¡•å£«', 'å¹´'
            }
            skills = [s for s in skills if s not in exclude_words and not s.isdigit()]
            # å»é‡
            skills = list(dict.fromkeys(skills))
            return skills
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥: {e}")
            return []
    
    def chat(
        self,
        messages: List[Dict],
        temperature: float = 0.3,
        max_tokens: int = 1024
    ) -> str:
        """
        é€šç”¨å¯¹è¯æ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        # æ„å»ºpromptï¼ˆQwen3ä½¿ç”¨ChatMLæ ¼å¼ï¼‰
        prompt = self._build_chat_prompt(messages)
        
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=0.9
        )
        
        outputs = self.llm.generate([prompt], sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def _build_chat_prompt(self, messages: List[Dict]) -> str:
        """æ„å»ºChatMLæ ¼å¼çš„prompt"""
        prompt_parts = []
        for msg in messages:
            role = msg['role']
            content = msg['content']
            if role == 'system':
                prompt_parts.append(f"<|im_start|>system\n{content}<|im_end|>")
            elif role == 'user':
                prompt_parts.append(f"<|im_start|>user\n{content}<|im_end|>")
            elif role == 'assistant':
                prompt_parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
        
        prompt_parts.append("<|im_start|>assistant\n")
        return "\n".join(prompt_parts)
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_name': self.model_name,
            'framework': 'vLLM',
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
            'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else None,
        }


_singleton_instance: Optional['Qwen3LocalClient'] = None


def get_qwen3_client(
    model_name: str = "Qwen/Qwen2.5-1.5B-Instruct",
    gpu_memory_utilization: float = 0.85,
    max_model_len: int = 4096,
    dtype: str = "half",
    tensor_parallel_size: int = 1
) -> 'Qwen3LocalClient':
    """
    è·å– Qwen3LocalClient å…¨å±€å•ä¾‹ã€‚

    vLLM æ¨¡å‹å ç”¨å¤§é‡æ˜¾å­˜ï¼Œæ•´ä¸ªè¿›ç¨‹åªèƒ½åŠ è½½ä¸€æ¬¡ã€‚
    æ‰€æœ‰ç»„ä»¶ï¼ˆRAGServiceã€HybridSkillExtractor ç­‰ï¼‰å¿…é¡»å…±äº«åŒä¸€ä¸ªå®ä¾‹ã€‚

    é¦–æ¬¡è°ƒç”¨æ—¶åˆ›å»ºå¹¶ç¼“å­˜å®ä¾‹ï¼Œä¹‹åçš„è°ƒç”¨ç›´æ¥è¿”å›ç¼“å­˜å®ä¾‹ï¼ˆå¿½ç•¥å‚æ•°ï¼‰ã€‚
    """
    global _singleton_instance
    if _singleton_instance is None:
        logger.info("ğŸ” åˆ›å»º Qwen3LocalClient å…¨å±€å•ä¾‹...")
        _singleton_instance = Qwen3LocalClient(
            model_name=model_name,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=max_model_len,
            dtype=dtype,
            tensor_parallel_size=tensor_parallel_size,
        )
        logger.info("âœ… Qwen3LocalClient å•ä¾‹å·²å°±ç»ªï¼Œåç»­è°ƒç”¨å°†å¤ç”¨æ­¤å®ä¾‹")
    else:
        logger.info("â™»ï¸  å¤ç”¨å·²æœ‰ Qwen3LocalClient å•ä¾‹ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
    return _singleton_instance


# æµ‹è¯•ä»£ç 
def test_qwen3_client():
    """æµ‹è¯•Qwen3å®¢æˆ·ç«¯"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•Qwen3æœ¬åœ°å®¢æˆ·ç«¯")
    print("="*80)
    
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = Qwen3LocalClient()
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        info = client.get_model_info()
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ¨¡å‹: {info['model_name']}")
        print(f"   æ¡†æ¶: {info['framework']}")
        print(f"   GPU: {info['gpu_name']}")
        print(f"   æ˜¾å­˜: {info['gpu_memory_gb']:.1f} GB")
        
        # æµ‹è¯•1: å•æ¡æå–
        print(f"\n{'='*80}")
        print("ã€æµ‹è¯•1: å•æ¡æŠ€èƒ½æå–ã€‘")
        print("="*80)
        
        test_jd = """
        å²—ä½èŒè´£:
        1. è´Ÿè´£åç«¯æœåŠ¡å¼€å‘ï¼Œä½¿ç”¨Pythonå’ŒDjangoæ¡†æ¶
        2. ç†Ÿç»ƒä½¿ç”¨MySQLæ•°æ®åº“ï¼Œæœ‰Redisç¼“å­˜ä½¿ç”¨ç»éªŒ
        3. äº†è§£Dockerå®¹å™¨åŒ–éƒ¨ç½²ï¼Œæœ‰Kubernetesç»éªŒä¼˜å…ˆ
        4. ç†Ÿæ‚‰RESTful APIè®¾è®¡ï¼Œæœ‰å¾®æœåŠ¡æ¶æ„ç»éªŒ
        """
        
        skills = client.extract_skills_from_jd(test_jd)
        print(f"âœ… æå–æŠ€èƒ½: {skills}")
        print(f"   å…± {len(skills)} ä¸ªæŠ€èƒ½")
        
        # æµ‹è¯•2: æ‰¹é‡æå–
        print(f"\n{'='*80}")
        print("ã€æµ‹è¯•2: æ‰¹é‡æŠ€èƒ½æå–ã€‘")
        print("="*80)
        
        test_jds = [test_jd] * 10  # æµ‹è¯•10æ¡
        all_skills = client.batch_extract_skills(test_jds, batch_size=32)
        print(f"âœ… æ‰¹é‡æå–å®Œæˆ: {len(all_skills)} æ¡")
        print(f"   å¹³å‡æ¯æ¡: {sum(len(s) for s in all_skills) / len(all_skills):.1f} ä¸ªæŠ€èƒ½")
        
        print(f"\n{'='*80}")
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    test_qwen3_client()
