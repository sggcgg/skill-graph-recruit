import os
import csv
import json
import time
import random
import traceback
import uuid
from loguru import logger
from datetime import datetime
from DrissionPage import ChromiumOptions, WebPage

# ç¡®ä¿å·¥ä½œç›®å½•ä¸ºé¡¹ç›®æ ¹ç›®å½•
# æ— è®ºä»å“ªé‡Œè¿è¡Œæ­¤è„šæœ¬ï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°é…ç½®æ–‡ä»¶
# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆå³é¡¹ç›®æ ¹ç›®å½•ï¼‰
# src/crawler/boss_listdata.py -> src/crawler -> src -> é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(SCRIPT_DIR))


class BossZhipinSpider(object):
    """BOSSç›´è˜èŒä½çˆ¬è™« - ç®€åŒ–ç‰ˆæœ¬ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰"""
    
    VERSION = "1.6.1"  # ç‰ˆæœ¬å·ï¼ˆæ™ºèƒ½æ£€æµ‹åº•éƒ¨ + æ€§èƒ½ä¼˜åŒ– + é£æ§ä¿æŠ¤ä¼˜åŒ–ï¼‰
    
    def __init__(self):
        self.logger = logger
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.join(PROJECT_ROOT, "src", "crawler", "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        self.logger.add(
            os.path.join(log_dir, "boss_spider_{time}.log"),
            rotation="500 MB",
            retention="10 days",
            level="INFO"
        )
        
        # ========== çˆ¬è™«é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆ v1.4ï¼‰==========
        # ä¸‹æ»‘æ¬¡æ•°é…ç½®ï¼šBossç›´è˜æ¯ä¸ªå…³é”®è¯çº¦300æ¡æ•°æ®ï¼Œä¸‹æ»‘90æ¬¡å¯è·å–æ›´å¤šæ•°æ®
        self.scroll_times_per_keyword = 90  # æ¯ä¸ªå…³é”®è¯ä¸‹æ»‘æ¬¡æ•°ï¼ˆ90æ¬¡è·å–æ›´å…¨é¢æ•°æ®ï¼‰
        self.page_delay_min = 2  # é¡µé¢åˆ‡æ¢æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.page_delay_max = 5  # é¡µé¢åˆ‡æ¢æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        
        # ========== é£æ§é…ç½®ï¼ˆä¿å®ˆç‰ˆï¼Œé™ä½å°å·é£é™©ï¼‰==========
        self.enable_anti_detection = True  # æ˜¯å¦å¯ç”¨åæ£€æµ‹å¢å¼º
        self.task_interval_min = 5    # ä»»åŠ¡é—´æœ€å°é—´éš”ï¼ˆç§’ï¼‰
        self.task_interval_max = 10   # ä»»åŠ¡é—´æœ€å¤§é—´éš”ï¼ˆç§’ï¼‰
        self.long_break_interval = 10 # æ¯10ä¸ªä»»åŠ¡å¼ºåˆ¶é•¿ä¼‘æ¯ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
        self.long_break_min = 15      # é•¿ä¼‘æ¯æœ€å°æ—¶é—´ï¼ˆç§’ï¼‰
        self.long_break_max = 25      # é•¿ä¼‘æ¯æœ€å¤§æ—¶é—´ï¼ˆç§’ï¼‰
        
        # ========== æ€§èƒ½ä¼˜åŒ–é…ç½® ==========
        self.fast_scroll_mode = True  # å¯ç”¨å¿«é€Ÿæ»šåŠ¨æ¨¡å¼
        self.batch_process_packets = True  # å¯ç”¨æ‰¹é‡å¤„ç†æ•°æ®åŒ…
        
        # ========== é£æ§ä¿æŠ¤é…ç½® ==========
        self.max_consecutive_failures = 5  # è¿ç»­å¤±è´¥æ¬¡æ•°ä¸Šé™ï¼ˆè§¦å‘è‡ªåŠ¨åœæ­¢ï¼‰
        self.consecutive_failures = 0  # å½“å‰è¿ç»­å¤±è´¥æ¬¡æ•°
        self.enable_risk_detection = True  # å¯ç”¨é£æ§æ£€æµ‹
        
        # ========== æ•°æ®å­˜å‚¨é…ç½® ==========
        # åŸå§‹æ•°æ®ä¿å­˜ç›®å½•ï¼šdata/raw/ ï¼ˆç›´æ¥ä¿å­˜åˆ°rawç›®å½•ï¼‰
        self.data_dir = os.path.join(PROJECT_ROOT, "data", "raw")
        os.makedirs(self.data_dir, exist_ok=True)
        
        # åŸå¸‚æ•°æ®æ–‡ä»¶è·¯å¾„ç¼“å­˜ï¼š{åŸå¸‚å: æ–‡ä»¶è·¯å¾„}
        self.city_files = {}
        # åŸå¸‚å·²æœ‰job_idé›†åˆï¼š{åŸå¸‚å: set(job_id1, job_id2, ...)}
        self.city_job_ids = {}
        # åŸå¸‚èŒä½æ•°æ®å†…å­˜ç¼“å­˜ï¼š{åŸå¸‚å: [job1, job2, ...]}
        # é¿å…æ¯æ¬¡å…³é”®è¯å®Œæˆåé‡å¤ä»ç£ç›˜è¯»å–åŸå¸‚å¤§æ–‡ä»¶ï¼ˆå…³é”®æ€§èƒ½ä¼˜åŒ–ï¼‰
        self.city_data_cache = {}
        
        # ========== æ–­ç‚¹ç»­ä¼ é…ç½® ==========
        # è¿›åº¦æ–‡ä»¶ä¿å­˜è·¯å¾„
        self.progress_file = os.path.join(PROJECT_ROOT, "data", "crawler_progress.json")
        self.enable_resume = True  # æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
        # è¿›åº¦ä¿¡æ¯å†…å­˜ç¼“å­˜ï¼Œé¿å…æ¯ä¸ªä»»åŠ¡é‡å¤è¯»å†™ç£ç›˜
        self._progress_cache = None
        
        # ========== æµ‹è¯•æ¨¡å¼é…ç½® ==========
        self.test_mode = False  # æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
        self.test_scroll_times = 5  # æµ‹è¯•æ¨¡å¼ä¸‹æ»‘æ¬¡æ•°ï¼ˆæ­£å¸¸55æ¬¡ï¼Œæµ‹è¯•5æ¬¡ï¼‰
        
        # ========== åŸå¸‚é…ç½® ==========
        # å·²æŠ“å–åŸå¸‚ï¼ˆç¬¬ä¸€æ‰¹ï¼‰
        self.cities_done = {
            'åŒ—äº¬': '101010100',
            'ä¸Šæµ·': '101020100',
            'å¹¿å·': '101280100',
            'æ·±åœ³': '101280600',
            'æ­å·': '101210100',
            'æˆéƒ½': '101270100',
        }

        # å¾…æŠ“å–åŸå¸‚ï¼ˆç¬¬äºŒæ‰¹ï¼ŒæŒ‰å­—æ¯/æ‹¼éŸ³æ’åºï¼‰
        self.cities_new = {
            'å¤©æ´¥': '101030100',
            'é‡åº†': '101040100',
            'å“ˆå°”æ»¨': '101050100',
            'é•¿æ˜¥': '101060100',
            'å¤§è¿': '101070200',
            'å‘¼å’Œæµ©ç‰¹': '101080100',
            'å¤ªåŸ': '101100100',
            'è¥¿å®‰': '101110100',
            'å…°å·': '101160100',
            'éƒ‘å·': '101180100',
            'å¼€å°': '101180800',
            'å—äº¬': '101190100',
            'æ— é”¡': '101190200',
            'è‹å·': '101190400',
            'æ‰¬å·': '101190600',
            'åˆè‚¥': '101220100',
            'èŠœæ¹–': '101220300',
            'ç¦å·': '101230100',
            'å¦é—¨': '101230200',
            'å—æ˜Œ': '101240100',
            'é•¿æ²™': '101250100',
            'å¸¸å¾·': '101250600',
            'è´µé˜³': '101260100',
            'æ­¦æ±‰': '101200100',
            'ä½›å±±': '101280800',
            'ä¸œè': '101281600',
            'æµ·å£': '101310100',
        }

        # å½“å‰æŠ“å–ä»»åŠ¡ç”¨çš„åŸå¸‚ï¼ˆç”±èœå•é€‰æ‹©å†³å®šï¼‰
        self.cities = self.cities_done  # é»˜è®¤å…¼å®¹åŸæœ‰é€»è¾‘
        
        # ========== å…³é”®è¯é…ç½® ==========
        # ä» data/crawl_keywords.json åŠ è½½å…³é”®è¯åˆ—è¡¨
        self.keywords_config_file = os.path.join(PROJECT_ROOT, "data", "crawl_keywords.json")
        self.all_keywords = []  # æ‰€æœ‰å…³é”®è¯ï¼ˆå²—ä½ç±»å‹ + æŠ€èƒ½å…³é”®è¯ï¼‰
        self.load_keywords()
        
    def load_keywords(self):
        """
        ä» data/crawl_keywords.json åŠ è½½å…³é”®è¯åˆ—è¡¨
        
        å…³é”®è¯æ¥æºï¼šç”± scripts/generate_crawl_keywords.py ä»æŠ€èƒ½è¯å…¸è‡ªåŠ¨ç”Ÿæˆ
        åŒ…å«ï¼š
          - job_type_keywords: å²—ä½ç±»å‹å…³é”®è¯ï¼ˆå¦‚"Pythonå¼€å‘"ã€"ç®—æ³•å·¥ç¨‹å¸ˆ"ï¼‰
          - skill_keywords: æŠ€èƒ½å…³é”®è¯ï¼ˆå¦‚"Spring Boot"ã€"Vue"ã€"Redis"ï¼‰
        
        é¢„è®¡æ•°æ®é‡ï¼ˆå¤šåŸå¸‚ç‰ˆæœ¬ï¼‰ï¼š
          - åŸå¸‚æ•°é‡ï¼š6ä¸ªï¼ˆåŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·ã€æ·±åœ³ã€æ­å·ã€æˆéƒ½ï¼‰
          - æ€»å…³é”®è¯æ•°ï¼š99ä¸ªï¼ˆå·²ä¼˜åŒ–ï¼‰
          - æ¯ä¸ªåŸå¸‚æ¯ä¸ªå…³é”®è¯ï¼šçº¦300æ¡å²—ä½
          - ç†è®ºæ€»æ•°ï¼š6 Ã— 99 Ã— 300 = 178,200æ¡
          - å»é‡åï¼šçº¦106,920æ¡ï¼ˆä¼˜ç§€çº§åˆ«ï¼‰
        """
        try:
            if not os.path.exists(self.keywords_config_file):
                self.logger.warning(f"å…³é”®è¯é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.keywords_config_file}")
                self.logger.info(f"è¯·å…ˆè¿è¡Œ: python scripts/generate_crawl_keywords.py")
                self.logger.info(f"æˆ–æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨äº: {self.keywords_config_file}")
                self.all_keywords = []
                return
            
            with open(self.keywords_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # åŠ è½½æ‰€æœ‰å…³é”®è¯ï¼ˆå²—ä½ç±»å‹ + æŠ€èƒ½å…³é”®è¯ï¼‰
            job_keywords = config.get('job_type_keywords', [])
            skill_keywords = config.get('skill_keywords', [])
            
            # åˆå¹¶å…³é”®è¯åˆ—è¡¨
            self.all_keywords = job_keywords + skill_keywords
            
            self.logger.info(f"âœ“ æˆåŠŸåŠ è½½å…³é”®è¯é…ç½®æ–‡ä»¶")
            self.logger.info(f"  - å²—ä½ç±»å‹å…³é”®è¯: {len(job_keywords)}ä¸ª")
            self.logger.info(f"  - æŠ€èƒ½å…³é”®è¯: {len(skill_keywords)}ä¸ª")
            self.logger.info(f"  - æ€»å…³é”®è¯æ•°: {len(self.all_keywords)}ä¸ª")
            self.logger.info(f"  - åŸå¸‚æ•°é‡: {len(self.cities)}ä¸ª")
            
            # é¢„ä¼°æ•°æ®é‡ï¼ˆå¤šåŸå¸‚ï¼‰
            estimated_total = len(self.cities) * len(self.all_keywords) * 300
            estimated_after_dedup = int(estimated_total * 0.6)
            self.logger.info(f"  - é¢„è®¡æŠ“å–: {estimated_total:,}æ¡ (å»é‡åçº¦ {estimated_after_dedup:,}æ¡)")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½å…³é”®è¯é…ç½®å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            self.all_keywords = []
    
    def human_like_delay(self, min_seconds=1, max_seconds=3):
        """æ¨¡æ‹Ÿäººç±»æ“ä½œå»¶è¿Ÿ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
        
    def random_scroll(self, page):
        """éšæœºæ»šåŠ¨é¡µé¢ï¼Œæ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""
        try:
            scroll_times = random.randint(2, 4)
            for _ in range(scroll_times):
                scroll_distance = random.randint(300, 800)
                page.run_js(f"window.scrollBy(0, {scroll_distance});")
                self.human_like_delay(0.5, 1.5)
                
            # æœ‰æ—¶å€™å‘ä¸Šæ»šåŠ¨ä¸€ç‚¹
            if random.random() > 0.7:
                page.run_js(f"window.scrollBy(0, -{random.randint(100, 300)});")
                self.human_like_delay(0.5, 1)
        except Exception as e:
            self.logger.warning(f"æ»šåŠ¨é¡µé¢å¤±è´¥: {e}")
    
    def simulate_human_interaction(self, page):
        """æ¨¡æ‹Ÿäººç±»äº¤äº’è¡Œä¸ºï¼ˆå¢å¼ºåæ£€æµ‹ï¼‰"""
        try:
            # éšæœºç§»åŠ¨é¼ æ ‡
            if random.random() > 0.5:
                x = random.randint(100, 800)
                y = random.randint(100, 600)
                page.run_js(f"""
                    var evt = new MouseEvent('mousemove', {{
                        clientX: {x},
                        clientY: {y}
                    }});
                    document.dispatchEvent(evt);
                """)
                time.sleep(random.uniform(0.1, 0.3))
            
            # å¶å°”å‘ä¸Šæ»šåŠ¨ï¼ˆæ¨¡æ‹Ÿå›çœ‹ï¼‰
            if random.random() > 0.8:
                scroll_up = random.randint(200, 500)
                page.run_js(f"window.scrollBy(0, -{scroll_up});")
                time.sleep(random.uniform(1, 2))
                
        except Exception as e:
            self.logger.debug(f"æ¨¡æ‹Ÿäº¤äº’å¤±è´¥: {e}")
    
    # ========== åæ£€æµ‹èµ„æºæ±  ==========
    # åªä½¿ç”¨ Windows + Chrome UAï¼Œä¸å®é™…è¿è¡Œç¯å¢ƒå®Œå…¨ä¸€è‡´
    # ç‰ˆæœ¬åœ¨æœ€è¿‘å‡ ä¸ªä¸»æµç‰ˆæœ¬ä¹‹é—´è½®æ¢ï¼Œé¿å…å›ºå®šç‰ˆæœ¬è¢«æ ‡è®°
    _UA_POOL = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.6423.119 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.6668.103 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.6613.120 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.6533.120 Safari/537.36',
    ]

    # å¸¸è§çœŸå®åˆ†è¾¨ç‡
    _WINDOW_SIZES = [
        (1920, 1080), (1920, 1080), (1920, 1080),  # æœ€å¸¸è§ï¼Œæƒé‡é«˜
        (1440, 900), (1536, 864), (2560, 1440),
        (1366, 768), (1280, 800), (1600, 900),
    ]

    def _get_stealth_js(self) -> str:
        """ç”Ÿæˆé«˜åº¦æ‹ŸçœŸçš„åæ£€æµ‹JSï¼ˆCanvas/WebGLæŒ‡çº¹éšæœºåŒ– + çœŸå®pluginsï¼‰"""
        # éšæœºCanvaså™ªå£°åç§»é‡ï¼ˆæ¯æ¬¡å¯åŠ¨ä¸åŒï¼‰
        r_offset = random.randint(1, 8)
        g_offset = random.randint(1, 8)
        b_offset = random.randint(1, 8)

        return f'''
(function() {{
    // ===== 1. å½»åº•æ¸…é™¤ webdriver æ ‡å¿— =====
    // åŒæ—¶è¦†ç›– prototype å’Œå®ä¾‹ï¼ŒåŒé‡ä¿éšœï¼Œé˜²æ­¢ä»»ä½•è®¿é—®è·¯å¾„è¿”å› true
    try {{
        Object.defineProperty(Navigator.prototype, 'webdriver', {{
            get: () => undefined,
            configurable: true,
            enumerable: false
        }});
    }} catch(e) {{}}
    try {{
        Object.defineProperty(navigator, 'webdriver', {{
            get: () => undefined,
            configurable: true,
            enumerable: false
        }});
    }} catch(e) {{}}
    try {{ delete Navigator.prototype.webdriver; }} catch(e) {{}}
    try {{ delete navigator.__proto__.webdriver; }} catch(e) {{}}

    // ===== 2. ä»¿çœŸ plugins =====
    try {{
        const pluginData = [
            {{ name: 'Chrome PDF Plugin',  filename: 'internal-pdf-viewer',             description: 'Portable Document Format' }},
            {{ name: 'Chrome PDF Viewer',  filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai', description: '' }},
            {{ name: 'Native Client',      filename: 'internal-nacl-plugin',             description: '' }},
        ];
        const pluginArray = pluginData.map(p => {{
            const plugin = Object.create(Plugin.prototype);
            Object.defineProperties(plugin, {{
                name:        {{ value: p.name,        enumerable: true }},
                filename:    {{ value: p.filename,    enumerable: true }},
                description: {{ value: p.description, enumerable: true }},
                length:      {{ value: 0,             enumerable: true }},
            }});
            return plugin;
        }});
        Object.defineProperty(navigator, 'plugins', {{
            get: () => Object.assign(pluginArray, {{
                length: pluginArray.length,
                item: i => pluginArray[i],
                namedItem: n => pluginArray.find(p => p.name === n) || null,
                refresh: () => {{}}
            }}),
            configurable: true
        }});
    }} catch(e) {{}}

    // ===== 3. è¯­è¨€ =====
    try {{
        Object.defineProperty(navigator, 'languages', {{
            get: () => ['zh-CN', 'zh', 'en-US', 'en'],
            configurable: true
        }});
    }} catch(e) {{}}

    // ===== 4. chrome å¯¹è±¡ =====
    try {{
        if (!window.chrome) {{
            window.chrome = {{
                app: {{ isInstalled: false }},
                runtime: {{}},
                csi: function() {{ return {{ startE: Date.now(), onloadT: Date.now() + 40, pageT: 1000 + Math.random()*500, tran: 15 }}; }},
                loadTimes: function() {{ return {{ commitLoadTime: Date.now()/1000 - 2, connectionInfo: 'h2', finishDocumentLoadTime: Date.now()/1000 - 0.5, finishLoadTime: Date.now()/1000 - 0.1, firstPaintTime: Date.now()/1000 - 1.5, navigationType: 'Other', wasNpnNegotiated: true }}; }},
            }};
        }}
    }} catch(e) {{}}

    // ===== 5. Canvas æŒ‡çº¹éšæœºåŒ– =====
    try {{
        const origToDataURL = HTMLCanvasElement.prototype.toDataURL;
        HTMLCanvasElement.prototype.toDataURL = function(type) {{
            if (type === 'image/png' && this.width > 16) {{
                const ctx = this.getContext('2d');
                if (ctx) {{
                    const imageData = ctx.getImageData(0, 0, this.width, this.height);
                    for (let i = 0; i < imageData.data.length; i += 4) {{
                        imageData.data[i]     = Math.min(255, imageData.data[i]     + {r_offset});
                        imageData.data[i + 1] = Math.min(255, imageData.data[i + 1] + {g_offset});
                        imageData.data[i + 2] = Math.min(255, imageData.data[i + 2] + {b_offset});
                    }}
                    ctx.putImageData(imageData, 0, 0);
                }}
            }}
            return origToDataURL.apply(this, arguments);
        }};
    }} catch(e) {{}}

    // ===== 6. WebGL æŒ‡çº¹éšæœºåŒ– =====
    try {{
        const _webglVendors = [
            ['Google Inc. (NVIDIA)', 'ANGLE (NVIDIA, NVIDIA GeForce RTX 3060 Direct3D11 vs_5_0 ps_5_0, D3D11)'],
            ['Google Inc. (Intel)',  'ANGLE (Intel, Intel(R) UHD Graphics 630 Direct3D11 vs_5_0 ps_5_0, D3D11)'],
            ['Google Inc. (AMD)',    'ANGLE (AMD, AMD Radeon RX 580 Direct3D11 vs_5_0 ps_5_0, D3D11)'],
            ['Google Inc. (NVIDIA)', 'ANGLE (NVIDIA, NVIDIA GeForce GTX 1660 Direct3D11 vs_5_0 ps_5_0, D3D11)'],
            ['Google Inc. (Intel)',  'ANGLE (Intel, Intel(R) Iris(R) Xe Graphics Direct3D11 vs_5_0 ps_5_0, D3D11)'],
        ];
        const _wgl = _webglVendors[{random.randint(0, 4)}];
        const getParamOrig = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {{
            if (parameter === 37445) return _wgl[0];
            if (parameter === 37446) return _wgl[1];
            return getParamOrig.call(this, parameter);
        }};
    }} catch(e) {{}}

    // ===== 7. æƒé™æŸ¥è¯¢ä¼ªè£… =====
    try {{
        const origQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
            parameters.name === 'notifications'
                ? Promise.resolve({{ state: Notification.permission }})
                : origQuery(parameters)
        );
    }} catch(e) {{}}

    // ===== 8. éšè— headless ç‰¹å¾ï¼ˆæ¯ä¸ªç‹¬ç«‹ try-catchï¼‰ =====
    try {{ Object.defineProperty(navigator, 'maxTouchPoints',      {{ get: () => 0,                               configurable: true }}); }} catch(e) {{}}
    try {{ Object.defineProperty(navigator, 'hardwareConcurrency', {{ get: () => {random.choice([4, 8, 12, 16])}, configurable: true }}); }} catch(e) {{}}
    try {{ Object.defineProperty(navigator, 'deviceMemory',        {{ get: () => {random.choice([4, 8, 16])},     configurable: true }}); }} catch(e) {{}}
    try {{ Object.defineProperty(screen, 'colorDepth',             {{ get: () => 24,                              configurable: true }}); }} catch(e) {{}}
    try {{ Object.defineProperty(screen, 'pixelDepth',             {{ get: () => 24,                              configurable: true }}); }} catch(e) {{}}
}})();
'''

    def _save_cookies(self, page, filename='boss_cookies.json'):
        """ä¿å­˜ç™»å½• Cookieï¼Œä¸‹æ¬¡ç›´æ¥å¤ç”¨ï¼Œé¿å…é¢‘ç¹æ‰«ç """
        try:
            cookie_path = os.path.join(PROJECT_ROOT, 'data', filename)
            cookies = page.cookies()
            with open(cookie_path, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False)
            self.logger.info(f"âœ“ Cookie å·²ä¿å­˜: {cookie_path}")
        except Exception as e:
            self.logger.warning(f"Cookie ä¿å­˜å¤±è´¥: {e}")

    def _load_cookies(self, page, filename='boss_cookies.json'):
        """åŠ è½½å·²ä¿å­˜çš„ Cookieï¼Œè·³è¿‡ç™»å½•æµç¨‹"""
        try:
            cookie_path = os.path.join(PROJECT_ROOT, 'data', filename)
            if not os.path.exists(cookie_path):
                return False
            # Cookie è¶…è¿‡12å°æ—¶åˆ™è§†ä¸ºè¿‡æœŸ
            mtime = os.path.getmtime(cookie_path)
            if time.time() - mtime > 12 * 3600:
                self.logger.info("Cookie å·²è¿‡æœŸï¼ˆ>12å°æ—¶ï¼‰ï¼Œéœ€è¦é‡æ–°ç™»å½•")
                return False
            with open(cookie_path, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            for cookie in cookies:
                try:
                    page.set.cookies(cookie)
                except Exception:
                    pass
            self.logger.info(f"âœ“ Cookie åŠ è½½æˆåŠŸï¼ˆ{len(cookies)} æ¡ï¼‰ï¼Œå°è¯•è·³è¿‡ç™»å½•")
            return True
        except Exception as e:
            self.logger.warning(f"Cookie åŠ è½½å¤±è´¥: {e}")
            return False

    def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨ - å¼ºåŒ–åæ£€æµ‹é…ç½®ï¼ˆCDPæ³¨å…¥ï¼Œé¡µé¢åŠ è½½å‰ç”Ÿæ•ˆï¼‰"""
        try:
            ua = random.choice(self._UA_POOL)
            win_w, win_h = random.choice(self._WINDOW_SIZES)
            self.logger.info(f"æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨... UA: {ua[:60]}...")
            self.logger.info(f"çª—å£å¤§å°: {win_w}x{win_h}")

            co = ChromiumOptions()

            # ========== å…³é”®è¯´æ˜ ==========
            # ä¸ä½¿ç”¨ --disable-blink-features=AutomationControlledï¼š
            #   è¯¥ flag ä¼šè®© Chrome æ˜¾ç¤º"ä¸å—æ”¯æŒçš„å‘½ä»¤è¡Œæ ‡è®°"è­¦å‘Šæ¡ï¼Œ
            #   è¿™ä¸ªè­¦å‘Šæ¡æœ¬èº«å°±æ˜¯ä¸€ä¸ªæ˜æ˜¾çš„è‡ªåŠ¨åŒ–ç‰¹å¾ï¼Œåè€Œæš´éœ²èº«ä»½ã€‚
            # ä¹Ÿä¸ä½¿ç”¨ --exclude-switches=enable-automationï¼š
            #   DrissionPage çš„ CDP æ¨¡å¼ä¸ä¾èµ– --enable-automationï¼Œ
            #   ä¸éœ€è¦æ’é™¤å®ƒã€‚
            # åæ£€æµ‹å®Œå…¨äº¤ç»™ CDP JS æ³¨å…¥ï¼ˆPage.addScriptToEvaluateOnNewDocumentï¼‰
            # å’Œ run_js åŒä¿é™©æ¥å®ç°ã€‚

            # ========== è®© Chrome è¡Œä¸ºæ›´æ¥è¿‘æ™®é€šç”¨æˆ· ==========
            co.set_argument('--no-first-run')
            co.set_argument('--no-default-browser-check')
            co.set_argument('--no-pings')

            # ========== æ— ç—•æ¨¡å¼ ==========
            co.incognito()

            # ========== ç¦ç”¨æ‰©å±•ï¼ˆæ— ç—•æœ¬å°±æ— æ‰©å±•ï¼ŒåŠ ä¸Šé¿å…ç³»ç»Ÿçº§æ‰©å±•å¹²æ‰°ç›‘å¬ï¼‰==========
            co.set_argument('--disable-extensions')

            # ========== ç¨³å®šæ€§å‚æ•° ==========
            co.set_argument('--disable-dev-shm-usage')
            co.set_argument('--disable-popup-blocking')
            co.set_argument('--disable-notifications')
            # æ³¨æ„ï¼šä¸åŠ  --disable-gpuï¼ŒçœŸå®æµè§ˆå™¨éƒ½æœ‰GPUåŠ é€Ÿ

            # ========== çª—å£è®¾ç½®ï¼ˆéšæœºåˆ†è¾¨ç‡ï¼‰==========
            co.set_argument(f'--window-size={win_w},{win_h}')

            # ========== åå¥½è®¾ç½® ==========
            # æ³¨æ„ï¼šä¸ç”¨ co.set_user_agent()ï¼Œæ”¹ç”¨åé¢çš„ CDP Emulation.setUserAgentOverride
            # co.set_user_agent åªæ”¹ navigator.userAgentï¼Œä¸æ”¹ navigator.userAgentData.brands
            # ä¸¤è€…ä¸ä¸€è‡´ä¼šè¢« BOSS æ£€æµ‹åˆ°
            co.set_pref('profile.default_content_setting_values.notifications', 2)
            co.set_pref('credentials_enable_service', False)
            co.set_pref('profile.password_manager_enabled', False)
            co.set_pref('intl.accept_languages', 'zh-CN,zh,en-US,en')

            # ========== åˆ›å»ºæµè§ˆå™¨å®ä¾‹ ==========
            page = WebPage(chromium_options=co)

            # ========== å…³é”®ï¼šç”¨ CDP åœ¨æ¯ä¸ªæ–°é¡µé¢åŠ è½½ã€ä¹‹å‰ã€‘æ³¨å…¥åæ£€æµ‹JS ==========
            # page.run_js() æ˜¯é¡µé¢åŠ è½½ã€ä¹‹åã€‘æ‰§è¡Œï¼Œé‚£æ—¶å€™å·²ç»è¢«æ£€æµ‹äº†
            # Page.addScriptToEvaluateOnNewDocument åœ¨ HTML è§£æå‰å°±è¿è¡Œï¼Œå½»åº•è§„é¿æ£€æµ‹
            # å¿…é¡»å…ˆ Page.enableï¼Œå¦åˆ™ addScriptToEvaluateOnNewDocument åœ¨éƒ¨åˆ†ç¯å¢ƒä¸‹æ— æ•ˆ
            try:
                page.run_cdp('Page.enable')
            except Exception:
                pass
            stealth_js = self._get_stealth_js()
            result = page.run_cdp('Page.addScriptToEvaluateOnNewDocument', source=stealth_js)
            self.logger.info(f"âœ“ CDPåæ£€æµ‹æ³¨å…¥æˆåŠŸ: scriptId={result.get('identifier', 'ok')}")

            # ========== ç”¨ CDP åŒæ—¶è¦†ç›– UA å’Œ userAgentDataï¼ˆä¸¤è€…å¿…é¡»ä¸€è‡´ï¼‰==========
            # co.set_user_agent åªæ”¹ navigator.userAgentï¼Œä¸æ”¹ userAgentData.brands
            # BOSS ä¼šæ¯”å¯¹ä¸¤è€…ï¼Œä¸ä¸€è‡´å³åˆ¤å®šä¸ºä¼ªé€  â†’ ç”¨ Emulation.setUserAgentOverride ä¸€æ¬¡è§£å†³
            try:
                # å…ˆè¯»å– Chrome å®é™…ç‰ˆæœ¬å·ï¼Œä»çœŸå® UA ä¸­æå–
                page.get('about:blank')
                real_ua = page.run_js('return navigator.userAgent') or ''
                self.logger.info(f"Chrome å®é™… UA: {real_ua}")
                # ä»å®é™… UA æå–ç‰ˆæœ¬ï¼Œæ¯”å¦‚ "Chrome/131.0.6778.86" â†’ "131"
                import re as _re
                _m = _re.search(r'Chrome/(\d+)', real_ua)
                chrome_ver = _m.group(1) if _m else '131'
                # ä½¿ç”¨å®é™… UAï¼ˆä¸æ”¹ç‰ˆæœ¬ï¼Œåªä¿è¯ä¸€è‡´æ€§ï¼‰
                page.run_cdp('Emulation.setUserAgentOverride',
                    userAgent=real_ua,
                    platform='Win32',
                    acceptLanguage='zh-CN,zh,en-US,en',
                    userAgentMetadata={
                        'brands': [
                            {'brand': 'Google Chrome',  'version': chrome_ver},
                            {'brand': 'Chromium',       'version': chrome_ver},
                            {'brand': 'Not_A Brand',    'version': '24'},
                        ],
                        'fullVersionList': [
                            {'brand': 'Google Chrome',  'version': f'{chrome_ver}.0.0.0'},
                            {'brand': 'Chromium',       'version': f'{chrome_ver}.0.0.0'},
                            {'brand': 'Not_A Brand',    'version': '24.0.0.0'},
                        ],
                        'platform': 'Windows',
                        'platformVersion': '10.0.0',
                        'architecture': 'x86',
                        'model': '',
                        'mobile': False,
                        'bitness': '64',
                        'wow64': False,
                    }
                )
                self.logger.info(f"âœ“ UA å’Œ userAgentData å·²åŒæ­¥ (Chrome/{chrome_ver})")
            except Exception as _e:
                self.logger.warning(f"UA/userAgentData åŒæ­¥å¤±è´¥ï¼ˆä½¿ç”¨é»˜è®¤ï¼‰: {_e}")

            self.logger.info("âœ“ æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸï¼ˆCDPåæ£€æµ‹å·²æ³¨å…¥ï¼Œé¡µé¢åŠ è½½å‰ç”Ÿæ•ˆï¼‰")
            return page

        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            raise
    
    def wait_for_login(self, page, wait_seconds=30):
        """
        ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨æ‰«ç ç™»å½•
        
        Args:
            page: æµè§ˆå™¨é¡µé¢å¯¹è±¡
            wait_seconds: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.logger.info("=" * 60)
        self.logger.info(f"è¯·åœ¨ {wait_seconds} ç§’å†…å®Œæˆæ‰«ç ç™»å½•!")
        self.logger.info("=" * 60)

        # åœ¨ç­‰å¾…è¿‡ç¨‹ä¸­æ¯éš”10ç§’é‡æ–°æ³¨å…¥ä¸€æ¬¡ webdriver è¦†ç›–ï¼ˆé˜²æ­¢BOSSé¡µé¢çš„JSæŠŠå®ƒè¿˜åŸï¼‰
        _wd_patch = (
            "try { Object.defineProperty(navigator, 'webdriver', "
            "{get: () => undefined, configurable: true, enumerable: false}); } catch(e) {}"
        )

        # å€’è®¡æ—¶æ˜¾ç¤ºï¼ˆæ¯è½®æ£€æµ‹å¹¶æ‰“å° webdriver + å½“å‰ URLï¼Œç”¨äºæ’æŸ¥æ˜¯å¦è¢«æ£€æµ‹åˆ°ï¼‰
        for remaining in range(wait_seconds, 0, -10):
            try:
                cur_url = page.url or ''
                wd_val  = page.run_js('return navigator.webdriver')
                wd_str  = repr(wd_val)
                status  = "âœ“ éšè—æˆåŠŸ" if not wd_val else "âš ï¸ ä»ä¸º trueï¼"
                self.logger.info(f"â° å‰©ä½™ {remaining}s | URL={cur_url[:60]} | webdriver={wd_str} {status}")
                if wd_val:
                    # webdriver æš´éœ²ï¼Œç«‹åˆ»é‡æ–°æ³¨å…¥
                    page.run_js(_wd_patch)
            except Exception as _e:
                self.logger.info(f"â° å‰©ä½™ {remaining}sï¼ˆçŠ¶æ€æ£€æµ‹å¼‚å¸¸: {_e}ï¼‰")
            time.sleep(10)
        
        self.logger.info("âœ“ ç™»å½•ç­‰å¾…æ—¶é—´ç»“æŸ")
        self.human_like_delay(2, 3)
    
    def parse_job_list(self, page):
        """
        è§£æèŒä½åˆ—è¡¨é¡µé¢
        
        Returns:
            list: èŒä½æ•°æ®åˆ—è¡¨
        """
        jobs = []
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            page.wait.doc_loaded(timeout=10)
            self.human_like_delay(2, 3)
            

            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„HTMLç»“æ„æ¥ç¼–å†™é€‰æ‹©å™¨
            # ç¤ºä¾‹ä»£ç ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰ï¼š
            
            # æŸ¥æ‰¾æ‰€æœ‰èŒä½å¡ç‰‡
            job_cards = page.eles('css:.job-card-wrapper')  # ç¤ºä¾‹é€‰æ‹©å™¨ï¼Œéœ€è¦æ ¹æ®å®é™…ä¿®æ”¹
            
            if not job_cards:
                self.logger.warning("æœªæ‰¾åˆ°èŒä½å¡ç‰‡å…ƒç´ ")
                return jobs
            
            self.logger.info(f"æ‰¾åˆ° {len(job_cards)} ä¸ªèŒä½å¡ç‰‡")
            
            for idx, card in enumerate(job_cards, 1):
                try:

                    # ä»¥ä¸‹æ˜¯ç¤ºä¾‹ä»£ç ï¼Œéœ€è¦æ ¹æ®BOSSç›´è˜å®é™…ç»“æ„ä¿®æ”¹
                    
                    job_data = {
                        'job_title': self.safe_get_text(card, '.job-title'),  # ç¤ºä¾‹
                        'company_name': self.safe_get_text(card, '.company-name'),  # ç¤ºä¾‹
                        'salary': self.safe_get_text(card, '.salary'),  # ç¤ºä¾‹
                        'location': self.safe_get_text(card, '.job-area'),  # ç¤ºä¾‹
                        'experience': self.safe_get_text(card, '.job-experience'),  # ç¤ºä¾‹
                        'education': self.safe_get_text(card, '.job-degree'),  # ç¤ºä¾‹
                        'job_tags': self.safe_get_text(card, '.tag-list'),  # ç¤ºä¾‹
                        'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    jobs.append(job_data)
                    self.logger.debug(f"è§£æèŒä½ {idx}: {job_data.get('job_title', 'Unknown')}")
                    
                except Exception as e:
                    self.logger.warning(f"è§£æç¬¬ {idx} ä¸ªèŒä½å¡ç‰‡å¤±è´¥: {e}")
                    continue
            
            self.logger.info(f"âœ“ æˆåŠŸè§£æ {len(jobs)} ä¸ªèŒä½")
            
        except Exception as e:
            self.logger.error(f"è§£æèŒä½åˆ—è¡¨å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
        
        return jobs
    
    def safe_get_text(self, element, selector):
        """å®‰å…¨åœ°è·å–å…ƒç´ æ–‡æœ¬"""
        try:
            target = element.ele(selector)
            if target:
                return target.text.strip()
        except:
            pass
        return ""
    
    def save_to_csv(self, jobs, keyword):
        """
        å°†èŒä½æ•°æ®ä¿å­˜åˆ°CSVæ–‡ä»¶
        
        Args:
            jobs: èŒä½æ•°æ®åˆ—è¡¨
            keyword: æœç´¢å…³é”®è¯
        """
        if not jobs:
            self.logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{self.data_dir}/boss_{keyword}_{timestamp}.csv"
            
            # è·å–æ‰€æœ‰å­—æ®µå
            fieldnames = list(jobs[0].keys())
            
            with open(filename, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(jobs)
            
            self.logger.info(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            self.logger.info(f"âœ“ å…±ä¿å­˜ {len(jobs)} æ¡èŒä½æ•°æ®")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
    
    def save_to_json(self, jobs, keyword):
        """
        å°†èŒä½æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶
        
        Args:
            jobs: èŒä½æ•°æ®åˆ—è¡¨
            keyword: æœç´¢å…³é”®è¯
        """
        if not jobs:
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{self.data_dir}/boss_{keyword}_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(jobs, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ“ JSONæ•°æ®å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
    
    def check_risk_control(self, page):
        """
        æ£€æµ‹æ˜¯å¦è§¦å‘é£æ§ï¼ˆéªŒè¯ç ã€ç™»å½•è¿‡æœŸç­‰ï¼‰
        
        æ£€æµ‹å†…å®¹ï¼š
          1. éªŒè¯ç é¡µé¢
          2. ç™»å½•è¿‡æœŸ
          3. è®¿é—®é™åˆ¶
          4. IPå°ç¦
        
        Returns:
            tuple: (is_blocked, reason)
              - is_blocked: boolï¼ŒTrueè¡¨ç¤ºè¢«é£æ§
              - reason: strï¼Œé£æ§åŸå› 
        """
        try:
            page_text = page.html
            page_url = page.url
            page_title = page.title
            
            # æ£€æµ‹1ï¼šéªŒè¯ç ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯ï¼Œé¿å…è¯¯åˆ¤ï¼‰
            # ä¼˜å…ˆæ£€æµ‹ä¸­æ–‡æç¤ºï¼ˆæ›´å‡†ç¡®ï¼‰
            captcha_keywords_high_priority = [
                'è¯·å®Œæˆå®‰å…¨éªŒè¯',
                'å®‰å…¨éªŒè¯ä¸­',
                'æ»‘åŠ¨éªŒè¯ç ',
                'ç‚¹å‡»å®ŒæˆéªŒè¯',
                'è¯·æ‹–åŠ¨æ»‘å—',
                'éªŒè¯ç éªŒè¯'
            ]
            for keyword in captcha_keywords_high_priority:
                if keyword in page_text:
                    self.logger.warning(f"æ£€æµ‹åˆ°é«˜ä¼˜å…ˆçº§éªŒè¯ç å…³é”®è¯: {keyword}")
                    return True, f"è§¦å‘éªŒè¯ç : {keyword}"
            
            # æ£€æµ‹æ ‡é¢˜ä¸­çš„éªŒè¯ç æç¤º
            if page_title and ('éªŒè¯' in page_title or 'captcha' in page_title.lower()):
                self.logger.warning(f"æ£€æµ‹åˆ°é¡µé¢æ ‡é¢˜åŒ…å«éªŒè¯ç : {page_title}")
                return True, f"é¡µé¢æ ‡é¢˜åŒ…å«éªŒè¯ç : {page_title}"
            
            # æ£€æµ‹URLä¸­çš„éªŒè¯ç æ ‡è¯†ï¼ˆæ›´å¯é ï¼‰
            if 'captcha' in page_url.lower() or 'verify' in page_url.lower():
                self.logger.warning(f"æ£€æµ‹åˆ°URLåŒ…å«éªŒè¯ç æ ‡è¯†: {page_url}")
                return True, f"URLåŒ…å«éªŒè¯ç : {page_url}"
            
            # æ£€æµ‹2ï¼šç™»å½•è¿‡æœŸï¼ˆä¼˜åŒ–ï¼šæ’é™¤æ­£å¸¸çš„ç™»å½•æŒ‰é’®ï¼‰
            login_keywords = [
                'ç™»å½•å·²è¿‡æœŸ',
                'éœ€è¦é‡æ–°ç™»å½•',
                'è¯·å…ˆç™»å½•åå†',
                'ç™»å½•çŠ¶æ€å¤±æ•ˆ'
            ]
            for keyword in login_keywords:
                if keyword in page_text:
                    self.logger.warning(f"æ£€æµ‹åˆ°ç™»å½•è¿‡æœŸå…³é”®è¯: {keyword}")
                    return True, f"ç™»å½•è¿‡æœŸ: {keyword}"
            
            # æ£€æµ‹3ï¼šè®¿é—®é™åˆ¶
            limit_keywords = [
                'è®¿é—®è¿‡äºé¢‘ç¹',
                'è¯·ç¨åå†è¯•',
                'ç³»ç»Ÿç¹å¿™',
                'è®¿é—®å—é™',
                'æ“ä½œå¤ªé¢‘ç¹',
                'è¯·æ±‚è¿‡äºé¢‘ç¹'
            ]
            for keyword in limit_keywords:
                if keyword in page_text:
                    self.logger.warning(f"æ£€æµ‹åˆ°è®¿é—®é™åˆ¶å…³é”®è¯: {keyword}")
                    return True, f"è®¿é—®é™åˆ¶: {keyword}"
            
            # æ£€æµ‹4ï¼šå¼‚å¸¸è·³è½¬ï¼ˆå¦‚è·³è½¬åˆ°é¦–é¡µã€é”™è¯¯é¡µï¼‰
            # ä¼˜åŒ–ï¼šåªæ£€æµ‹æ˜ç¡®çš„é”™è¯¯é¡µé¢ï¼Œæ’é™¤æ­£å¸¸é¦–é¡µ
            if '/error' in page_url.lower() or page_url.endswith('/404') or page_url.endswith('/403'):
                self.logger.warning(f"æ£€æµ‹åˆ°å¼‚å¸¸è·³è½¬: {page_url}")
                return True, f"å¼‚å¸¸è·³è½¬: {page_url}"
            
            return False, ""
            
        except Exception as e:
            self.logger.debug(f"é£æ§æ£€æµ‹å¤±è´¥: {e}")
            return False, ""
    
    def check_if_reached_bottom(self, page):
        """
        æ£€æµ‹æ˜¯å¦å·²åˆ°è¾¾é¡µé¢åº•éƒ¨
        
        æ£€æµ‹æ–¹æ³•ï¼š
          1. æ£€æŸ¥æ˜¯å¦å‡ºç°"å·²ç»åˆ°åº•äº†"æç¤º
          2. æ£€æŸ¥æ»šåŠ¨ä½ç½®æ˜¯å¦ä¸å†å˜åŒ–
          3. æ£€æŸ¥æ˜¯å¦æœ‰"æ²¡æœ‰æ›´å¤šæ•°æ®"çš„æ ‡è¯†
        
        Returns:
            bool: Trueè¡¨ç¤ºå·²åˆ°åº•ï¼ŒFalseè¡¨ç¤ºæœªåˆ°åº•
        """
        try:
            # æ–¹æ³•1ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰"å·²ç»åˆ°åº•äº†"ã€"æ²¡æœ‰æ›´å¤šäº†"ç­‰æç¤º
            bottom_texts = [
                'å·²ç»åˆ°åº•äº†',
                'æ²¡æœ‰æ›´å¤šäº†',
                'æš‚æ— æ›´å¤šèŒä½',
                'å·²åŠ è½½å…¨éƒ¨',
                'åˆ°åº•å•¦'
            ]
            
            page_text = page.html
            for text in bottom_texts:
                if text in page_text:
                    self.logger.info(f"âœ“ æ£€æµ‹åˆ°åº•éƒ¨æç¤º: {text}")
                    return True
            
            # æ–¹æ³•2ï¼šæ£€æŸ¥æ»šåŠ¨ä½ç½®
            scroll_info = page.run_js("""
                return {
                    scrollTop: document.documentElement.scrollTop,
                    scrollHeight: document.documentElement.scrollHeight,
                    clientHeight: document.documentElement.clientHeight
                };
            """)
            
            if scroll_info:
                scroll_top = scroll_info.get('scrollTop', 0)
                scroll_height = scroll_info.get('scrollHeight', 0)
                client_height = scroll_info.get('clientHeight', 0)
                
                # å¦‚æœæ»šåŠ¨ä½ç½® + å¯è§†é«˜åº¦ >= æ€»é«˜åº¦ - 100pxï¼Œè®¤ä¸ºåˆ°åº•äº†
                if scroll_top + client_height >= scroll_height - 100:
                    self.logger.info(f"âœ“ æ£€æµ‹åˆ°é¡µé¢åº•éƒ¨ï¼ˆæ»šåŠ¨ä½ç½®ï¼‰")
                    return True
            
            return False
            
        except Exception as e:
            self.logger.debug(f"æ£€æµ‹åº•éƒ¨å¤±è´¥: {e}")
            return False
    
    def scroll_to_bottom(self, page, scroll_times=55):
        """
        æ™ºèƒ½ä¸‹æ»‘ç­–ç•¥ v1.6 - æ›´é¢‘ç¹åº•éƒ¨æ£€æµ‹ + æ›´å¤§æ»šåŠ¨æ­¥é•¿
        
        ä¼˜åŒ–ç‚¹ï¼ˆç›¸è¾ƒ v1.5ï¼‰ï¼š
          - åº•éƒ¨æ£€æµ‹é—´éš”ï¼š5æ¬¡ â†’ 3æ¬¡ï¼ˆæ›´æ—©å‘ç°åˆ°åº•ï¼ŒèŠ‚çœæ— æ•ˆæ»šåŠ¨ï¼‰
          - å•æ¬¡æ»šåŠ¨è·ç¦»ï¼š500-800px â†’ 600-1000pxï¼ˆæ­¥é•¿æ›´å¤§ï¼Œæ›´å¿«åˆ°åº•ï¼‰
          - é¢å¤–åœé¡¿é—´éš”ï¼š10-15æ¬¡ â†’ 12-18æ¬¡ï¼ˆä¿ç•™éšæœºæ€§åŒæ—¶å‡å°‘åœé¡¿æ¬¡æ•°ï¼‰
          - ä¿ç•™å…¨éƒ¨é£æ§ä¿æŠ¤ï¼šéšæœºè·ç¦»/å»¶è¿Ÿ/å¶å‘åœé¡¿
        
        æ€§èƒ½ä¼°ç®—ï¼ˆå¯¹æ¯” v1.5ï¼‰ï¼š
          - æ›´å¤§æ­¥é•¿ï¼šæ›´æ—©è§¦åº•ï¼Œå¹³å‡èŠ‚çœ 5-15 æ¬¡æ»šåŠ¨
          - æ›´é¢‘ç¹æ£€æµ‹ï¼šè§¦åº•åæœ€å¤šé¢å¤–3æ¬¡æ‰åœï¼Œè€Œéæœ€å¤š5æ¬¡
          - ç»¼åˆèŠ‚çœï¼šæ¯å…³é”®è¯çº¦ 5-20 ç§’

        Args:
            page: æµè§ˆå™¨é¡µé¢å¯¹è±¡
            scroll_times: æœ€å¤§ä¸‹æ»‘æ¬¡æ•°
        """
        try:
            self.logger.info(f"å¼€å§‹ä¸‹æ»‘é¡µé¢ï¼Œæœ€å¤š {scroll_times} æ¬¡ï¼ˆæ™ºèƒ½æ£€æµ‹æ¨¡å¼ v1.6ï¼‰...")

            last_scroll_top = 0
            no_change_count = 0

            for i in range(scroll_times):
                # æ¯3æ¬¡æ£€æµ‹ä¸€æ¬¡æ˜¯å¦åˆ°åº•ï¼ˆv1.5 æ˜¯æ¯5æ¬¡ï¼Œæ›´æ—©æ„ŸçŸ¥åº•éƒ¨ï¼‰
                if i > 0 and i % 3 == 0:
                    current_scroll_top = page.run_js("return document.documentElement.scrollTop;")

                    if current_scroll_top == last_scroll_top:
                        no_change_count += 1
                        self.logger.debug(f"æ»šåŠ¨ä½ç½®æœªå˜åŒ–ï¼ˆ{no_change_count}æ¬¡ï¼‰")

                        if no_change_count >= 3:
                            early_stop_msg = f"âœ“ æ£€æµ‹åˆ°é¡µé¢åº•éƒ¨ï¼ˆç¬¬{i+1}æ¬¡ä¸‹æ»‘ï¼‰ï¼Œæå‰åœæ­¢"
                            print(f"\nğŸ¯ {early_stop_msg}")
                            self.logger.info(early_stop_msg)
                            self.logger.info(f"èŠ‚çœä¸‹æ»‘æ¬¡æ•°: {scroll_times - i - 1} æ¬¡")
                            break
                    else:
                        no_change_count = 0
                        last_scroll_top = current_scroll_top

                    # æ£€æµ‹åº•éƒ¨æç¤ºæ–‡å­—
                    if self.check_if_reached_bottom(page):
                        early_stop_msg = f"âœ“ æ£€æµ‹åˆ°åº•éƒ¨æç¤ºï¼ˆç¬¬{i+1}æ¬¡ä¸‹æ»‘ï¼‰ï¼Œæå‰åœæ­¢"
                        print(f"\nğŸ¯ {early_stop_msg}")
                        self.logger.info(early_stop_msg)
                        self.logger.info(f"èŠ‚çœä¸‹æ»‘æ¬¡æ•°: {scroll_times - i - 1} æ¬¡")
                        break

                # å•æ¬¡æ»šåŠ¨è·ç¦»ï¼š600-1000pxï¼ˆv1.5 æ˜¯ 500-800pxï¼Œæ­¥é•¿æ›´å¤§è§¦åº•æ›´å¿«ï¼‰
                scroll_distance = random.randint(600, 1000)

                page.run_js(f"window.scrollBy(0, {scroll_distance});")

                if (i + 1) % 10 == 0 or i == 0 or i == scroll_times - 1:
                    self.logger.info(f"âœ“ ç¬¬ {i+1}/{scroll_times} æ¬¡ä¸‹æ»‘å®Œæˆ")

                # å»¶è¿Ÿç­–ç•¥ï¼ˆä¿æŒä¸ v1.5 ç›¸åŒï¼Œå·²æ˜¯æœ€ä¼˜çš„éšæœºåŒºé—´ï¼‰
                if self.fast_scroll_mode:
                    if i < scroll_times * 0.2:
                        delay = random.uniform(0.3, 0.5)
                    elif i < scroll_times * 0.8:
                        delay = random.uniform(0.4, 0.7)
                    else:
                        delay = random.uniform(0.3, 0.6)
                else:
                    if i < scroll_times * 0.3:
                        delay = random.uniform(1.0, 2.0)
                    elif i < scroll_times * 0.7:
                        delay = random.uniform(2.0, 4.0)
                    else:
                        delay = random.uniform(1.2, 2.5)

                time.sleep(delay)

                # å¶å‘æ€§é¢å¤–åœé¡¿ï¼ˆé£æ§ä¿æŠ¤ï¼‰ï¼šé—´éš”æ‹‰é•¿åˆ° 12-18 æ¬¡ï¼ˆv1.5 æ˜¯ 10-15 æ¬¡ï¼‰
                if (i + 1) % random.randint(12, 18) == 0:
                    extra_delay = random.uniform(1.5, 3.0)
                    self.logger.debug(f"ç¬¬ {i+1} æ¬¡ä¸‹æ»‘ï¼Œé¢å¤–åœé¡¿ {extra_delay:.2f} ç§’...")
                    time.sleep(extra_delay)

            self.logger.info("âœ“ é¡µé¢ä¸‹æ»‘å®Œæˆ")

        except Exception as e:
            self.logger.error(f"ä¸‹æ»‘é¡µé¢å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
    
    def crawl_city_keyword(self, page, city_name, city_code, keyword):
        """
        æŠ“å–æŒ‡å®šåŸå¸‚å’Œå…³é”®è¯çš„èŒä½æ•°æ®ï¼ˆé€šè¿‡ç›´æ¥è®¿é—®URLï¼‰
        
        Args:
            page: æµè§ˆå™¨é¡µé¢å¯¹è±¡
            city_name: åŸå¸‚åç§°ï¼ˆå¦‚"åŒ—äº¬"ï¼‰
            city_code: åŸå¸‚ç¼–å·ï¼ˆå¦‚"101010100"ï¼‰
            keyword: æœç´¢å…³é”®è¯
        
        Returns:
            tuple: (success, jobs, error_reason)
              - success: boolï¼Œæ˜¯å¦æˆåŠŸ
              - jobs: listï¼ŒæŠ“å–åˆ°çš„èŒä½æ•°æ®
              - error_reason: strï¼Œå¤±è´¥åŸå› ï¼ˆæˆåŠŸæ—¶ä¸ºç©ºï¼‰
        """
        all_jobs = []
        
        try:
            # ä»»åŠ¡å¼€å§‹æç¤ºï¼ˆæ§åˆ¶å°+æ—¥å¿—ï¼‰
            task_header = "=" * 70
            task_title = f"ğŸš€ å¼€å§‹ä»»åŠ¡: {city_name} - {keyword}"
            print(f"\n{task_header}")
            print(task_title)
            print(task_header)
            
            self.logger.info(f"{'='*60}")
            self.logger.info(f"åŸå¸‚: {city_name} ({city_code}) - å…³é”®è¯: {keyword}")
            self.logger.info(f"{'='*60}")
            
            # ========== æ­¥éª¤1: å¯åŠ¨ç½‘ç»œç›‘å¬ ==========
            # âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨è®¿é—®URLä¹‹å‰å¯åŠ¨ç›‘å¬ï¼Œå¦åˆ™ä¼šæ¼æ‰ç¬¬ä¸€é¡µæ•°æ®
            # ç›‘å¬BOSSç›´è˜èŒä½åˆ—è¡¨API
            target_packet_name = 'wapi/zpgeek/search/joblist'
            
            self.logger.info(f"å¯åŠ¨ç½‘ç»œç›‘å¬ï¼Œç›®æ ‡åŒ…: {target_packet_name}")
            
            # ç›‘å¬joblistæ¥å£
            page.listen.start(target_packet_name)
            self.logger.info("âœ“ ç›‘å¬å·²å¯åŠ¨")
            
            # ========== æ­¥éª¤2: ç›´æ¥è®¿é—®ç›®æ ‡URL ==========
            # æ„å»ºURLï¼šhttps://www.zhipin.com/web/geek/jobs?city={city_code}&query={keyword}
            # æ³¨æ„ï¼šä½¿ç”¨æµè§ˆå™¨åŸç”Ÿç¼–ç ï¼Œä¸æ‰‹åŠ¨ç¼–ç ä¸­æ–‡ï¼ˆæ›´è‡ªç„¶ï¼‰
            target_url = f"https://www.zhipin.com/web/geek/jobs?city={city_code}&query={keyword}"
            
            self.logger.info(f"è®¿é—®URL: {target_url}")
            page.get(target_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            self.logger.info("ç­‰å¾…é¡µé¢åŠ è½½ï¼ˆç›‘å¬ä¼šæ•è·ç¬¬ä¸€é¡µæ•°æ®åŒ…ï¼‰...")
            page.wait.doc_loaded(timeout=15)
            self.human_like_delay(2, 3)

            # ===== å…³é”®ï¼šéªŒè¯å½“å‰ URLï¼Œæ£€æµ‹æ˜¯å¦è¢«é‡å®šå‘å›é¦–é¡µ =====
            current_url = page.url or ''
            self.logger.info(f"å½“å‰é¡µé¢URL: {current_url}")
            if 'geek/jobs' not in current_url and 'zpgeek' not in current_url:
                redirect_msg = f"âš ï¸ è¢«é‡å®šå‘ï¼ç›®æ ‡={target_url}ï¼Œå®é™…={current_url}"
                self.logger.warning(redirect_msg)
                return False, [], redirect_msg

            # ã€æ–°å¢ã€‘æ£€æµ‹é£æ§
            if self.enable_risk_detection:
                is_blocked, reason = self.check_risk_control(page)
                if is_blocked:
                    error_msg = f"âš ï¸  æ£€æµ‹åˆ°é£æ§: {reason}"
                    print(f"\n{error_msg}")
                    self.logger.error(error_msg)
                    return False, [], reason
            
            # ã€ä¼˜åŒ–ã€‘å‡å°‘åˆå§‹æµè§ˆå»¶è¿Ÿ
            browse_delay = random.uniform(0.8, 1.5)  # ä¼˜åŒ–ï¼š1-2ç§’ â†’ 0.8-1.5ç§’
            self.logger.info(f"æ¨¡æ‹Ÿæµè§ˆé¡µé¢ {browse_delay:.2f} ç§’...")
            time.sleep(browse_delay)
            
            # ã€ä¼˜åŒ–ã€‘ç®€åŒ–äººç±»äº¤äº’æ¨¡æ‹Ÿï¼ˆå¯é€‰ï¼‰
            if random.random() < 0.3:  # åªæœ‰30%æ¦‚ç‡æ‰§è¡Œï¼Œå‡å°‘è€—æ—¶
                self.simulate_human_interaction(page)
            
            # ========== æ­¥éª¤3: ä¸‹æ»‘é¡µé¢è§¦å‘æ›´å¤šæ•°æ®åŠ è½½ ==========
            # âš ï¸ é‡è¦ï¼šBossç›´è˜æ¯ä¸ªå…³é”®è¯çº¦300æ¡æ•°æ®ï¼Œéœ€è¦ä¸‹æ»‘55æ¬¡æ‰èƒ½å®Œå…¨åŠ è½½
            # æµ‹è¯•æ¨¡å¼ï¼šåªä¸‹æ»‘5æ¬¡ï¼Œå¿«é€ŸéªŒè¯åŠŸèƒ½
            if self.test_mode:
                actual_scroll_times = self.test_scroll_times
                scroll_msg = f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä¸‹æ»‘ {actual_scroll_times} æ¬¡ï¼ˆæ­£å¸¸æ¨¡å¼55æ¬¡ï¼‰"
                print(scroll_msg)
                self.logger.info(scroll_msg)
            else:
                # éšæœºè°ƒæ•´ä¸‹æ»‘æ¬¡æ•°ï¼Œé¿å…æ¨¡å¼è¯†åˆ«
                actual_scroll_times = random.randint(self.scroll_times_per_keyword - 3, self.scroll_times_per_keyword + 3)
                scroll_msg = f"â¬‡ï¸  å¼€å§‹ä¸‹æ»‘é¡µé¢è§¦å‘æ•°æ®åŠ è½½ï¼ˆå…±{actual_scroll_times}æ¬¡ï¼‰..."
                print(scroll_msg)
                self.logger.info(f"å¼€å§‹ä¸‹æ»‘é¡µé¢è§¦å‘æ›´å¤šæ•°æ®åŠ è½½ï¼ˆå…±{actual_scroll_times}æ¬¡ï¼‰...")
            
            self.scroll_to_bottom(page, scroll_times=actual_scroll_times)
            print(f"âœ… ä¸‹æ»‘å®Œæˆï¼")
            
            # ========== æ­¥éª¤4: è·å–ç›‘å¬åˆ°çš„æ•°æ®åŒ… ==========
            print(f"ğŸ“¦ ç­‰å¾…å¹¶è·å–æ•°æ®åŒ…...")
            self.logger.info("ç­‰å¾…å¹¶è·å–æ•°æ®åŒ…...")
            time.sleep(1.0)  # ä¼˜åŒ–ï¼š1.5ç§’ â†’ 1.0ç§’ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
            
            # è·å–æ‰€æœ‰ç›‘å¬åˆ°çš„æ•°æ®åŒ…ï¼ˆé‡è¦ï¼šå¿…é¡»åœ¨åœæ­¢ç›‘å¬ä¹‹å‰è·å–ï¼‰
            try:
                print(f"ğŸ” æ­£åœ¨æå–æ•°æ®åŒ…...")
                self.logger.info("æ­£åœ¨æå–æ•°æ®åŒ…...")
                
                # ä½¿ç”¨wait()æ–¹æ³•è·å–æ•°æ®åŒ…ï¼Œè®¾ç½®timeouté¿å…æ— é™ç­‰å¾…
                packets = []
                packet_count = 0
                
                # æŒç»­è·å–æ•°æ®åŒ…ï¼Œç›´åˆ°è¶…æ—¶ï¼ˆæ²¡æœ‰æ–°æ•°æ®ï¼‰
                while True:
                    try:
                        # ç­‰å¾…ä¸‹ä¸€ä¸ªæ•°æ®åŒ…ï¼Œè¶…æ—¶æ—¶é—´1ç§’
                        packet = page.listen.wait(timeout=1)
                        
                        if packet:
                            packet_count += 1
                            packets.append(packet)
                            self.logger.debug(f"æå–ç¬¬ {packet_count} ä¸ªæ•°æ®åŒ…")
                        else:
                            # æ²¡æœ‰æ›´å¤šæ•°æ®åŒ…
                            self.logger.info("æ²¡æœ‰æ›´å¤šæ•°æ®åŒ…")
                            break
                            
                        # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šè·å–100ä¸ªåŒ…
                        if packet_count >= 100:
                            self.logger.warning("æ•°æ®åŒ…æ•°é‡è¾¾åˆ°100ï¼Œåœæ­¢æå–")
                            break
                            
                    except Exception as e:
                        # è¶…æ—¶æˆ–å…¶ä»–å¼‚å¸¸ï¼Œè¡¨ç¤ºæ²¡æœ‰æ›´å¤šæ•°æ®
                        if "timeout" in str(e).lower() or "è¶…æ—¶" in str(e):
                            self.logger.info(f"æ•°æ®åŒ…è·å–å®Œæˆï¼ˆè¶…æ—¶ï¼‰")
                        else:
                            self.logger.warning(f"æå–æ•°æ®åŒ…å¼‚å¸¸: {e}")
                        break
                
            except Exception as e:
                self.logger.error(f"è·å–æ•°æ®åŒ…æ—¶å‡ºé”™: {e}")
                self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                packets = []
            
            # åœæ­¢ç›‘å¬ï¼ˆåœ¨è·å–æ•°æ®ä¹‹åï¼‰
            try:
                self.logger.info("åœæ­¢ç›‘å¬...")
                page.listen.stop()
            except Exception as e:
                self.logger.warning(f"åœæ­¢ç›‘å¬æ—¶å‡ºé”™: {e}")
            
            # ç»Ÿè®¡å’Œæ˜¾ç¤ºç»“æœ
            if packets:
                packet_msg = f"âœ“ æˆåŠŸæ•è· {len(packets)} ä¸ªæ•°æ®åŒ…"
                print(f"âœ… {packet_msg}")
                self.logger.info(packet_msg)
            else:
                warning_msg = f"æœªæ•è·åˆ°ç›®æ ‡æ•°æ®åŒ…: {target_packet_name}"
                print(f"âš ï¸  {warning_msg}")
                self.logger.warning(warning_msg)
                self.logger.info("æç¤ºï¼šè¯·æ£€æŸ¥ç½‘ç»œè¯·æ±‚æ˜¯å¦æ­£å¸¸ï¼Œæˆ–å°è¯•å¢åŠ ç­‰å¾…æ—¶é—´")
                
                # ã€æ–°å¢ã€‘æ²¡æœ‰æ•°æ®åŒ…ï¼Œå¯èƒ½æ˜¯é£æ§ï¼Œå†æ¬¡æ£€æµ‹
                if self.enable_risk_detection:
                    is_blocked, reason = self.check_risk_control(page)
                    if is_blocked:
                        error_msg = f"âš ï¸  æ£€æµ‹åˆ°é£æ§: {reason}"
                        print(f"\n{error_msg}")
                        self.logger.error(error_msg)
                        return False, [], reason
                
                # æ²¡æœ‰æ•°æ®åŒ…ï¼Œè¿”å›å¤±è´¥
                return False, [], "æœªæ•è·åˆ°æ•°æ®åŒ…"
            
            # ========== æ­¥éª¤5: æå–å“åº”æ•°æ® ==========
            if packets:
                for idx, packet in enumerate(packets, 1):
                    self.logger.debug(f"æå–æ•°æ®åŒ… {idx}/{len(packets)}")
                    
                    # æå–å“åº”æ•°æ®
                    try:
                        response_body = packet.response.body
                        
                        # å¦‚æœæ˜¯JSONæ ¼å¼ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                        if isinstance(response_body, (dict, list)):
                            all_jobs.append(response_body)
                        else:
                            # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                            try:
                                data = json.loads(response_body)
                                all_jobs.append(data)
                            except:
                                self.logger.warning(f"æ•°æ®åŒ… {idx} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                        
                    except Exception as e:
                        self.logger.warning(f"æå–æ•°æ®åŒ… {idx} å¤±è´¥: {e}")
            
            # ä»»åŠ¡å®Œæˆæç¤ºï¼ˆæ§åˆ¶å°+æ—¥å¿—ï¼‰
            complete_msg = f"âœ… {city_name}-{keyword} æŠ“å–å®Œæˆï¼æ•è·æ•°æ®åŒ…: {len(all_jobs)} ä¸ª"
            print(f"\n{complete_msg}\n")
            self.logger.info(f"âœ“ {city_name}-{keyword} æŠ“å–å®Œæˆ")
            self.logger.info(f"  - æ•è·æ•°æ®åŒ…: {len(all_jobs)} ä¸ª")
            
            # è¿”å›æˆåŠŸ
            return True, all_jobs, ""
            
        except Exception as e:
            error_msg = f"æŠ“å– {city_name}-{keyword} æ—¶å‡ºé”™: {e}"
            print(f"\nâŒ {error_msg}\n")
            self.logger.error(error_msg)
            self.logger.error(traceback.format_exc())
            
            # è¿”å›å¤±è´¥
            return False, [], str(e)
    
    def parse_job_data(self, response_data):
        """
        è§£æBossç›´è˜APIè¿”å›çš„èŒä½æ•°æ®
        
        Args:
            response_data: APIå“åº”æ•°æ®ï¼ˆå­—å…¸æˆ–JSONå­—ç¬¦ä¸²ï¼‰
        
        Returns:
            list: è§£æåçš„èŒä½åˆ—è¡¨
        """
        parsed_jobs = []
        
        try:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè§£ææˆå­—å…¸
            if isinstance(response_data, str):
                response_data = json.loads(response_data)
            
            # æå–jobList
            job_list = response_data.get('zpData', {}).get('jobList', [])
            
            if not job_list:
                self.logger.warning("æœªæ‰¾åˆ°jobListæ•°æ®")
                return parsed_jobs
            
            # éå†æ¯ä¸ªèŒä½
            for job in job_list:
                try:
                    # ========== åŸºç¡€ä¿¡æ¯ ==========
                    job_id = job.get('encryptJobId', '') or str(uuid.uuid4())
                    title = job.get('jobName', '').strip()
                    company = job.get('brandName', '').strip()
                    
                    # ========== åœ°ç†ä½ç½® ==========
                    city = job.get('cityName', '').strip()
                    district = job.get('areaDistrict', '').strip()
                    business_district = job.get('businessDistrict', '').strip()
                    
                    # ========== è–ªèµ„ä¿¡æ¯ ==========
                    salary_desc = job.get('salaryDesc', '').strip()
                    salary_min, salary_max = self.parse_salary(salary_desc)
                    
                    # ========== ä»»èŒè¦æ±‚ ==========
                    experience = job.get('jobExperience', '').strip()
                    education = job.get('jobDegree', '').strip()
                    
                    # ä»jobLabelsæå–ï¼ˆå¤‡ç”¨ï¼‰
                    job_labels = job.get('jobLabels', [])
                    if not experience and len(job_labels) > 0:
                        experience = job_labels[0]
                    if not education and len(job_labels) > 1:
                        education = job_labels[1]
                    
                    # ========== æŠ€èƒ½ä¿¡æ¯ ==========
                    skills = job.get('skills', [])
                    
                    # ========== å…¬å¸ä¿¡æ¯ ==========
                    company_size = job.get('brandScaleName', '').strip()
                    company_industry = job.get('brandIndustry', '').strip()
                    company_stage = job.get('brandStageName', '').strip()
                    
                    # ========== Bossä¿¡æ¯ ==========
                    boss_name = job.get('bossName', '').strip()
                    boss_title = job.get('bossTitle', '').strip()
                    
                    # ========== ç¦åˆ©ä¿¡æ¯ ==========
                    welfare_list = job.get('welfareList', [])
                    
                    # ========== å‘å¸ƒæ—¶é—´ ==========
                    publish_date = datetime.now().strftime('%Y-%m-%d')
                    
                    # ========== æ„å»ºæ ‡å‡†åŒ–æ•°æ® ==========
                    parsed_job = {
                        # èŒä½åŸºæœ¬ä¿¡æ¯
                        'job_id': job_id,
                        'title': title,
                        'company': company,
                        
                        # åœ°ç†ä½ç½®
                        'city': city,
                        'district': district,
                        'business_district': business_district,
                        
                        # è–ªèµ„ä¿¡æ¯
                        'salary_min': salary_min,
                        'salary_max': salary_max,
                        'salary_text': salary_desc,
                        
                        # ä»»èŒè¦æ±‚
                        'experience': experience,
                        'education': education,
                        
                        # æŠ€èƒ½åˆ—è¡¨ï¼ˆé‡è¦ï¼ç”¨äºæŠ€èƒ½å›¾è°±æ„å»ºï¼‰
                        'skills': skills,
                        
                        # å…¬å¸ä¿¡æ¯
                        'company_size': company_size,
                        'company_industry': company_industry,
                        'company_stage': company_stage,
                        
                        # Bossä¿¡æ¯
                        'boss_name': boss_name,
                        'boss_title': boss_title,
                        
                        # ç¦åˆ©ä¿¡æ¯
                        'welfare': welfare_list,
                        
                        # å‘å¸ƒä¿¡æ¯
                        'publish_date': publish_date,
                        'source': 'bossç›´è˜',
                        
                        # åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼Œä¾¿äºè°ƒè¯•ï¼‰
                        '_raw': {
                            'security_id': job.get('securityId', ''),
                            'lid': job.get('lid', ''),
                            'item_id': job.get('itemId', 0)
                        }
                    }
                    
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    if self.validate_job_data(parsed_job):
                        parsed_jobs.append(parsed_job)
                    else:
                        self.logger.warning(f"èŒä½æ•°æ®éªŒè¯å¤±è´¥: {title}")
                        
                except Exception as e:
                    self.logger.warning(f"è§£æå•ä¸ªèŒä½å¤±è´¥: {e}")
                    continue
            
            self.logger.info(f"âœ“ æˆåŠŸè§£æ {len(parsed_jobs)}/{len(job_list)} ä¸ªèŒä½")
            
        except Exception as e:
            self.logger.error(f"è§£æèŒä½æ•°æ®å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
        
        return parsed_jobs
    
    def parse_salary(self, salary_text):
        """
        è§£æè–ªèµ„æ–‡æœ¬
        
        Args:
            salary_text: è–ªèµ„æè¿°ï¼Œå¦‚ "20-30KÂ·16è–ª"ã€"15-25K"
        
        Returns:
            tuple: (æœ€ä½è–ªèµ„, æœ€é«˜è–ªèµ„)ï¼Œå•ä½ï¼šk
        """
        if not salary_text:
            return 0, 0
        
        try:
            # åŒ¹é…æ¨¡å¼ï¼š20-30Kã€20k-30kã€20-30kÂ·16è–ª
            import re
            pattern = r'(\d+)[kK]?-(\d+)[kK]?'
            match = re.search(pattern, salary_text)
            
            if match:
                salary_min = int(match.group(1))
                salary_max = int(match.group(2))
                return salary_min, salary_max
            else:
                # æ— æ³•è§£æï¼Œè¿”å›0
                return 0, 0
                
        except Exception as e:
            self.logger.warning(f"è§£æè–ªèµ„å¤±è´¥: {salary_text}, é”™è¯¯: {e}")
            return 0, 0
    
    def validate_job_data(self, job):
        """
        éªŒè¯èŒä½æ•°æ®æ˜¯å¦å®Œæ•´
        
        Args:
            job: èŒä½æ•°æ®å­—å…¸
        
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        # å¿…éœ€å­—æ®µ
        required_fields = ['job_id', 'title', 'company', 'city']
        
        for field in required_fields:
            if not job.get(field):
                self.logger.debug(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        # è‡³å°‘æœ‰è–ªèµ„æˆ–æŠ€èƒ½ä¿¡æ¯
        if not job.get('salary_text') and not job.get('skills'):
            self.logger.debug("ç¼ºå°‘è–ªèµ„å’ŒæŠ€èƒ½ä¿¡æ¯")
            return False
        
        return True
    
    def save_city_keyword_data(self, data, city_name, keyword):
        """
        ä¿å­˜æŒ‡å®šåŸå¸‚å’Œå…³é”®è¯çš„æ•°æ®åˆ°åŸå¸‚JSONæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ + å®æ—¶å»é‡ï¼‰
        
        æ–°æ–¹æ¡ˆï¼š
          - ä¸€ä¸ªåŸå¸‚ä¸€ä¸ªæ–‡ä»¶ï¼šdata/raw/boss_åŒ—äº¬.json
          - è¿½åŠ æ¨¡å¼ï¼šæ¯æŠ“å®Œä¸€ä¸ªå…³é”®è¯ï¼Œè¿½åŠ æ–°æ•°æ®åˆ°åŸå¸‚æ–‡ä»¶
          - å®æ—¶å»é‡ï¼šè¿½åŠ å‰æ£€æŸ¥job_idæ˜¯å¦å·²å­˜åœ¨
          - æ–­ç‚¹å®‰å…¨ï¼šä¸­æ–­åå·²ä¿å­˜æ•°æ®ä¸ä¸¢å¤±
        
        Args:
            data: æ•°æ®åˆ—è¡¨ï¼ˆAPIå“åº”æ•°æ®åŒ…åˆ—è¡¨ï¼‰
            city_name: åŸå¸‚åç§°
            keyword: å…³é”®è¯
        """
        if not data:
            self.logger.warning(f"{city_name}-{keyword}: æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        try:
            # ========== æ­¥éª¤1: è§£ææ‰€æœ‰æ•°æ®åŒ… ==========
            all_parsed_jobs = []
            
            print(f"\nğŸ”„ æ­£åœ¨è§£ææ•°æ®: {city_name} - {keyword} (å…± {len(data)} ä¸ªæ•°æ®åŒ…)")
            self.logger.info(f"å¼€å§‹è§£ææ•°æ®: {city_name} - {keyword}")
            
            for idx, response_data in enumerate(data, 1):
                self.logger.debug(f"è§£æç¬¬ {idx}/{len(data)} ä¸ªæ•°æ®åŒ…")
                parsed_jobs = self.parse_job_data(response_data)
                all_parsed_jobs.extend(parsed_jobs)
                print(f"  âœ“ å·²è§£æ {idx}/{len(data)} ä¸ªæ•°æ®åŒ… (å½“å‰åŒ…: {len(parsed_jobs)} æ¡, ç´¯è®¡: {len(all_parsed_jobs)} æ¡)")
            
            if not all_parsed_jobs:
                warning_msg = f"{city_name}-{keyword}: è§£æåæ²¡æœ‰æœ‰æ•ˆæ•°æ®"
                print(f"âš ï¸  {warning_msg}")
                self.logger.warning(warning_msg)
                return
            
            print(f"âœ… è§£æå®Œæˆï¼å…±è·å¾— {len(all_parsed_jobs)} æ¡æ•°æ®\n")
            
            # ========== æµ‹è¯•æ¨¡å¼ï¼šåªè§£æä¸ä¿å­˜ ==========
            if self.test_mode:
                test_msg = f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå·²è§£æ {len(all_parsed_jobs)} æ¡æ•°æ®ï¼ˆä¸ä¿å­˜ï¼‰"
                print(test_msg)
                self.logger.info(test_msg)
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                self.print_data_statistics(all_parsed_jobs, city_name, keyword)
                return
            
            # ========== æ­¥éª¤2: åŠ è½½åŸå¸‚æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨å†…å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤ç£ç›˜ IOï¼‰==========
            city_file = os.path.join(self.data_dir, f"boss_{city_name}.json")

            if city_name not in self.city_job_ids:
                # ç¬¬ä¸€æ¬¡å¤„ç†è¯¥åŸå¸‚ï¼šä»ç£ç›˜åŠ è½½ï¼ŒåŒæ—¶å»ºç«‹å†…å­˜ç¼“å­˜
                if os.path.exists(city_file):
                    with open(city_file, 'r', encoding='utf-8') as f:
                        existing_jobs = json.load(f)
                    self.city_job_ids[city_name] = {job['job_id'] for job in existing_jobs if 'job_id' in job}
                    self.city_data_cache[city_name] = existing_jobs  # ç¼“å­˜åˆ°å†…å­˜
                    print(f"ğŸ“‚ åŠ è½½å·²æœ‰æ•°æ®: {len(existing_jobs)} æ¡ (job_id: {len(self.city_job_ids[city_name])} ä¸ª)")
                else:
                    self.city_job_ids[city_name] = set()
                    existing_jobs = []
                    self.city_data_cache[city_name] = []  # åˆå§‹åŒ–ç©ºç¼“å­˜
                    print(f"ğŸ“ åˆ›å»ºæ–°æ–‡ä»¶: {city_file}")
            else:
                # åç»­å…³é”®è¯ï¼šç›´æ¥ä½¿ç”¨å†…å­˜ç¼“å­˜ï¼Œæ— éœ€ç£ç›˜ IO
                existing_jobs = self.city_data_cache.get(city_name, [])
            
            # ========== æ­¥éª¤3: å»é‡ï¼ˆåŸºäºjob_idï¼‰==========
            new_jobs = []
            duplicate_count = 0
            
            for job in all_parsed_jobs:
                job_id = job.get('job_id')
                if not job_id:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if job_id in self.city_job_ids[city_name]:
                    duplicate_count += 1
                else:
                    new_jobs.append(job)
                    self.city_job_ids[city_name].add(job_id)
            
            dedup_msg = f"å»é‡å¤„ç†: {len(all_parsed_jobs)} â†’ {len(new_jobs)} æ¡æ–°æ•°æ® (å»é™¤ {duplicate_count} æ¡é‡å¤)"
            print(f"ğŸ”§ {dedup_msg}")
            self.logger.info(dedup_msg)
            
            if not new_jobs:
                print(f"âš ï¸  æ‰€æœ‰æ•°æ®éƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
                return
            
            # ========== æ­¥éª¤4: åˆå¹¶å¹¶ä¿å­˜åˆ°åŸå¸‚æ–‡ä»¶ ==========
            all_jobs = existing_jobs + new_jobs

            # åŒæ­¥æ›´æ–°å†…å­˜ç¼“å­˜ï¼ˆä¸‹æ¬¡å…³é”®è¯ç›´æ¥ä»å†…å­˜è¯»ï¼Œæ— éœ€ç£ç›˜ IOï¼‰
            self.city_data_cache[city_name] = all_jobs

            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åˆ°åŸå¸‚æ–‡ä»¶...")

            # ä¸ä½¿ç”¨ indent=2ï¼šå¯¹äºå«æ•°ä¸‡æ¡æ•°æ®çš„å¤§æ–‡ä»¶ï¼Œæ— ç¼©è¿›å¯èŠ‚çœ 30-50% å†™å…¥æ—¶é—´å’Œç£ç›˜ç©ºé—´
            with open(city_file, 'w', encoding='utf-8') as f:
                json.dump(all_jobs, f, ensure_ascii=False)

            save_msg = f"âœ“ æ•°æ®å·²ä¿å­˜: {city_file}"
            count_msg = f"âœ“ æœ¬æ¬¡æ–°å¢: {len(new_jobs)} æ¡ | ç´¯è®¡: {len(all_jobs)} æ¡"
            print(f"âœ… {save_msg}")
            print(f"âœ… {count_msg}")
            self.logger.info(save_msg)
            self.logger.info(count_msg)
            
            # ========== æ­¥éª¤5: æ‰“å°æ•°æ®ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡æœ¬æ¬¡æ–°å¢ï¼‰==========
            self.print_data_statistics(new_jobs, city_name, keyword)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
    
    def print_data_statistics(self, jobs, city_name, keyword):
        """æ‰“å°æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—ï¼‰"""
        if not jobs:
            return
        
        try:
            # ç»Ÿè®¡æŠ€èƒ½
            all_skills = []
            for job in jobs:
                all_skills.extend(job.get('skills', []))
            
            skill_count = len(set(all_skills))
            skill_list = sorted(set(all_skills))[:10]  # å‰10ä¸ªæŠ€èƒ½
            
            # ç»Ÿè®¡è–ªèµ„
            salaries = [job.get('salary_max', 0) for job in jobs if job.get('salary_max', 0) > 0]
            avg_salary = sum(salaries) / len(salaries) if salaries else 0
            min_salary = min(salaries) if salaries else 0
            max_salary = max(salaries) if salaries else 0
            
            # ç»Ÿè®¡å…¬å¸
            companies = set([job.get('company', '') for job in jobs if job.get('company')])
            
            # ç»Ÿè®¡å­¦å†è¦æ±‚
            education_stats = {}
            for job in jobs:
                edu = job.get('education', 'ä¸é™')
                education_stats[edu] = education_stats.get(edu, 0) + 1
            
            # ç»Ÿè®¡ç»éªŒè¦æ±‚
            experience_stats = {}
            for job in jobs:
                exp = job.get('experience', 'ä¸é™')
                experience_stats[exp] = experience_stats.get(exp, 0) + 1
            
            # æ„å»ºç»Ÿè®¡ä¿¡æ¯
            stats_lines = [
                "",
                "=" * 70,
                f"ğŸ“Š æ•°æ®ç»Ÿè®¡ - {city_name} - {keyword}",
                "=" * 70,
                f"âœ“ èŒä½æ•°é‡: {len(jobs)} æ¡",
                f"âœ“ å…¬å¸æ•°é‡: {len(companies)} å®¶",
                f"âœ“ æŠ€èƒ½ç§ç±»: {skill_count} ç§",
                f"âœ“ è–ªèµ„èŒƒå›´: {min_salary}K ~ {max_salary}K (å¹³å‡: {avg_salary:.1f}K)" if avg_salary > 0 else "âœ“ è–ªèµ„èŒƒå›´: æ— æ•°æ®",
                f"âœ“ å­¦å†è¦æ±‚: {', '.join([f'{k}({v})' for k, v in sorted(education_stats.items(), key=lambda x: -x[1])[:3]])}",
                f"âœ“ ç»éªŒè¦æ±‚: {', '.join([f'{k}({v})' for k, v in sorted(experience_stats.items(), key=lambda x: -x[1])[:3]])}",
            ]
            
            if skill_list:
                stats_lines.append(f"âœ“ çƒ­é—¨æŠ€èƒ½: {', '.join(skill_list[:10])}" + ("..." if skill_count > 10 else ""))
            
            stats_lines.append("=" * 70)
            stats_lines.append("")
            
            # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—
            for line in stats_lines:
                print(line)  # æ§åˆ¶å°è¾“å‡º
                self.logger.info(line)  # æ—¥å¿—è¾“å‡º
            
        except Exception as e:
            error_msg = f"ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}"
            print(f"âš ï¸  {error_msg}")
            self.logger.warning(error_msg)
    
    def load_progress(self):
        """
        åŠ è½½æŠ“å–è¿›åº¦ï¼ˆå¸¦å†…å­˜ç¼“å­˜ï¼Œé¿å…æ¯ä¸ªä»»åŠ¡é‡å¤è¯»å–ç£ç›˜ï¼‰
        
        Returns:
            dict: è¿›åº¦ä¿¡æ¯ï¼ŒåŒ…å«å·²å®Œæˆçš„åŸå¸‚å’Œå…³é”®è¯
        """
        # æœ‰ç¼“å­˜ç›´æ¥è¿”å›ï¼Œæ— éœ€ç£ç›˜ IO
        if self._progress_cache is not None:
            return self._progress_cache

        _empty = {
            'completed_cities': [],
            'current_city': None,
            'completed_keywords': [],
            'last_update': None
        }
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                self.logger.info(f"âœ“ åŠ è½½è¿›åº¦æ–‡ä»¶: {self.progress_file}")
            else:
                self.logger.info("æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹æŠ“å–")
                progress = _empty
        except Exception as e:
            self.logger.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            progress = _empty

        self._progress_cache = progress
        return self._progress_cache
    
    def save_progress(self, city_name, keyword=None, city_completed=False):
        """
        ä¿å­˜æŠ“å–è¿›åº¦ï¼ˆåŒæ—¶æ›´æ–°å†…å­˜ç¼“å­˜ï¼Œå‡å°‘ä¸‹æ¬¡ load_progress çš„ç£ç›˜ IOï¼‰
        
        Args:
            city_name: å½“å‰åŸå¸‚åç§°
            keyword: å½“å‰å®Œæˆçš„å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            city_completed: å½“å‰åŸå¸‚æ˜¯å¦å®Œæˆ
        """
        try:
            # ç›´æ¥ä½¿ç”¨/æ›´æ–°å†…å­˜ç¼“å­˜ï¼Œé¿å…å…ˆè¯»æ–‡ä»¶å†å†™æ–‡ä»¶
            progress = self.load_progress()

            if city_completed:
                if city_name not in progress['completed_cities']:
                    progress['completed_cities'].append(city_name)
                progress['current_city'] = None
                progress['completed_keywords'] = []
            else:
                progress['current_city'] = city_name
                if keyword and keyword not in progress['completed_keywords']:
                    progress['completed_keywords'].append(keyword)

            progress['last_update'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # å†™å…¥ç£ç›˜ï¼ˆè¿›åº¦æ–‡ä»¶è¾ƒå°ï¼Œæ— éœ€å»æ‰ç¼©è¿›ï¼‰
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)

            # ç¼“å­˜å·²åœ¨ load_progress è¿”å›çš„å¼•ç”¨ä¸ŠåŸåœ°ä¿®æ”¹ï¼Œæ— éœ€é‡æ–°èµ‹å€¼
            self.logger.debug(f"âœ“ è¿›åº¦å·²ä¿å­˜: {city_name} - {keyword if keyword else 'åŸå¸‚å®Œæˆ'}")

        except Exception as e:
            self.logger.warning(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def is_task_completed(self, city_name, keyword):
        """
        æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ
        
        Args:
            city_name: åŸå¸‚åç§°
            keyword: å…³é”®è¯
        
        Returns:
            bool: æ˜¯å¦å·²å®Œæˆ
        """
        progress = self.load_progress()
        
        # æ£€æŸ¥åŸå¸‚æ˜¯å¦å·²å®Œæˆ
        if city_name in progress['completed_cities']:
            return True
        
        # æ£€æŸ¥å…³é”®è¯æ˜¯å¦å·²å®Œæˆ
        if progress['current_city'] == city_name and keyword in progress['completed_keywords']:
            return True
        
        return False
    
    def clear_progress(self):
        """æ¸…é™¤è¿›åº¦æ–‡ä»¶ï¼ˆé‡æ–°å¼€å§‹ï¼‰ï¼ŒåŒæ—¶æ¸…ç©ºå†…å­˜ç¼“å­˜"""
        try:
            if os.path.exists(self.progress_file):
                os.remove(self.progress_file)
                self.logger.info("âœ“ è¿›åº¦æ–‡ä»¶å·²æ¸…é™¤")
            self._progress_cache = None  # æ¸…ç©ºç¼“å­˜ï¼Œä¸‹æ¬¡ä»ç©ºçŠ¶æ€é‡å»º
        except Exception as e:
            self.logger.warning(f"æ¸…é™¤è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    def run(self, keywords=None, cities=None):
        """
        ä¸»è¿è¡Œå‡½æ•° - æ‰¹é‡æŠ“å–Bossç›´è˜èŒä½æ•°æ®ï¼ˆå¤šåŸå¸‚å¤šå…³é”®è¯ï¼‰
        
        å·¥ä½œæµç¨‹ï¼š
          1. åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆæ— ç—•æ¨¡å¼ + åæ£€æµ‹ï¼‰
          2. è®¿é—®Bossç›´è˜å¹¶ç­‰å¾…ç”¨æˆ·ç™»å½•
          3. åŒå±‚å¾ªç¯å¤„ç†æ‰€æœ‰åŸå¸‚å’Œå…³é”®è¯ï¼š
             - å¤–å±‚å¾ªç¯ï¼šéå†æ¯ä¸ªåŸå¸‚
             - å†…å±‚å¾ªç¯ï¼šéå†æ¯ä¸ªå…³é”®è¯
             - ç›´æ¥è®¿é—®URLï¼šhttps://www.zhipin.com/web/geek/jobs?city={åŸå¸‚ç¼–å·}&query={å…³é”®è¯}
             - å¯åŠ¨ç½‘ç»œç›‘å¬æ•è·APIæ•°æ®
             - ä¸‹æ»‘é¡µé¢55æ¬¡è§¦å‘æ‰€æœ‰æ•°æ®åŠ è½½
             - æå–ç›‘å¬åˆ°çš„æ‰€æœ‰æ•°æ®åŒ…
             - ä¿å­˜åŸå§‹æ•°æ®åˆ°JSONæ–‡ä»¶
          4. å…³é—­æµè§ˆå™¨
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»é…ç½®æ–‡ä»¶åŠ è½½çš„å…³é”®è¯
            cities: åŸå¸‚å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„6ä¸ªåŸå¸‚
        
        æ•°æ®ä¿å­˜ä½ç½®ï¼š
            data/raw/boss_jobs/boss_{åŸå¸‚}_{å…³é”®è¯}_{æ—¶é—´æˆ³}.json
        
        é¢„è®¡æ•°æ®é‡ï¼š
            - 6ä¸ªåŸå¸‚ Ã— 99ä¸ªå…³é”®è¯ Ã— 300æ¡/ä¸ª = 178,200æ¡ï¼ˆåŸå§‹ï¼‰
            - å»é‡åçº¦106,920æ¡
        """
        # å¦‚æœæœªæŒ‡å®šå…³é”®è¯ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®è¯
        if keywords is None:
            keywords = self.all_keywords
            
        # å¦‚æœæœªæŒ‡å®šåŸå¸‚ï¼Œä½¿ç”¨é»˜è®¤åŸå¸‚
        if cities is None:
            cities = self.cities
            
        if not keywords:
            self.logger.error("æ²¡æœ‰å¯ç”¨çš„å…³é”®è¯ï¼è¯·å…ˆè¿è¡Œ scripts/generate_crawl_keywords.py")
            return
        
        if not cities:
            self.logger.error("æ²¡æœ‰å¯ç”¨çš„åŸå¸‚é…ç½®ï¼")
            return
            
        page = None
        
        try:
            # 1. åˆå§‹åŒ–æµè§ˆå™¨
            page = self.init_browser()

            # 2. è®¿é—®BOSSç›´è˜é¦–é¡µ
            self.logger.info("æ­£åœ¨è®¿é—®BOSSç›´è˜...")
            page.get('https://www.zhipin.com/web/user/?ka=header-login')
            page.wait.doc_loaded(timeout=15)
            self.human_like_delay(2, 4)

            # CDP çš„ addScriptToEvaluateOnNewDocument åªå¯¹ã€æ–°ã€‘é¡µé¢ç”Ÿæ•ˆï¼Œ
            # å½“å‰å·²åŠ è½½çš„ç™»å½•é¡µéœ€è¦ç”¨ run_js è¡¥æ³¨ webdriver ä¿®å¤ï¼ˆåªæ³¨æœ€å°ä»£ç ï¼Œé¿å…æŠ¥é”™ï¼‰
            _min_patch = """
try {
    Object.defineProperty(navigator, 'webdriver', {get: () => undefined, configurable: true, enumerable: false});
} catch(e) {}
try { delete Navigator.prototype.webdriver; } catch(e) {}
try { delete navigator.__proto__.webdriver; } catch(e) {}
"""
            try:
                page.run_js(_min_patch)
            except Exception as _e:
                self.logger.warning(f"run_js è¡¥æ³¨å¤±è´¥ï¼ˆä¸å½±å“æ–°é¡µé¢ï¼‰: {_e}")

            # éªŒè¯ navigator.webdriver æ˜¯å¦å·²éšè—ï¼Œæ‰“å°ç»“æœæ–¹ä¾¿æ’æŸ¥
            try:
                wd_val = page.run_js('return navigator.webdriver')
                if not wd_val:
                    self.logger.info("âœ“ navigator.webdriver = undefinedï¼ˆæ£€æµ‹è§„é¿æˆåŠŸï¼‰")
                else:
                    self.logger.warning(f"âš ï¸ navigator.webdriver = {wd_val}ï¼ˆä»å¯èƒ½è¢«æ£€æµ‹ï¼ï¼‰")
            except Exception:
                pass

            # 3. å°è¯•å¤ç”¨å·²ä¿å­˜çš„ Cookieï¼Œå¦åˆ™ç­‰å¾…æ‰«ç ç™»å½•
            cookie_loaded = self._load_cookies(page)
            if cookie_loaded:
                # åˆ·æ–°é¡µé¢éªŒè¯ Cookie æ˜¯å¦æœ‰æ•ˆ
                page.refresh()
                page.wait.doc_loaded(timeout=10)
                self.human_like_delay(2, 3)
                page_text = page.html or ''
                # å¦‚æœé¡µé¢å‡ºç°ç™»å½•æ¡†åˆ™ Cookie å¤±æ•ˆï¼Œä»éœ€æ‰‹åŠ¨ç™»å½•
                if 'æ‰«ç ç™»å½•' in page_text or 'login' in page.url.lower():
                    self.logger.warning("Cookie å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°æ‰«ç ç™»å½•")
                    self.wait_for_login(page, wait_seconds=30)
                    self._save_cookies(page)
                else:
                    self.logger.info("âœ“ Cookie æœ‰æ•ˆï¼Œå·²è·³è¿‡ç™»å½•")
            else:
                # å…¨æ–°ç™»å½•ï¼Œç™»å½•æˆåŠŸåä¿å­˜ Cookie
                self.wait_for_login(page, wait_seconds=30)
                self._save_cookies(page)

            # ===== å…³é”®ï¼šç™»å½•åå…ˆåœ¨é¦–é¡µæ­£å¸¸æµè§ˆä¸€æ®µæ—¶é—´å†å¼€å§‹æŠ“å– =====
            # ç›´æ¥ä»ç™»å½•è·³åˆ°èŒä½æœç´¢é¡µï¼Œè·³è·ƒå¤ªå¤§ï¼ŒBOSSé£æ§ä¼šè¯†åˆ«ä¸ºå¼‚å¸¸è¡Œä¸º
            self.logger.info("âœ“ ç™»å½•å®Œæˆï¼Œå…ˆåœ¨é¦–é¡µæµè§ˆä¸€æ®µæ—¶é—´ï¼ˆæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼‰...")
            try:
                page.get('https://www.zhipin.com/')
                page.wait.doc_loaded(timeout=10)
                self.human_like_delay(3, 6)   # åœ¨é¦–é¡µåœç•™ 3~6 ç§’
                # è½»å¾®æ»šåŠ¨ï¼Œæ¨¡æ‹Ÿç”¨æˆ·åœ¨çœ‹é¦–é¡µ
                page.run_js("window.scrollBy(0, 300);")
                self.human_like_delay(1, 3)
                page.run_js("window.scrollBy(0, 200);")
                self.human_like_delay(2, 4)
            except Exception as _e:
                self.logger.warning(f"é¦–é¡µæµè§ˆå¤±è´¥ï¼ˆç»§ç»­ï¼‰: {_e}")

            # 4. åŠ è½½è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
            progress = None
            if self.enable_resume:
                progress = self.load_progress()
                if progress['completed_cities'] or progress['completed_keywords']:
                    self.logger.info(f"\n{'='*70}")
                    self.logger.info(f"æ£€æµ‹åˆ°ä¸Šæ¬¡ä¸­æ–­çš„è¿›åº¦ï¼š")
                    self.logger.info(f"  - å·²å®ŒæˆåŸå¸‚: {', '.join(progress['completed_cities']) if progress['completed_cities'] else 'æ— '}")
                    self.logger.info(f"  - å½“å‰åŸå¸‚: {progress['current_city'] if progress['current_city'] else 'æ— '}")
                    self.logger.info(f"  - å·²å®Œæˆå…³é”®è¯: {len(progress['completed_keywords'])}ä¸ª")
                    self.logger.info(f"  - ä¸Šæ¬¡æ›´æ–°: {progress['last_update']}")
                    self.logger.info(f"{'='*70}")
                    
                    response = input("\næ˜¯å¦ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­ï¼Ÿ(yes/noï¼Œè¾“å…¥'reset'é‡æ–°å¼€å§‹): ")
                    if response.lower() == 'reset':
                        self.clear_progress()
                        progress = self.load_progress()
                        self.logger.info("âœ“ å·²é‡ç½®è¿›åº¦ï¼Œå°†ä»å¤´å¼€å§‹æŠ“å–\n")
                    elif response.lower() not in ['yes', 'y']:
                        self.logger.info("å·²å–æ¶ˆ")
                        return
                    else:
                        self.logger.info("âœ“ å°†ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­\n")
            
            # 5. åŒå±‚å¾ªç¯ï¼šåŸå¸‚ Ã— å…³é”®è¯
            total_tasks = len(cities) * len(keywords)
            current_task = 0
            skipped_tasks = 0
            
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"å¼€å§‹æŠ“å–æ•°æ®")
            self.logger.info(f"  - åŸå¸‚æ•°é‡: {len(cities)}")
            self.logger.info(f"  - å…³é”®è¯æ•°é‡: {len(keywords)}")
            self.logger.info(f"  - æ€»ä»»åŠ¡æ•°: {total_tasks}")
            self.logger.info(f"{'='*70}\n")
            
            for city_idx, (city_name, city_code) in enumerate(cities.items(), 1):
                # æ£€æŸ¥åŸå¸‚æ˜¯å¦å·²å®Œæˆ
                if self.enable_resume and progress and city_name in progress['completed_cities']:
                    self.logger.info(f"â­ï¸  è·³è¿‡å·²å®ŒæˆåŸå¸‚: {city_name}")
                    skipped_tasks += len(keywords)
                    continue
                
                self.logger.info(f"\n{'#'*70}")
                self.logger.info(f"# åŸå¸‚ [{city_idx}/{len(cities)}]: {city_name} ({city_code})")
                self.logger.info(f"{'#'*70}\n")
                
                for keyword_idx, keyword in enumerate(keywords, 1):
                    current_task += 1
                    
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                    if self.enable_resume and self.is_task_completed(city_name, keyword):
                        self.logger.info(f"â­ï¸  è·³è¿‡å·²å®Œæˆä»»åŠ¡: {city_name} - {keyword}")
                        skipped_tasks += 1
                        continue
                    
                    self.logger.info(f"\n{'='*60}")
                    self.logger.info(f"ä»»åŠ¡è¿›åº¦: {current_task}/{total_tasks} (å·²è·³è¿‡: {skipped_tasks})")
                    self.logger.info(f"åŸå¸‚: {city_name} [{city_idx}/{len(cities)}]")
                    self.logger.info(f"å…³é”®è¯: {keyword} [{keyword_idx}/{len(keywords)}]")
                    self.logger.info(f"{'='*60}\n")
                    
                    # æŠ“å–æ•°æ®
                    success, jobs, error_reason = self.crawl_city_keyword(page, city_name, city_code, keyword)
                    
                    # ã€æ–°å¢ã€‘æ£€æµ‹è¿ç»­å¤±è´¥
                    if not success:
                        self.consecutive_failures += 1
                        failure_msg = f"âš ï¸  ä»»åŠ¡å¤±è´¥ ({self.consecutive_failures}/{self.max_consecutive_failures}): {error_reason}"
                        print(f"\n{failure_msg}\n")
                        self.logger.warning(failure_msg)
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿ç»­å¤±è´¥ä¸Šé™
                        if self.consecutive_failures >= self.max_consecutive_failures:
                            critical_msg = f"ğŸ›‘ è¿ç»­å¤±è´¥ {self.consecutive_failures} æ¬¡ï¼Œå¯èƒ½è§¦å‘é£æ§ï¼Œè‡ªåŠ¨åœæ­¢æŠ“å–ï¼"
                            print(f"\n{'='*70}")
                            print(critical_msg)
                            print(f"{'='*70}\n")
                            self.logger.critical(critical_msg)
                            self.logger.info("è¿›åº¦å·²ä¿å­˜ï¼Œå¯ç¨åé‡æ–°è¿è¡Œç»§ç»­æŠ“å–")
                            
                            # ä¿å­˜å½“å‰è¿›åº¦åé€€å‡º
                            if self.enable_resume:
                                self.save_progress(city_name, keyword)
                            
                            # æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘finallyå—å…³é—­æµè§ˆå™¨
                            raise RuntimeError(f"è¿ç»­å¤±è´¥{self.consecutive_failures}æ¬¡ï¼Œè‡ªåŠ¨åœæ­¢")
                    else:
                        # æˆåŠŸåˆ™é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                        if self.consecutive_failures > 0:
                            self.logger.info(f"âœ“ ä»»åŠ¡æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°ï¼ˆä¹‹å‰: {self.consecutive_failures}ï¼‰")
                            self.consecutive_failures = 0
                    
                    # ä¿å­˜æ•°æ®
                    if jobs:
                        self.save_city_keyword_data(jobs, city_name, keyword)
                    
                    # ä¿å­˜è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                    if self.enable_resume:
                        self.save_progress(city_name, keyword)
                    
                    # ä»»åŠ¡é—´å»¶è¿Ÿï¼ˆé¿å…é£æ§ï¼‰
                    if current_task < total_tasks:
                        # å¢åŠ éšæœºæ€§ï¼šæ¯Nä¸ªä»»åŠ¡ä¼‘æ¯ä¸€æ¬¡
                        if keyword_idx % self.long_break_interval == 0:
                            delay = random.uniform(self.long_break_min, self.long_break_max)
                            self.logger.info(f"\nâ° ç¬¬{keyword_idx}ä¸ªå…³é”®è¯ï¼Œé•¿ä¼‘æ¯ {delay:.2f} ç§’...\n")
                        else:
                            delay = random.uniform(self.task_interval_min, self.task_interval_max)
                            self.logger.info(f"\nâ° ç­‰å¾… {delay:.2f} ç§’åç»§ç»­...\n")
                        time.sleep(delay)
                
                # æ ‡è®°åŸå¸‚å®Œæˆï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                if self.enable_resume:
                    self.save_progress(city_name, city_completed=True)
                
                # æ¸…ç†å†…å­˜ï¼šé‡Šæ”¾è¯¥åŸå¸‚çš„ job_id é›†åˆå’Œæ•°æ®ç¼“å­˜
                if city_name in self.city_job_ids:
                    del self.city_job_ids[city_name]
                if city_name in self.city_data_cache:
                    del self.city_data_cache[city_name]
                self.logger.info(f"âœ“ å·²é‡Šæ”¾åŸå¸‚ {city_name} çš„å†…å­˜ç¼“å­˜ï¼ˆjob_id é›†åˆ + æ•°æ®ç¼“å­˜ï¼‰")
                
                # åŸå¸‚é—´çš„é¢å¤–å»¶è¿Ÿ
                if city_idx < len(cities):
                    extra_delay = random.uniform(10, 15)
                    self.logger.info(f"\n{'='*60}")
                    self.logger.info(f"âœ“ åŸå¸‚ {city_name} å®Œæˆ")
                    self.logger.info(f"â° ç­‰å¾… {extra_delay:.2f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªåŸå¸‚...")
                    self.logger.info(f"{'='*60}\n")
                    time.sleep(extra_delay)
            
            self.logger.info("\n" + "="*70)
            self.logger.info("âœ“âœ“âœ“ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼âœ“âœ“âœ“")
            self.logger.info(f"  - å¤„ç†åŸå¸‚: {len(cities)}ä¸ª")
            self.logger.info(f"  - å¤„ç†å…³é”®è¯: {len(keywords)}ä¸ª")
            self.logger.info(f"  - æ€»ä»»åŠ¡æ•°: {total_tasks}ä¸ª")
            self.logger.info(f"  - è·³è¿‡ä»»åŠ¡: {skipped_tasks}ä¸ª")
            self.logger.info(f"  - å®é™…æ‰§è¡Œ: {total_tasks - skipped_tasks}ä¸ª")
            self.logger.info("="*70)
            
            # æ¸…é™¤è¿›åº¦æ–‡ä»¶ï¼ˆæ‰€æœ‰ä»»åŠ¡å®Œæˆï¼‰
            if self.enable_resume:
                self.clear_progress()
                self.logger.info("âœ“ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œè¿›åº¦æ–‡ä»¶å·²æ¸…é™¤")
            
        except KeyboardInterrupt:
            self.logger.warning("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            self.logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
            self.logger.error(traceback.format_exc())
        finally:
            # å…³é—­æµè§ˆå™¨
            if page:
                self.logger.info("æ­£åœ¨å…³é—­æµè§ˆå™¨...")
                try:
                    page.quit()
                    self.logger.info("âœ“ æµè§ˆå™¨å·²å…³é—­")
                except:
                    pass
                



if __name__ == "__main__":
    """
    è¿è¡Œçˆ¬è™«ä¸»ç¨‹åº - å¤šåŸå¸‚å¤šå…³é”®è¯ç‰ˆæœ¬
    
    ä½¿ç”¨æ–¹å¼ï¼š
      1. è¿è¡Œæ­¤è„šæœ¬ä¼šè‡ªåŠ¨ä» data/crawl_keywords.json åŠ è½½å…³é”®è¯
      2. è‡ªåŠ¨éå†6ä¸ªåŸå¸‚ï¼ˆåŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·ã€æ·±åœ³ã€æ­å·ã€æˆéƒ½ï¼‰
      3. æ¯ä¸ªåŸå¸‚æŠ“å–æ‰€æœ‰å…³é”®è¯çš„æ•°æ®
    
    æ•°æ®é‡è¯´æ˜ï¼š
      - åŸå¸‚æ•°é‡ï¼š6ä¸ª
      - å…³é”®è¯æ€»æ•°ï¼š99ä¸ªï¼ˆä»æŠ€èƒ½è¯å…¸è‡ªåŠ¨ç”Ÿæˆï¼‰
      - æ¯ä¸ªåŸå¸‚æ¯ä¸ªå…³é”®è¯ï¼šçº¦300æ¡èŒä½ï¼ˆä¸‹æ»‘55æ¬¡ç¡®ä¿å®Œæ•´ï¼‰
      - é¢„è®¡æ€»æ•°ï¼š6 Ã— 99 Ã— 300 = 178,200æ¡åŸå§‹æ•°æ®
      - å»é‡åï¼šçº¦106,920æ¡ï¼ˆä¼˜ç§€çº§åˆ«ï¼‰
    
    æ‰§è¡Œæ—¶é—´ï¼š
      - æ¯ä¸ªä»»åŠ¡ï¼ˆåŸå¸‚+å…³é”®è¯ï¼‰çº¦éœ€2-3åˆ†é’Ÿï¼ˆåŒ…å«ä¸‹æ»‘å’Œç­‰å¾…ï¼‰
      - æ€»ä»»åŠ¡æ•°ï¼š6 Ã— 99 = 594ä¸ªä»»åŠ¡
      - é¢„è®¡æ€»è€—æ—¶ï¼šçº¦20-30å°æ—¶
    
    æ³¨æ„äº‹é¡¹ï¼š
      1. ç¡®ä¿å·²è¿è¡Œ scripts/generate_crawl_keywords.py ç”Ÿæˆå…³é”®è¯åˆ—è¡¨
      2. å¯åŠ¨åä¼šæ‰“å¼€æµè§ˆå™¨ï¼Œéœ€è¦æ‰‹åŠ¨æ‰«ç ç™»å½•ï¼ˆ35ç§’å†…ï¼‰
      3. æ•°æ®ä¼šå®æ—¶ä¿å­˜åˆ° data/raw/boss_jobs/ ç›®å½•
      4. æ–‡ä»¶å‘½åï¼šboss_{åŸå¸‚}_{å…³é”®è¯}_{æ—¶é—´æˆ³}.json
      5. å¦‚é‡åˆ°åçˆ¬ï¼Œå¯æš‚åœåé‡æ–°è¿è¡Œï¼ˆå·²ä¿å­˜çš„æ•°æ®ä¸ä¼šä¸¢å¤±ï¼‰
    """
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider = BossZhipinSpider()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\n" + "="*70)
    print(f"Bossç›´è˜æ‰¹é‡çˆ¬è™« v{spider.VERSION} - å¤šåŸå¸‚å¤šå…³é”®è¯ç‰ˆæœ¬")
    print("="*70)
    print(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"åŸå¸‚æ•°é‡: {len(spider.cities)}")
    print(f"åŸå¸‚åˆ—è¡¨: {', '.join(spider.cities.keys())}")
    print(f"å…³é”®è¯æ€»æ•°: {len(spider.all_keywords)}")
    print(f"æ¯ä¸ªå…³é”®è¯ä¸‹æ»‘æ¬¡æ•°: {spider.scroll_times_per_keyword}")
    print(f"æ€»ä»»åŠ¡æ•°: {len(spider.cities)} Ã— {len(spider.all_keywords)} = {len(spider.cities) * len(spider.all_keywords)}")
    print(f"æ•°æ®ä¿å­˜ç›®å½•: {spider.data_dir}")
    print(f"å…³é”®è¯é…ç½®æ–‡ä»¶: {spider.keywords_config_file}")
    print("="*70)
    
    if spider.all_keywords and spider.cities:
        print("\nå…³é”®è¯é¢„è§ˆï¼ˆå‰10ä¸ªï¼‰ï¼š")
        for i, kw in enumerate(spider.all_keywords[:10], 1):
            print(f"  {i}. {kw}")
        if len(spider.all_keywords) > 10:
            print(f"  ... è¿˜æœ‰ {len(spider.all_keywords) - 10} ä¸ªå…³é”®è¯")
        
        print("\né¢„è®¡æ•°æ®é‡ï¼ˆå…¨éƒ¨åŸå¸‚ï¼‰ï¼š")
        total_raw = len(spider.cities) * len(spider.all_keywords) * 300
        total_dedup = int(total_raw * 0.6)
        print(f"  - åŸå§‹æ•°æ®ï¼šçº¦ {total_raw:,} æ¡")
        print(f"  - å»é‡åï¼šçº¦ {total_dedup:,} æ¡")
        
        # ========== åˆ†æ‰¹æ¬¡æŠ“å–æ¨¡å¼ ==========
        # æ–°åŸå¸‚åˆ—è¡¨ï¼Œæ¯5ä¸ªä¸€æ‰¹
        new_city_items = list(spider.cities_new.items())
        batch_size = 5
        batches = [new_city_items[i:i+batch_size] for i in range(0, len(new_city_items), batch_size)]

        print("\n" + "="*70)
        print("æŠ“å–æ¨¡å¼é€‰æ‹©")
        print("="*70)
        print("1. å•åŸå¸‚æ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šä»å·²æœ‰åŸå¸‚ä¸­é€‰1ä¸ªï¼Œçº¦4-5å°æ—¶")
        print("2. å…¨éƒ¨å·²æœ‰åŸå¸‚æ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ“å–å…¨éƒ¨å·²é…ç½®åŸå¸‚ï¼Œçº¦20-30å°æ—¶")
        print("3. ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¿«é€Ÿæµ‹è¯•å°‘é‡æ•°æ®ï¼ˆä¸ä¿å­˜ï¼‰ï¼Œçº¦2-3åˆ†é’Ÿ")
        print("â”€"*70)
        print("â”€â”€ æ–°åŸå¸‚æŠ“å– â”€â”€")
        print(f"4. æ–°åŸå¸‚æ‰¹æ¬¡æ¨¡å¼ï¼ˆæ¯æ‰¹5ä¸ªåŸå¸‚ï¼Œå…±{len(batches)}æ‰¹ï¼‰ï¼šæ¯æ‰¹çº¦20-25å°æ—¶")
        for bi, batch in enumerate(batches, 1):
            city_names = "ã€".join(c[0] for c in batch)
            print(f"   ç¬¬{bi}æ‰¹: {city_names}")
        print(f"5. å…¨éƒ¨æ–°åŸå¸‚æ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ“å–å…¨éƒ¨{len(spider.cities_new)}ä¸ªæ–°åŸå¸‚ï¼Œçº¦{len(spider.cities_new)*4}-{len(spider.cities_new)*5}å°æ—¶")
        print("="*70)

        mode = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1/2/3/4/5): ").strip()
        
        if mode == "1":
            # å•åŸå¸‚æ¨¡å¼
            print("\nå¯é€‰åŸå¸‚ï¼š")
            city_list = list(spider.cities.keys())
            for idx, city in enumerate(city_list, 1):
                print(f"  {idx}. {city} ({spider.cities[city]})")
            
            print(f"  {len(city_list) + 1}. å…¨éƒ¨åŸå¸‚ï¼ˆä¸€æ¬¡æ€§æŠ“å–ï¼‰")
            
            city_choice = input(f"\nè¯·é€‰æ‹©è¦æŠ“å–çš„åŸå¸‚ (1-{len(city_list) + 1}): ").strip()
            
            try:
                choice_num = int(city_choice)
                if 1 <= choice_num <= len(city_list):
                    # é€‰æ‹©å•ä¸ªåŸå¸‚
                    selected_city = city_list[choice_num - 1]
                    selected_cities = {selected_city: spider.cities[selected_city]}
                    
                    # è®¡ç®—å•åŸå¸‚æ•°æ®é‡
                    single_city_raw = len(spider.all_keywords) * 300
                    single_city_dedup = int(single_city_raw * 0.6)
                    
                    print("\n" + "="*70)
                    print(f"å·²é€‰æ‹©ï¼š{selected_city}")
                    print("="*70)
                    print(f"å…³é”®è¯æ•°é‡: {len(spider.all_keywords)}")
                    print(f"ä»»åŠ¡æ•°: {len(spider.all_keywords)}")
                    print(f"é¢„è®¡æ•°æ®é‡: {single_city_raw:,} æ¡ï¼ˆå»é‡å {single_city_dedup:,} æ¡ï¼‰")
                    print(f"é¢„è®¡è€—æ—¶: 4-5 å°æ—¶")
                    print("="*70)
                    
                    confirm = input("\nç¡®è®¤å¼€å§‹æŠ“å–? (yes/no): ")
                    if confirm.lower() in ['yes', 'y']:
                        spider.run(cities=selected_cities)
                    else:
                        print("å·²å–æ¶ˆ")
                        
                elif choice_num == len(city_list) + 1:
                    # é€‰æ‹©å…¨éƒ¨åŸå¸‚
                    print("\nâš ï¸  è­¦å‘Šï¼šå…¨éƒ¨åŸå¸‚æ¨¡å¼éœ€è¦è¿ç»­è¿è¡Œ20-30å°æ—¶ï¼")
                    confirm = input("ç¡®è®¤å¼€å§‹æŠ“å–æ‰€æœ‰åŸå¸‚? (yes/no): ")
                    if confirm.lower() in ['yes', 'y']:
                        spider.run()
                    else:
                        print("å·²å–æ¶ˆ")
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼")
                
        elif mode == "2":
            # å…¨éƒ¨åŸå¸‚æ¨¡å¼
            print("\nâš ï¸  è­¦å‘Šï¼šå…¨éƒ¨åŸå¸‚æ¨¡å¼éœ€è¦è¿ç»­è¿è¡Œ20-30å°æ—¶ï¼")
            print("å»ºè®®ä½¿ç”¨å•åŸå¸‚æ¨¡å¼åˆ†æ‰¹æ¬¡æŠ“å–ï¼Œé™ä½é£é™©ã€‚")
            confirm = input("\nç¡®è®¤å¼€å§‹æŠ“å–æ‰€æœ‰åŸå¸‚? (yes/no): ")
            if confirm.lower() in ['yes', 'y']:
                spider.run()  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰å…³é”®è¯å’ŒåŸå¸‚
            else:
                print("å·²å–æ¶ˆ")
                
        elif mode == "3":
            # æµ‹è¯•æ¨¡å¼
            print("\n" + "="*70)
            print("ğŸ§ª æµ‹è¯•æ¨¡å¼")
            print("="*70)
            print("åŠŸèƒ½ï¼šå¿«é€ŸéªŒè¯çˆ¬è™«åŠŸèƒ½ï¼ŒæŠ“å–å°‘é‡æ•°æ®")
            print("ç‰¹ç‚¹ï¼š")
            print("  - åªæŠ“å–1ä¸ªåŸå¸‚çš„2ä¸ªå…³é”®è¯")
            print("  - æ¯ä¸ªå…³é”®è¯åªä¸‹æ»‘5æ¬¡ï¼ˆçº¦30-50æ¡æ•°æ®ï¼‰")
            print("  - è§£æå¹¶æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡")
            print("  - âš ï¸  ä¸ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶")
            print("  - è€—æ—¶ï¼šçº¦2-3åˆ†é’Ÿ")
            print("="*70)
            
            # é€‰æ‹©æµ‹è¯•åŸå¸‚
            print("\nå¯é€‰æµ‹è¯•åŸå¸‚ï¼š")
            city_list = list(spider.cities.keys())
            for idx, city in enumerate(city_list, 1):
                print(f"  {idx}. {city}")
            
            city_choice = input(f"\né€‰æ‹©æµ‹è¯•åŸå¸‚ (1-{len(city_list)}ï¼Œé»˜è®¤1-åŒ—äº¬): ").strip()
            4
            try:
                if not city_choice:
                    choice_num = 1
                else:
                    choice_num = int(city_choice)
                
                if 1 <= choice_num <= len(city_list):
                    selected_city = city_list[choice_num - 1]
                    test_cities = {selected_city: spider.cities[selected_city]}
                    
                    # å›ºå®šæµ‹è¯•å…³é”®è¯
                    test_keywords = ['Pythonå¼€å‘', 'Javaå¼€å‘']
                    
                    print("\n" + "="*70)
                    print(f"ğŸ§ª æµ‹è¯•é…ç½®")
                    print("="*70)
                    print(f"æµ‹è¯•åŸå¸‚: {selected_city}")
                    print(f"æµ‹è¯•å…³é”®è¯: {', '.join(test_keywords)}")
                    print(f"ä¸‹æ»‘æ¬¡æ•°: {spider.test_scroll_times}æ¬¡/å…³é”®è¯")
                    print(f"é¢„è®¡æ•°æ®: çº¦60-100æ¡ï¼ˆä¸ä¿å­˜ï¼‰")
                    print(f"é¢„è®¡è€—æ—¶: 2-3åˆ†é’Ÿ")
                    print("="*70)
                    
                    confirm = input("\nå¼€å§‹æµ‹è¯•? (yes/no): ")
                    if confirm.lower() in ['yes', 'y', '']:
                        # å¯ç”¨æµ‹è¯•æ¨¡å¼
                        spider.test_mode = True
                        spider.enable_resume = False  # æµ‹è¯•æ¨¡å¼ç¦ç”¨æ–­ç‚¹ç»­ä¼ 
                        
                        print("\nğŸ§ª æµ‹è¯•æ¨¡å¼å·²å¯ç”¨")
                        print("âš ï¸  æé†’ï¼šæµ‹è¯•æ•°æ®ä¸ä¼šä¿å­˜åˆ°æ–‡ä»¶\n")
                        
                        spider.run(keywords=test_keywords, cities=test_cities)
                        
                        print("\n" + "="*70)
                        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
                        print("="*70)
                        print("æç¤ºï¼š")
                        print("  - å¦‚æœæ•°æ®æ­£å¸¸ï¼Œå¯ä»¥ä½¿ç”¨æ¨¡å¼1æˆ–2è¿›è¡Œæ­£å¼æŠ“å–")
                        print("  - æ­£å¼æŠ“å–ä¼šä¿å­˜æ•°æ®åˆ° data/raw/boss_jobs/")
                        print("="*70)
                    else:
                        print("å·²å–æ¶ˆ")
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼")
        elif mode == "4":
            # æ–°åŸå¸‚æ‰¹æ¬¡æ¨¡å¼ï¼ˆæ¯æ‰¹5ä¸ªåŸå¸‚ï¼‰
            print("\n" + "="*70)
            print("ğŸ—ºï¸  æ–°åŸå¸‚æ‰¹æ¬¡æ¨¡å¼")
            print("="*70)
            print(f"å…± {len(batches)} æ‰¹ï¼Œæ¯æ‰¹5ä¸ªåŸå¸‚ï¼Œçº¦20-25å°æ—¶/æ‰¹")
            print("â”€"*70)
            for bi, batch in enumerate(batches, 1):
                city_names = "ã€".join(c[0] for c in batch)
                print(f"  ç¬¬{bi}æ‰¹: {city_names}")
            print("="*70)

            batch_choice = input(f"\nè¯·é€‰æ‹©è¦æŠ“å–çš„æ‰¹æ¬¡ (1-{len(batches)}): ").strip()
            try:
                batch_num = int(batch_choice)
                if 1 <= batch_num <= len(batches):
                    selected_batch = dict(batches[batch_num - 1])
                    city_names = "ã€".join(selected_batch.keys())
                    batch_raw = len(spider.all_keywords) * 300 * len(selected_batch)
                    batch_dedup = int(batch_raw * 0.6)

                    print("\n" + "="*70)
                    print(f"å·²é€‰æ‹©ç¬¬{batch_num}æ‰¹ï¼š{city_names}")
                    print("="*70)
                    print(f"åŸå¸‚æ•°é‡: {len(selected_batch)}")
                    print(f"å…³é”®è¯æ•°é‡: {len(spider.all_keywords)}")
                    print(f"ä»»åŠ¡æ€»æ•°: {len(selected_batch) * len(spider.all_keywords)}")
                    print(f"é¢„è®¡æ•°æ®é‡: {batch_raw:,} æ¡ï¼ˆå»é‡åçº¦ {batch_dedup:,} æ¡ï¼‰")
                    print(f"é¢„è®¡è€—æ—¶: çº¦20-25å°æ—¶")
                    print("="*70)

                    confirm = input("\nç¡®è®¤å¼€å§‹æŠ“å–? (yes/no): ")
                    if confirm.lower() in ['yes', 'y']:
                        # æ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ç‹¬ç«‹çš„è¿›åº¦æ–‡ä»¶ï¼Œé¿å…å¤šæ‰¹æ¬¡å¹¶å‘æ—¶äº’ç›¸è¦†ç›–
                        spider.progress_file = os.path.join(
                            PROJECT_ROOT, "data",
                            f"crawler_progress_batch{batch_num}.json"
                        )
                        print(f"ğŸ“‚ ä½¿ç”¨ç‹¬ç«‹è¿›åº¦æ–‡ä»¶: crawler_progress_batch{batch_num}.json")
                        spider.cities = selected_batch
                        spider.run(cities=selected_batch)
                    else:
                        print("å·²å–æ¶ˆ")
                else:
                    print(f"âŒ è¯·è¾“å…¥ 1 åˆ° {len(batches)} ä¹‹é—´çš„æ•°å­—ï¼")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ï¼")

        elif mode == "5":
            # å…¨éƒ¨æ–°åŸå¸‚æ¨¡å¼
            all_new = dict(spider.cities_new)
            total_raw = len(spider.all_keywords) * 300 * len(all_new)
            total_dedup = int(total_raw * 0.6)
            est_hours_min = len(all_new) * 4
            est_hours_max = len(all_new) * 5

            print("\n" + "="*70)
            print("ğŸ—ºï¸  å…¨éƒ¨æ–°åŸå¸‚æ¨¡å¼")
            print("="*70)
            print(f"åŸå¸‚æ•°é‡: {len(all_new)} ä¸ª")
            print(f"åŸå¸‚åˆ—è¡¨: {'ã€'.join(all_new.keys())}")
            print(f"å…³é”®è¯æ•°é‡: {len(spider.all_keywords)}")
            print(f"é¢„è®¡æ•°æ®é‡: {total_raw:,} æ¡ï¼ˆå»é‡åçº¦ {total_dedup:,} æ¡ï¼‰")
            print(f"é¢„è®¡è€—æ—¶: {est_hours_min}-{est_hours_max} å°æ—¶ï¼ˆè¿ç»­è¿è¡Œï¼‰")
            print("â”€"*70)
            print("âš ï¸  è­¦å‘Šï¼šè€—æ—¶æé•¿ï¼Œå»ºè®®ä½¿ç”¨æ¨¡å¼4åˆ†æ‰¹æ¬¡æŠ“å–ï¼Œé™ä½é£é™©ï¼")
            print("="*70)

            confirm = input("\nç¡®è®¤å¼€å§‹æŠ“å–å…¨éƒ¨æ–°åŸå¸‚? (yes/no): ")
            if confirm.lower() in ['yes', 'y']:
                spider.cities = all_new
                spider.run(cities=all_new)
            else:
                print("å·²å–æ¶ˆ")

        else:
            print("âŒ æ— æ•ˆçš„æ¨¡å¼é€‰æ‹©ï¼")
    else:
        if not spider.all_keywords:
            print("\nâŒ æœªæ‰¾åˆ°å…³é”®è¯é…ç½®ï¼")
            print("è¯·å…ˆè¿è¡Œ: python scripts/generate_crawl_keywords.py")
        if not spider.cities:
            print("\nâŒ æœªæ‰¾åˆ°åŸå¸‚é…ç½®ï¼")
    
    # ã€æµ‹è¯•é€‰é¡¹ã€‘æ‰‹åŠ¨æŒ‡å®šå°‘é‡å…³é”®è¯å’ŒåŸå¸‚ï¼ˆæµ‹è¯•ç”¨ï¼‰
    # å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šæ¥ä½¿ç”¨æµ‹è¯•æ¨¡å¼
    """
    test_keywords = ['Pythonå¼€å‘', 'Javaå¼€å‘']
    test_cities = {'åŒ—äº¬': '101010100', 'ä¸Šæµ·': '101020100'}
    spider.run(keywords=test_keywords, cities=test_cities)
    """
