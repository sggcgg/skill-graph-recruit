"""
èŒä½æ¨èAgent
åŸºäº LangGraph create_react_agent å®ç°å¤šè½®å¯¹è¯ï¼ˆå…¼å®¹ LangChain 1.xï¼‰
"""
import asyncio
import logging
import yaml
from pathlib import Path
from typing import List, AsyncGenerator
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from src.agent.tools import AgentTools

logger = logging.getLogger(__name__)

# ç›´æ¥è·¯å¾„ï¼šå¯è¯†åˆ«çš„åŸå¸‚åç§°
_CITIES = {
    'åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½', 'æ­¦æ±‰', 'å—äº¬', 'è¥¿å®‰', 'é‡åº†',
    'è‹å·', 'å¤©æ´¥', 'åˆè‚¥', 'å¦é—¨', 'é•¿æ²™', 'éƒ‘å·', 'å®æ³¢', 'é’å²›', 'æµå—', 'å¤§è¿',
}

SYSTEM_PROMPT = """ä½ æ˜¯ã€Œæ™ºè˜åŠ©æ‰‹ã€ï¼Œä¸“æ³¨ä¸­å›½ IT å°±ä¸šå¸‚åœºçš„å²—ä½æ¨èä¸èŒä¸šè§„åˆ’åŠ©æ‰‹ã€‚

# æ ¸å¿ƒå†³ç­–è§„åˆ™ï¼ˆæ¯æ¬¡å›å¤å‰å¿…é¡»å…ˆè¿‡ä¸€éï¼‰

## è§„åˆ™ A â€” å†å²é‡Œæœ‰æ•°æ®æ—¶ï¼Œç¦æ­¢é‡å¤æ£€ç´¢
å½“ç”¨æˆ·ä½¿ç”¨ä»¥ä¸‹ä»»æ„è¿½é—®è¯æ—¶ï¼š
"å“ªä¸ªæ›´å¥½"ã€"ä¸Šé¢"ã€"å‰é¢"ã€"è¿™äº›"ã€"è¿™ä¸ª"ã€"é‚£ä¸ª"ã€"å“ªä¸ª"ã€
"æ¯”è¾ƒ"ã€"å¯¹æ¯”"ã€"æ’åº"ã€"é€‚åˆæˆ‘å—"ã€"æ¨èå“ª"ã€"ç¬¬å‡ ä¸ª"

â†’ **å¯¹è¯å†å²å·²åŒ…å«æ‰€éœ€æ•°æ®ï¼Œç«‹å³åˆ†æï¼Œä¸å¾—è°ƒç”¨ä»»ä½•å·¥å…·**
â†’ è¿åæ­¤è§„åˆ™ = é¢å¤–ç­‰å¾… 10 ç§’ä»¥ä¸Šï¼Œç”¨æˆ·ä½“éªŒæå·®

## è§„åˆ™ B â€” ä»¥ä¸‹åœºæ™¯å¿…é¡»è°ƒå¯¹åº”å·¥å…·ï¼Œä¸”ä»…è°ƒ 1 æ¬¡

### B1 â€” å²—ä½æœç´¢/æ¨èï¼ˆå«ä»¥ä¸‹è¯ï¼‰
"å¸®æˆ‘æ‰¾"ã€"æœç´¢"ã€"æŸ¥æ‰¾"ã€"æ¨èå²—ä½"ã€"æ‰¾èŒä½"ã€"æœ‰å“ªäº›å²—ä½"ã€"æ‰¾ä¸€ä¸‹"
- `search_jobs(query, city)` â†’ query å¡«æŠ€èƒ½æˆ–å²—ä½åï¼›æœ‰åŸå¸‚æ—¶åŒæ­¥ä¼  city
- `recommend_jobs(user_skills, city)` â†’ user_skills é€—å·åˆ†éš”ï¼›æœ‰åŸå¸‚æ—¶ä¼  city

### B2 â€” æŠ€èƒ½å·®è·åˆ†æï¼ˆå«ä»¥ä¸‹è¯æ—¶å¿…é¡»è°ƒ analyze_skill_gapï¼‰
"æŠ€èƒ½å·®è·"ã€"å·®è·åœ¨å“ª"ã€"è¿˜å·®ä»€ä¹ˆ"ã€"è¿˜ç¼ºä»€ä¹ˆ"ã€"ç¼ºå“ªäº›æŠ€èƒ½"ã€"å¸®æˆ‘åˆ†æå·®è·"ã€"åšxxxéœ€è¦ä»€ä¹ˆæŠ€èƒ½"
- `analyze_skill_gap(user_skills, target_position)` â†’ ç”¨æˆ·æœªè¯´æ˜æŠ€èƒ½æ—¶ï¼Œuser_skills ä¼ ç©ºå­—ç¬¦ä¸² ""ï¼Œtarget_position å¡«ç›®æ ‡å²—ä½
- å·¥å…·ä¼šä»å›¾è°±æå–ç›®æ ‡å²—ä½é«˜é¢‘æŠ€èƒ½å¹¶ç»™å‡ºå·®è·æŠ¥å‘Š

### B3 â€” æŠ€èƒ½å›¾è°±æŸ¥è¯¢
- `query_skill_graph(skill_name)` â†’ æŸ¥è¯¢å•ä¸ªæŠ€èƒ½çš„å¸‚åœºçƒ­åº¦ã€è–ªèµ„ã€å…³è”å²—ä½

**é“å¾‹ï¼šæ¯è½®æœ€å¤šè°ƒ 1 æ¬¡å·¥å…·ï¼Œè°ƒå®Œç«‹å³å›ç­”ï¼Œç»“æœå·²æœ‰æ—¶ç»ä¸é‡è°ƒ**

## è§„åˆ™ C â€” ä»¥ä¸‹æƒ…å†µç›´æ¥ç”¨çŸ¥è¯†å›ç­”ï¼Œä¸è°ƒä»»ä½•å·¥å…·
- é—²èŠã€é—®å€™ã€æ„Ÿè°¢ã€è¡¨æƒ…
- å­¦ä¹ è·¯å¾„å»ºè®®ã€å­¦ä¹ è®¡åˆ’åˆ¶å®šï¼ˆå·²æœ‰æŠ€èƒ½å·®è·æ•°æ®æ—¶å¯ç›´æ¥è§„åˆ’ï¼‰
- è–ªèµ„è¡Œæƒ…ä¼°ç®—ï¼ˆå‡­é¢†åŸŸçŸ¥è¯†ï¼‰
- å¯¹å·²æœ‰æœç´¢ç»“æœçš„åˆ†æã€æ¯”è¾ƒã€æ¨èã€è¯„ä»·

# è¾“å‡ºæ ¼å¼
- è¿½é—® / åˆ†æç±»ï¼šç›´æ¥ç»™å‡ºåˆ¤æ–­å’Œå…·ä½“å»ºè®®ï¼Œè¨€ç®€æ„èµ…ï¼Œä¸ç»•å¼¯
- å²—ä½åˆ—è¡¨ï¼šåºå· + **åŠ ç²—å²—ä½å** + è–ªèµ„ + åœ°ç‚¹ + åŒ¹é…æŠ€èƒ½
- ä¸­æ–‡å›å¤ï¼Œä¸“ä¸šç®€æ´ï¼Œé¿å…æ— æ•ˆåºŸè¯"""

# è¿½é—®æ¨¡å¼è¿½åŠ çš„åŠ¨æ€æç¤ºï¼ˆç”± state_modifier åœ¨æ£€æµ‹åˆ°è¿½é—®æ—¶æ³¨å…¥ï¼‰
# å‚è€ƒ When2Call 2025 è®ºæ–‡å®è·µï¼šæ˜¾å¼å£°æ˜"å·²æœ‰æ•°æ®"å¯å°†ä¸å¿…è¦å·¥å…·è°ƒç”¨å‡å°‘ 50%+
_FOLLOWUP_ADDON = """

## âš¡ æœ¬è½®æ¨¡å¼ï¼šå¯¹è¯å†å²åˆ†æï¼ˆæœ€é«˜ä¼˜å…ˆçº§è¦†ç›–ï¼‰
ç³»ç»Ÿå·²æ£€æµ‹åˆ°æœ¬è½®ä¸ºè¿½é—®ç±»è¯·æ±‚ï¼Œå¯¹è¯å†å²ä¸­åŒ…å«å®Œæ•´çš„å²—ä½/æ•°æ®ã€‚
æœ¬è½®è§„åˆ™ï¼š**ä¸¥ç¦è°ƒç”¨ä»»ä½•å·¥å…·**ï¼Œç›´æ¥åŸºäºå·²æœ‰æ•°æ®åˆ†æå¹¶ç»™å‡ºå…·ä½“å»ºè®®ã€‚"""

# è¿½é—®å…³é”®è¯ï¼ˆä¸ SYSTEM_PROMPT è§„åˆ™ A ä¿æŒåŒæ­¥ï¼‰
# å«è¿™äº›è¯ + æ— æœç´¢æ„å›¾ + æœ‰ AI å†å²å›å¤ â†’ è¿½é—®æ¨¡å¼
_FOLLOWUP_WORDS = (
    'ä¸Šé¢', 'ä¸Šè¾¹', 'å‰é¢', 'è¿™äº›', 'è¿™ä¸ª', 'é‚£äº›', 'é‚£ä¸ª', 'å®ƒä»¬',
    'å“ªä¸ª', 'å“ªäº›', 'å“ªå®¶', 'æ¯”è¾ƒ', 'å¯¹æ¯”', 'æ›´å¥½', 'æ›´é€‚åˆ',
    'æœ€å¥½', 'æœ€é€‚åˆ', 'ç¬¬ä¸€ä¸ª', 'ç¬¬äºŒä¸ª', 'ç¬¬ä¸‰ä¸ª', 'é€‚åˆæˆ‘', 'æ¨èå“ª',
    'æ’åº', 'æ’ä¸€ä¸‹', 'æ€ä¹ˆæ’', 'ç¬¬å‡ ', 'å“ªä¸€',
)
# æœ‰è¿™äº›è¯è¯´æ˜ç”¨æˆ·åœ¨å‘èµ·æ–°æœç´¢ï¼Œä¼˜å…ˆçº§é«˜äºè¿½é—®è¯
_SEARCH_TRIGGER = ('æœç´¢', 'å¸®æˆ‘æ‰¾', 'æŸ¥æ‰¾', 'æ‰¾å²—ä½', 'æ‰¾èŒä½', 'æœä¸€ä¸‹', 'æœ‰å“ªäº›å²—ä½', 'æ‰¾ä¸€ä¸‹')


class JobRecommendAgent:
    """
    èŒä½æ¨èAgentï¼ˆLangGraph create_react_agentï¼Œå…¼å®¹ LangChain 1.xï¼‰

    åŠŸèƒ½:
    - å¤šè½®å¯¹è¯ï¼ˆæ‰‹åŠ¨ç»´æŠ¤ messages å†å²ï¼‰
    - è‡ªåŠ¨è°ƒç”¨å·¥å…·ï¼ˆRAGæ£€ç´¢ã€æŠ€èƒ½åˆ†æã€Neo4jæŸ¥è¯¢ï¼‰
    - è®°å¿†ä¸Šä¸‹æ–‡
    """

    def __init__(self, config_path: str = "config.yaml", rag_service=None):
        """
        åˆå§‹åŒ–Agent

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
            rag_service: å·²æœ‰çš„ RAGService å®ä¾‹ï¼ˆä¼ å…¥å¯é¿å…é‡å¤åŠ è½½ VectorDB/m3e-baseï¼‰
        """
        logger.info("åˆå§‹åŒ–èŒä½æ¨èAgent...")

        _project_root = Path(__file__).parent.parent.parent
        config_file = _project_root / config_path
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        llm_config = config['llm']
        self._verbose = config.get('agent', {}).get('verbose', True)
        self._max_iterations = config.get('agent', {}).get('max_iterations', 3)

        # åˆå§‹åŒ–LLM
        # enable_thinking=Falseï¼šå…³é—­ Qwen3 ç³»åˆ—çš„æ€ç»´é“¾(CoT)æ¨¡å¼
        # æ€ç»´é“¾é»˜è®¤å¼€å¯ï¼Œä¼šè®© AI å…ˆå†…éƒ¨"æ€è€ƒ" 5-30 ç§’å†å›ç­”ï¼Œå¯¹è¯åœºæ™¯æ— å¿…è¦
        self.llm = ChatOpenAI(
            base_url=llm_config['base_url'],
            api_key=llm_config['api_key'],
            model=llm_config['model'],
            temperature=llm_config.get('temperature', 0.3),
            max_tokens=llm_config.get('max_tokens', 1000),
            extra_body={"enable_thinking": False},
        )

        # åˆå§‹åŒ–å·¥å…·ï¼ˆä¼ å…¥å·²æœ‰ rag_service é¿å…é‡å¤åŠ è½½ï¼‰
        self.agent_tools = AgentTools(rag_service=rag_service)
        self.tools = self.agent_tools.get_tools()

        # ä½¿ç”¨ LangGraph create_react_agentï¼ˆLangChain 1.x æ¨èæ–¹å¼ï¼‰
        # åˆ›å»ºä¸¤ä¸ª graphï¼Œå…¼å®¹ LangGraph 1.0.xï¼ˆstate_modifier callable ç­¾ååœ¨ 1.0.x ä¸ç¨³å®šï¼‰ï¼š
        # - graph: é€šç”¨è·¯å¾„ï¼ˆæœç´¢/æ·±åº¦åˆ†æï¼‰
        # - followup_graph: è¿½é—®è·¯å¾„ï¼ˆæœ«å°¾è¿½åŠ "ç¦æ­¢è°ƒå·¥å…·"æŒ‡ä»¤ï¼Œé˜²æ­¢é‡å¤æ£€ç´¢ï¼‰
        self.graph = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt=SYSTEM_PROMPT,
        )
        self.followup_graph = create_react_agent(
            model=self.llm,
            tools=self.tools,
            prompt=SYSTEM_PROMPT + _FOLLOWUP_ADDON,
        )

        # æŒ‰ session_id éš”ç¦»çš„å¯¹è¯å†å²ï¼Œé¿å…å¤šç”¨æˆ·ä¸²è¯
        self._sessions: dict = {}

        logger.info(f"Agentåˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨å·¥å…·: {len(self.tools)}")

    def close(self):
        """å…³é—­ Agent æŒæœ‰çš„å¤–éƒ¨è¿æ¥ï¼ˆNeo4j ç­‰ï¼‰"""
        if self.agent_tools and self.agent_tools.neo4j_available:
            self.agent_tools.neo4j.close()

    def chat(self, user_input: str, session_id: str = "default") -> str:
        """
        å¯¹è¯æ¥å£ï¼ˆæŒ‰ session_id éš”ç¦»ï¼Œä¸åŒç”¨æˆ·/ä¼šè¯äº’ä¸å¹²æ‰°ï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            session_id: ä¼šè¯ IDï¼Œç›¸åŒ ID å…±äº«å†å²ï¼Œä¸åŒ ID äº’ç›¸éš”ç¦»

        Returns:
            Agentå“åº”
        """
        try:
            logger.info(f"[session={session_id}] ç”¨æˆ·è¾“å…¥: {user_input}")

            # è·å–æˆ–åˆå§‹åŒ–è¯¥ session çš„å¯¹è¯å†å²
            if session_id not in self._sessions:
                self._sessions[session_id] = []
            messages = self._sessions[session_id]

            # è¿½åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append(HumanMessage(content=user_input))

            # é™åˆ¶å†å²é•¿åº¦
            MAX_HISTORY = 8
            if len(messages) > MAX_HISTORY:
                messages = messages[-MAX_HISTORY:]
                self._sessions[session_id] = messages

            # è°ƒç”¨ LangGraph agentï¼ˆéæµå¼æ¥å£ï¼Œè¿½é—®åŒæ ·é™ä½ recursion_limitï¼‰
            _fq = (
                any(w in user_input for w in _FOLLOWUP_WORDS)
                and not any(k in user_input for k in _SEARCH_TRIGGER)
                and len(messages) > 1
            )
            active = self.followup_graph if _fq else self.graph
            result = active.invoke(
                {"messages": messages},
                config={"recursion_limit": 3 if _fq else 8}
            )

            # å–æœ€åä¸€æ¡ AIMessage ä½œä¸ºå“åº”
            all_messages = result.get("messages", [])
            response = ""
            for msg in reversed(all_messages):
                if isinstance(msg, AIMessage):
                    response = msg.content
                    break

            if not response:
                response = "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›å¤ã€‚"

            # ç”¨ graph è¿”å›çš„å®Œæ•´æ¶ˆæ¯åˆ—è¡¨æ›´æ–°è¯¥ session å†å²
            self._sessions[session_id] = all_messages

            logger.info(f"[session={session_id}] Agentå“åº”: {response[:100]}...")
            return response

        except Exception as e:
            logger.error(f"Agentå¤„ç†å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†å‡ºé”™: {str(e)}"

    @staticmethod
    def _detect_direct_action(text: str):
        """
        æ£€æµ‹æ˜¯å¦ä¸ºå¯ç›´æ¥æ‰§è¡Œçš„æœç´¢/æ¨èæ„å›¾ï¼Œè¿”å› (action, query, city)ã€‚
        action: None | 'search' | 'recommend'

        ç›´æ¥è·¯å¾„å®Œå…¨è·³è¿‡ LangGraph çš„ä¸¤æ¬¡ LLM æ¨æ–­ï¼Œå“åº”é€Ÿåº¦ä» 5-10s é™åˆ° <1sã€‚
        åªåœ¨æ„å›¾æ˜ç¡®æ—¶å¯ç”¨ï¼Œå¤æ‚é—®é¢˜ä»èµ° LangGraphã€‚
        """
        stripped = text.strip()
        if len(stripped) > 80:
            return None, "", ""

        lower = stripped.lower()

        # â”€â”€ çŸ¥è¯†/å»ºè®®ç±»æŸ¥è¯¢ä¼˜å…ˆæ’é™¤ï¼Œäº¤ç»™ LLM å›ç­” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ä¾‹ï¼š"Pythonåç«¯éœ€è¦å“ªäº›æŠ€èƒ½"ã€"å¦‚ä½•æˆä¸ºå…¨æ ˆå·¥ç¨‹å¸ˆ"ã€"Javaè–ªèµ„è¡Œæƒ…"
        knowledge_patterns = [
            'éœ€è¦å“ªäº›', 'éœ€è¦ä»€ä¹ˆ', 'å“ªäº›æŠ€èƒ½', 'ä»€ä¹ˆæŠ€èƒ½', 'æŠ€èƒ½è¦æ±‚',
            'å¦‚ä½•æˆä¸º', 'æ€ä¹ˆæˆä¸º', 'å¦‚ä½•å­¦', 'æ€ä¹ˆå­¦', 'å­¦ä¹ è·¯å¾„',
            'å­¦ä¹ è·¯çº¿', 'æŠ€æœ¯è·¯çº¿', 'å­¦ä»€ä¹ˆ', 'éœ€è¦æŒæ¡', 'å‘å±•å‰æ™¯',
            'å‰æ™¯æ€ä¹ˆ', 'è–ªèµ„è¡Œæƒ…', 'å¹³å‡è–ªèµ„', 'å¹³å‡å·¥èµ„', 'å·¥èµ„æ°´å¹³',
            'å¹´è–ªå¤šå°‘', 'æœˆè–ªå¤šå°‘', 'å¤§æ¦‚å¤šå°‘', 'æœ‰ä»€ä¹ˆåŒºåˆ«', 'å’Œ.*çš„åŒºåˆ«',
        ]
        # â”€â”€ æŠ€èƒ½å·®è·/åˆ†æç±»æŸ¥è¯¢æ’é™¤ç›´æ¥è·¯å¾„ï¼Œå¿…é¡»è¿› LangGraph è°ƒ analyze_skill_gap â”€â”€
        # ä¾‹ï¼š"æˆ‘çš„æŠ€èƒ½å·®è·åœ¨å“ªé‡Œ"ã€"å¸®æˆ‘åˆ†ææŠ€èƒ½å·®è·"ã€"åšxxxéœ€è¦è¡¥å……å“ªäº›æŠ€èƒ½"
        gap_analysis_patterns = [
            'æŠ€èƒ½å·®è·', 'å·®è·åœ¨å“ª', 'å·®è·åˆ†æ', 'æŠ€èƒ½ç¼ºå£', 'å·®å‡ ä¸ª',
            'å¸®æˆ‘åˆ†æ', 'åˆ†æä¸€ä¸‹', 'åˆ†ææˆ‘çš„', 'æˆ‘çš„å·®è·', 'ç›®å‰çš„å·®è·',
            'éœ€è¦è¡¥å……', 'è¿˜éœ€è¦å­¦', 'è¿˜å·®ä»€ä¹ˆ', 'è¿˜ç¼ºä»€ä¹ˆ', 'è¿˜ç¼ºå“ªäº›',
        ]
        if any(w in stripped for w in knowledge_patterns):
            return None, "", ""
        if any(w in stripped for w in gap_analysis_patterns):
            return None, "", ""  # è·¯ç”±åˆ° LangGraphï¼Œç”± analyze_skill_gap å·¥å…·å¤„ç†

        # æœç´¢è§¦å‘è¯ï¼ˆå®½æ³›ï¼šä»¥"æ‰¾"å¼€å¤´ã€å«"æœ"ç­‰ï¼‰
        search_triggers = [
            'å¸®æˆ‘æ‰¾', 'å¸®æ‰¾', 'æ‰¾æ‰¾', 'æœç´¢', 'æœä¸€ä¸‹', 'æŸ¥æ‰¾', 'æŸ¥è¯¢',
            'æœ‰å“ªäº›', 'æœ‰ä»€ä¹ˆ', 'æ‰¾ä¸€ä¸‹', 'æ‰¾ä¸‹',
        ]
        # "æ‰¾" å•å­—è§¦å‘ï¼šåªåœ¨å¼€å¤´æˆ–æ˜æ˜¾æ˜¯åŠ¨è¯æ—¶ï¼ˆé¿å…"æ‰¾ä¸åˆ°"ç­‰è¯¯è§¦ï¼‰
        starts_with_find = stripped.startswith('æ‰¾') and len(stripped) >= 4
        # æ¨èè§¦å‘è¯ï¼ˆç”¨æˆ·ä¸»åŠ¨è¯´å‡ºè‡ªå·±çš„æŠ€èƒ½ï¼‰
        recommend_triggers = ['æ¨è', 'é€‚åˆæˆ‘', 'åˆé€‚çš„å²—ä½', 'é€‚åˆçš„èŒä½']
        # å²—ä½ç›¸å…³è¯ï¼ˆåˆ¤æ–­æ˜¯å¦ä¸ºå²—ä½æœç´¢è¯­å¢ƒï¼‰
        job_words = [
            'èŒä½', 'å²—ä½', 'å·¥ä½œ', 'å·¥ç¨‹å¸ˆ', 'å¼€å‘', 'ç¨‹åºå‘˜',
            'æ¶æ„å¸ˆ', 'è¿ç»´', 'æµ‹è¯•', 'äº§å“ç»ç†', 'æ•°æ®',
            'å…¨æ ˆ', 'å‰ç«¯', 'åç«¯', 'ç®—æ³•', 'æ¶æ„', 'devops',
        ]
        # è¯­ä¹‰æœç´¢ä¿¡å·ï¼šå«è¿™äº›è¯è¯´æ˜æ˜¯æ¨¡ç³Š/è¯­ä¹‰æŸ¥è¯¢ï¼Œåº”èµ° RAG
        semantic_signals = ['ç±»ä¼¼', 'ç›¸å…³', 'æåˆ°', 'JD', 'jd', 'æè¿°', 'è¯´æ˜', 'æ¶‰åŠ', 'åŒ…å«', 'æœ‰å…³']

        has_job = any(w in stripped or w in lower for w in job_words)
        has_search = any(w in stripped for w in search_triggers) or starts_with_find
        has_recommend = any(w in stripped for w in recommend_triggers)
        has_semantic = any(w in stripped or w in lower for w in semantic_signals)

        if not has_job:
            return None, "", ""

        # æå–åŸå¸‚
        city = next((c for c in _CITIES if c in stripped), "")

        # æ¨èæ„å›¾ï¼šç”¨æˆ·è¯´"æˆ‘ä¼šXXX"/"æˆ‘æ‡‚XXX"ç­‰ï¼Œæˆ–å«"æ¨è"/"é€‚åˆæˆ‘"
        i_know = any(w in stripped for w in ('æˆ‘ä¼š', 'æˆ‘æ‡‚', 'æˆ‘ç†Ÿæ‚‰', 'æˆ‘å­¦è¿‡', 'æˆ‘æŒæ¡'))
        if has_recommend or (i_know and (has_job or has_search)):
            return 'recommend', stripped, city

        # æœç´¢æ„å›¾ï¼šæœ‰æ˜¾å¼æœç´¢è¯
        if has_search:
            return 'search', stripped, city

        # è¯­ä¹‰æœç´¢æ„å›¾ï¼šå«"ç±»ä¼¼/JDé‡Œ/æåˆ°"ç­‰æ¨¡ç³Šæè¿° + å²—ä½è¯ â†’ èµ°ç›´æ¥è·¯å¾„ï¼ˆRAG å…œåº•ï¼‰
        # ä¾‹ï¼š"æ‰¾ç±»ä¼¼å…¨æ ˆå¼€å‘çš„å·¥ä½œ"ã€"JD é‡Œæåˆ°å¾®æœåŠ¡æ¶æ„çš„å²—ä½"
        if has_semantic and has_job:
            return 'search', stripped, city

        # å«èŒä½å…³é”®è¯ + åŸå¸‚ï¼ˆæ— å…¶ä»–è§¦å‘è¯ï¼‰
        if has_job and city:
            return 'search', stripped, city

        # çº¯å²—ä½è¯æŸ¥è¯¢ï¼ˆ5 å­—ä»¥ä¸Šä¸”ä¸åƒé—²èŠ/çŸ¥è¯†è¯¢é—®ï¼‰
        # ä¾‹ï¼š"å…¨æ ˆå¼€å‘å·¥ç¨‹å¸ˆ"ã€"Pythonåç«¯å²—ä½"
        _not_search = ('æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'ä»€ä¹ˆæ˜¯', 'ä»‹ç»', 'è§£é‡Š', 'åŒºåˆ«',
                       'éœ€è¦å“ªäº›', 'éœ€è¦ä»€ä¹ˆ', 'å“ªäº›æŠ€èƒ½', 'ä»€ä¹ˆæŠ€èƒ½', 'éœ€è¦æŒæ¡',
                       'æ€ä¹ˆå­¦', 'å¦‚ä½•å­¦', 'å­¦ä»€ä¹ˆ', 'å‰æ™¯', 'è–ªèµ„')
        if has_job and len(stripped) >= 5 and not any(w in stripped for w in _not_search):
            return 'search', stripped, city

        return None, "", ""

    @staticmethod
    def _is_planning_query(text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºè§„åˆ’/å»ºè®®/å­¦ä¹ ç±»æŸ¥è¯¢ï¼Œæ— éœ€è°ƒç”¨å·¥å…·ï¼Œç›´æ¥èµ° LLM æµå¼å›ç­”ã€‚

        è¿™ç±»é—®é¢˜ä¸éœ€è¦æ£€ç´¢å²—ä½æ•°æ®ï¼ŒLangGraph åè€Œä¼šå› ä¸ºæ— æ³•åŒ¹é…å·¥å…·è€Œå¤±è´¥æˆ–è¶…æ—¶ã€‚
        ç›´æ¥èµ° LLM å¯ä»¥ä¿è¯ 100% æœ‰å›å¤ï¼Œä¸”é€Ÿåº¦æ›´å¿«ã€‚
        """
        # æ³¨æ„ï¼šæŠ€èƒ½å·®è·/åˆ†æç±»ï¼ˆ'æŠ€èƒ½å·®è·','å·®è·åˆ†æ','æŠ€èƒ½ç¼ºå£','å¸®æˆ‘åˆ†æ','è¿˜ç¼ºä»€ä¹ˆ'ç­‰ï¼‰
        # ä¸åœ¨æ­¤åˆ—è¡¨â€”â€”å®ƒä»¬åº”è·¯ç”±åˆ° LangGraph ç”± analyze_skill_gap å·¥å…·å¤„ç†ã€‚
        planning_keywords = [
            'å­¦ä¹ è®¡åˆ’', 'å­¦ä¹ è·¯å¾„', 'å­¦ä¹ è·¯çº¿', 'æå‡è®¡åˆ’', 'æŠ€èƒ½æå‡è®¡åˆ’',
            'å¦‚ä½•æå‡', 'æ€ä¹ˆæå‡', 'æ€ä¹ˆå­¦ä¹ ', 'å¦‚ä½•å­¦ä¹ ', 'å­¦ä¹ å»ºè®®',
            'æ±‚èŒå»ºè®®', 'èŒä¸šè§„åˆ’', 'èŒä¸šå‘å±•', 'æˆé•¿è·¯å¾„', 'å‘å±•æ–¹å‘',
            'å¸®æˆ‘åˆ¶å®š', 'åˆ¶å®šè®¡åˆ’', 'åˆ¶å®šæ–¹æ¡ˆ', 'åˆ¶å®šè·¯çº¿', 'ç»™æˆ‘å»ºè®®',
            'æ¨èèµ„æº', 'å­¦ä¹ èµ„æ–™', 'å­¦ä¹ é¡ºåº', 'é¢„è®¡æ—¶é—´', 'é‡ç‚¹ç»“åˆ',
            'æˆ‘è¿˜ç¼ºå°‘', 'æˆ‘è¿˜ç¼º',   # æ¥è‡ª MatchDashboard æ¡¥æ¥ promptï¼Œçº¯å­¦ä¹ å»ºè®®
            'ç®€å†ä¼˜åŒ–', 'ç®€å†åˆ†æ', 'ç®€å†å»ºè®®', 'ç®€å†æ”¹è¿›', 'ä¼˜åŒ–å»ºè®®',
            'é¢è¯•æŠ€å·§', 'é¢è¯•å‡†å¤‡', 'å¦‚ä½•å‡†å¤‡', 'é¢è¯•ç»éªŒ',
        ]
        return any(kw in text for kw in planning_keywords)

    @staticmethod
    def _is_simple_chat(text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•é—²èŠ/é—®å€™ï¼Œæ— éœ€è°ƒç”¨å·¥å…·ã€‚
        ç®€å•é—®é¢˜ç›´æ¥èµ° LLM æµå¼ï¼Œè·³è¿‡ LangGraph å¼€é”€ï¼Œé¦– token æ›´å¿«ã€‚

        ä»¥ä¸‹æƒ…å†µä¸èµ°ç®€å•è·¯å¾„ï¼Œäº¤ç»™ LangGraph å¤„ç†ï¼š
        1. å«æŠ€æœ¯å…³é”®è¯ï¼ˆå²—ä½/æŠ€èƒ½/è–ªèµ„ç­‰ï¼‰
        2. å«è¿½é—®ä»£è¯ï¼ˆè¿™äº›/ä»–ä»¬/ä¸Šé¢çš„ç­‰ï¼‰â€”â€”éœ€è¦ç»“åˆä¸Šæ–‡å›ç­”ï¼ŒLangGraph æ›´å‡†ç¡®
        3. å«è¯„ä¼°/æ¯”è¾ƒç±»é—®é¢˜ï¼ˆé€‚åˆ/å“ªä¸ªæ›´å¥½/å¯¹æ¯”ç­‰ï¼‰
        """
        text = text.strip()
        if len(text) > 40:
            return False

        # è¿½é—®ä¸Šæ–‡çš„ä»£è¯/è¿è¯ â†’ éœ€è¦ç»“åˆå¯¹è¯å†å²åˆ†æï¼Œèµ° LangGraph
        followup_words = [
            'è¿™äº›', 'è¿™ä¸ª', 'é‚£äº›', 'é‚£ä¸ª', 'å®ƒä»¬', 'ä»–ä»¬', 'ä¸Šé¢', 'å‰é¢',
            'åˆšæ‰', 'ä¹‹å‰', 'è¿™ä»½', 'å“ªä¸ª', 'å“ªäº›', 'å“ªå®¶', 'æ¯”è¾ƒ', 'å¯¹æ¯”',
            'æ›´å¥½', 'æ›´é€‚åˆ', 'æœ€å¥½', 'æœ€é€‚åˆ', 'æœ‰æ²¡æœ‰æ›´', 'è¿˜æœ‰å—',
        ]
        if any(w in text for w in followup_words):
            return False

        # è¯„ä¼°/åˆ†æç±»è¯ â†’ éœ€è¦æ¨ç†ï¼Œèµ° LangGraph
        analysis_words = ['é€‚åˆ', 'åˆé€‚', 'åŒ¹é…', 'é€‚ä¸é€‚', 'å¥½ä¸å¥½', 'æ€ä¹ˆæ ·', 'å¦‚ä½•', 'è¯„ä»·']
        if any(w in text for w in analysis_words):
            return False

        # æŠ€æœ¯å…³é”®è¯ â†’ äº¤ç»™ LangGraph å¤„ç†
        tech_keywords = [
            'å²—ä½', 'èŒä½', 'æ‹›è˜', 'å·¥ä½œ', 'æŠ€èƒ½', 'è–ªèµ„', 'è–ªé…¬', 'å·¥èµ„',
            'æ¨è', 'æœç´¢', 'åˆ†æ', 'å·®è·', 'å›¾è°±', 'å­¦ä¹ ', 'è·¯å¾„', 'è½¬',
            'python', 'java', 'go', 'vue', 'react', 'node', 'docker',
            'k8s', 'ai', 'ml', 'ç®—æ³•', 'å‰ç«¯', 'åç«¯', 'å…¨æ ˆ', 'è¿ç»´',
        ]
        lower = text.lower()
        return not any(kw in lower for kw in tech_keywords)

    async def async_chat_stream(self, user_input: str, session_id: str = "default", mode: str = "auto") -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯æ¥å£ â€”â€” é€šè¿‡ astream_events é€ token æ¨é€

        SSE äº‹ä»¶æ ¼å¼ï¼š
          event: session / data: <id>   â€”â€” ä¼šè¯æ ‡è¯†ï¼ˆå‰ç«¯ skipï¼‰
          event: tool_status / data: â€¦  â€”â€” æ€è€ƒé˜¶æ®µçŠ¶æ€æ–‡å­—ï¼ˆæ—‹è½¬åœˆæ—ï¼‰
          data: <token>                 â€”â€” æ–‡æœ¬ token
          data: [DONE]                  â€”â€” æµç»“æŸ
          event: error / data: â€¦        â€”â€” é”™è¯¯
        """
        # â”€â”€ ç¬¬ä¸€æ­¥ï¼šç«‹å³ yield ä¸€ä¸ªç©ºæ³¨é‡Šï¼Œè§¦å‘ HTTP å“åº”å¤´å‘é€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ä½œç”¨ï¼šè®© FastAPI StreamingResponse ç«‹åˆ»å‘é€ HTTP å“åº”å¤´ï¼ˆsession event
        # å·²ç”± main.py çš„ event_generator å‘å‡ºï¼Œè¿™é‡Œåªéœ€è¦è®©æµå¼€å§‹å³å¯ï¼‰ï¼Œ
        # å‰ç«¯ fetch() ç«‹åˆ» resolve â†’ pushMsgBubble() â†’ AI æ°”æ³¡ + æ—‹è½¬åœˆç«‹åˆ»å‡ºç°ã€‚
        yield ": ping\n\n"

        try:
            logger.info(f"[stream][session={session_id}] ç”¨æˆ·è¾“å…¥: {user_input}")

            if session_id not in self._sessions:
                self._sessions[session_id] = []
            messages = self._sessions[session_id]
            messages.append(HumanMessage(content=user_input))

            # é™åˆ¶å†å²é•¿åº¦ï¼šæœ€å¤šä¿ç•™æœ€è¿‘ 8 æ¡ï¼ˆ4 è½®ï¼‰ï¼Œé˜²æ­¢ token æ•°è†¨èƒ€æ‹–æ…¢ API
            MAX_HISTORY = 8
            if len(messages) > MAX_HISTORY:
                messages = messages[-MAX_HISTORY:]
                self._sessions[session_id] = messages

            # â”€â”€ mode=llmï¼šå¼ºåˆ¶èµ°æ¨¡å‹ï¼Œè·³è¿‡æ‰€æœ‰å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if mode == "llm":
                logger.info(f"[stream][session={session_id}] å¼ºåˆ¶æ¨¡å‹æ¨¡å¼")
                yield "event: source\ndata: llm\n\n"
                yield "event: tool_status\ndata: ğŸ’¬ æ™ºè˜åŠ©æ‰‹å›å¤ä¸­...\n\n"
                sys_msg = SystemMessage(content=SYSTEM_PROMPT)
                chat_messages = [sys_msg] + messages
                full_response = []
                async for chunk in self.llm.astream(chat_messages):
                    content = getattr(chunk, 'content', '') or ''
                    if content:
                        full_response.append(content)
                        safe = content.replace('\n', '\\n')
                        yield f"data: {safe}\n\n"
                yield "event: tool_status\ndata: \n\n"
                yield "data: [DONE]\n\n"
                messages.append(AIMessage(content="".join(full_response)))
                self._sessions[session_id] = messages
                return

            # â”€â”€ ç›´æ¥å·¥å…·è·¯å¾„ï¼šæ„å›¾æ˜ç¡®çš„æœç´¢/æ¨èï¼Œå®Œå…¨è·³è¿‡ LangGraph ä¸¤æ¬¡ LLM æ¨æ–­ â”€â”€
            # å“åº”æ—¶é—´ï¼šLangGraph 5-10s â†’ ç›´æ¥è·¯å¾„ <2sï¼ˆåªæœ‰ Neo4j æŸ¥è¯¢ï¼‰
            # mode=graph/rag æ—¶å¼ºåˆ¶èµ°ç›´æ¥è·¯å¾„ï¼ˆè·³è¿‡æ„å›¾æ£€æµ‹ï¼‰
            if mode in ("graph", "rag"):
                auto_action = "search"  # é auto æ—¶é»˜è®¤ search
                city = next((c for c in _CITIES if c in user_input), "")
                action = auto_action
            else:
                action, _, city = self._detect_direct_action(user_input)
            if action in ('search', 'recommend'):
                logger.info(f"[stream][session={session_id}] ç›´æ¥å·¥å…·è·¯å¾„: action={action}, city={city or 'å…¨å›½'}")
                tip = "âš¡ æ­£åœ¨æ£€ç´¢å²—ä½æ•°æ®åº“..." if action == 'search' else "ğŸ¯ æ­£åœ¨åŒ¹é…æ¨èå²—ä½..."
                yield f"event: tool_status\ndata: {tip}\n\n"
                force_source = mode if mode in ("graph", "rag") else "auto"
                result_tuple = None
                try:
                    if action == 'search':
                        result_tuple = await asyncio.wait_for(
                            asyncio.to_thread(self.agent_tools.search_direct, user_input, city, force_source),
                            timeout=8.0,
                        )
                    else:
                        # skills æå–å¤±è´¥æ—¶æ”¹ç”¨ search_directï¼ˆä¸æ”¾å¼ƒç›´æ¥è·¯å¾„ï¼‰
                        skills = AgentTools._extract_skills(user_input)
                        if skills:
                            result_tuple = await asyncio.wait_for(
                                asyncio.to_thread(self.agent_tools.recommend_direct, skills, city, force_source),
                                timeout=8.0,
                            )
                        else:
                            # æ²¡æå–åˆ°æŠ€èƒ½è¯ â†’ é€€å› search_direct å…œåº•
                            result_tuple = await asyncio.wait_for(
                                asyncio.to_thread(self.agent_tools.search_direct, user_input, city, force_source),
                                timeout=8.0,
                            )
                except asyncio.TimeoutError:
                    logger.warning(f"[stream] ç›´æ¥è·¯å¾„è¶…æ—¶ï¼Œé™çº§åˆ° LangGraph")
                    result_tuple = None
                except Exception as e:
                    logger.warning(f"[stream] ç›´æ¥è·¯å¾„å¤±è´¥ï¼ˆ{e}ï¼‰ï¼Œé™çº§åˆ° LangGraph")
                    result_tuple = None

                if result_tuple:
                    result, source_type = result_tuple
                    logger.info(f"[stream] ç›´æ¥è·¯å¾„è¿”å› {len(result)} å­—ç¬¦ï¼Œæ¥æº={source_type}")
                    # é€šçŸ¥å‰ç«¯æ•°æ®æ¥æºï¼ˆgraph/ragï¼‰
                    yield f"event: source\ndata: {source_type}\n\n"
                    # æ¸…é™¤ tool_statusï¼ŒæŒ‰æ®µè½åˆ†å—æ¨é€
                    yield "event: tool_status\ndata: \n\n"
                    for para in result.split('\n\n'):
                        safe = para.replace('\n', '\\n')
                        if safe:
                            yield f"data: {safe}\\n\\n\n\n"
                    messages.append(AIMessage(content=result))
                    self._sessions[session_id] = messages
                    yield "data: [DONE]\n\n"
                    return
                # result ä¸ºç©ºæ—¶é™é»˜é™çº§åˆ°ä¸‹é¢çš„è·¯å¾„

            # â”€â”€ ç®€å•é—²èŠå¿«é€Ÿè·¯å¾„ï¼šè·³è¿‡ LangGraphï¼Œç›´æ¥æµå¼è°ƒ LLM â”€â”€
            if self._is_simple_chat(user_input):
                logger.info(f"[stream][session={session_id}] ç®€å•å¯¹è¯å¿«é€Ÿè·¯å¾„")
                yield "event: source\ndata: llm\n\n"
                yield "event: tool_status\ndata: ğŸ’¬ æ™ºè˜åŠ©æ‰‹å›å¤ä¸­...\n\n"
                sys_msg = SystemMessage(content=SYSTEM_PROMPT)
                chat_messages = [sys_msg] + messages
                full_response = []
                async for chunk in self.llm.astream(chat_messages):
                    content = getattr(chunk, 'content', '') or ''
                    if content:
                        full_response.append(content)
                        safe = content.replace('\n', '\\n')
                        yield f"data: {safe}\n\n"
                if full_response:
                    messages.append(AIMessage(content=''.join(full_response)))
                    self._sessions[session_id] = messages
                yield "data: [DONE]\n\n"
                return

            # â”€â”€ è§„åˆ’/å»ºè®®ç±»å¿«é€Ÿè·¯å¾„ï¼šå­¦ä¹ è®¡åˆ’ã€ç®€å†å»ºè®®ç­‰ç›´æ¥èµ° LLMï¼Œæ— éœ€å·¥å…· â”€â”€
            # è¿™ç±»é—®é¢˜ LangGraph æ— æ³•åŒ¹é…åˆé€‚çš„å·¥å…·ï¼Œå¸¸å¸¸è¶…æ—¶æˆ–è¿”å›ç©ºï¼Œç›´æ¥ LLM æ›´å¯é 
            if self._is_planning_query(user_input):
                logger.info(f"[stream][session={session_id}] è§„åˆ’å»ºè®®å¿«é€Ÿè·¯å¾„")
                yield "event: source\ndata: llm\n\n"
                yield "event: tool_status\ndata: ğŸ“ AI è§„åˆ’æ–¹æ¡ˆç”Ÿæˆä¸­...\n\n"
                sys_msg = SystemMessage(content=SYSTEM_PROMPT)
                chat_messages = [sys_msg] + messages
                full_response = []
                async for chunk in self.llm.astream(chat_messages):
                    content = getattr(chunk, 'content', '') or ''
                    if content:
                        full_response.append(content)
                        safe = content.replace('\n', '\\n')
                        yield f"data: {safe}\n\n"
                if full_response:
                    messages.append(AIMessage(content=''.join(full_response)))
                    self._sessions[session_id] = messages
                yield "data: [DONE]\n\n"
                return

            full_response = []
            tool_call_count = 0      # ç´¯è®¡å·¥å…·è°ƒç”¨æ¬¡æ•°
            final_answer_started = False  # æ˜¯å¦å·²è¿›å…¥æœ€ç»ˆå›ç­”é˜¶æ®µ

            # è¿½é—®ç±»é—®é¢˜ï¼šrecursion_limit=3ï¼ˆæœ€å¤š 1 æ¬¡å·¥å…·è°ƒç”¨åç›´æ¥å›ç­”ï¼‰
            # æ™®é€šé—®é¢˜ï¼šrecursion_limit=8ï¼ˆå…è®¸å¤šæ­¥æ¨ç†ï¼‰
            _is_followup_now = (
                any(w in user_input for w in _FOLLOWUP_WORDS)
                and not any(k in user_input for k in _SEARCH_TRIGGER)
                and len(messages) > 1
            )
            recursion = 3 if _is_followup_now else 8
            logger.info(f"[stream][session={session_id}] LangGraph è·¯å¾„ recursion_limit={recursion} followup={_is_followup_now}")

            # LangGraph è·¯å¾„ï¼šå…ˆæ ‡è®°ä¸º llmï¼Œå·¥å…·è°ƒç”¨åé€šè¿‡ on_tool_end æ›´æ–°ä¸ºçœŸå®æ¥æº
            yield "event: source\ndata: llm\n\n"
            # è·¯å¾„ä¸“å±åˆå§‹çŠ¶æ€æ–‡å­—ï¼šè®©ç”¨æˆ·ç«‹åˆ»çŸ¥é“ AI åœ¨åšä»€ä¹ˆ
            if _is_followup_now:
                yield "event: tool_status\ndata: ğŸ“Š æ­£åœ¨åˆ†æå†å²å²—ä½æ•°æ®...\n\n"
            else:
                yield "event: tool_status\ndata: ğŸ§  AI æ·±åº¦åˆ†æä¸­...\n\n"

            # è¿½é—®ç”¨ followup_graphï¼ˆå·²å†…ç½®"ç¦æ­¢è°ƒå·¥å…·"æŒ‡ä»¤ï¼‰ï¼Œæ™®é€šç”¨ graph
            active_graph = self.followup_graph if _is_followup_now else self.graph

            async for event in active_graph.astream_events(
                {"messages": messages},
                config={"recursion_limit": recursion},
                version="v2",
            ):
                kind = event.get("event", "")

                # â”€â”€ LLM å¼€å§‹ç”Ÿæˆï¼ˆè§„åˆ’ or æœ€ç»ˆå›ç­”ï¼‰ï¼šæ›´æ–°çŠ¶æ€æ–‡å­—
                if kind == "on_chat_model_start":
                    if tool_call_count == 0:
                        # ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨ï¼šæ­£åœ¨è§„åˆ’ / å‡†å¤‡å›ç­”
                        status = "âœï¸ æ­£åœ¨ç»„ç»‡å›ç­”..." if _is_followup_now else "ğŸ¤” AI æ­£åœ¨æ¨ç†..."
                    else:
                        # å·¥å…·è°ƒç”¨ç»“æŸåçš„ LLM è°ƒç”¨ï¼šæ­£åœ¨æ•´åˆç»“æœ
                        status = "âœï¸ æ­£åœ¨æ•´åˆç»“æœ..."
                    yield f"event: tool_status\ndata: {status}\n\n"

                # â”€â”€ å·¥å…·è°ƒç”¨å¼€å§‹ï¼šæ¨é€ç²¾ç¡®çš„è¿›åº¦æ–‡å­—
                elif kind == "on_tool_start":
                    tool_name = event.get("name", "")
                    tool_tips = {
                        "search_jobs": "ğŸ” æ­£åœ¨æ£€ç´¢å²—ä½æ•°æ®åº“...",
                        "analyze_skill_gap": "ğŸ“Š æ­£åœ¨åˆ†ææŠ€èƒ½å·®è·...",
                        "recommend_jobs": "ğŸ¯ æ­£åœ¨åŒ¹é…æ¨èå²—ä½...",
                        "query_skill_graph": "ğŸ§  æ­£åœ¨æŸ¥è¯¢æŠ€èƒ½å›¾è°±...",
                    }
                    tip = tool_tips.get(tool_name, f"âš™ï¸ æ­£åœ¨å¤„ç†: {tool_name}...")
                    tool_call_count += 1
                    yield f"event: tool_status\ndata: {tip}\n\n"

                # â”€â”€ å·¥å…·è°ƒç”¨ç»“æŸï¼šæ›´æ–°çœŸå®æ¥æº + åˆ‡æ¢ä¸º"æ•´åˆç»“æœ"çŠ¶æ€
                elif kind == "on_tool_end" and tool_call_count > 0:
                    # æ ¹æ®è°ƒç”¨çš„å·¥å…·åæ¨æ–­æ•°æ®æ¥æº
                    tool_name = event.get("name", "")
                    if tool_name in ("search_jobs", "recommend_jobs"):
                        # å›¾è°±/RAG å·¥å…·ï¼šå°è¯•ä»è¾“å‡ºåˆ¤æ–­æ¥æº
                        tool_output = event.get("data", {}).get("output", "")
                        is_rag_output = isinstance(tool_output, str) and "å›¾è°±" not in tool_output[:20]
                        detected = "rag" if is_rag_output else "graph"
                        yield f"event: source\ndata: {detected}\n\n"
                    yield f"event: tool_status\ndata: âœï¸ æ­£åœ¨æ•´åˆç»“æœ...\n\n"

                # â”€â”€ LLM token æµ
                elif kind == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    if not chunk:
                        continue
                    content = getattr(chunk, "content", None)
                    if not content:
                        continue

                    # åˆ¤æ–­æ˜¯è§„åˆ’ tokenï¼ˆtool_call_chunksï¼‰è¿˜æ˜¯æœ€ç»ˆå›ç­” token
                    tool_call_chunks = getattr(chunk, "tool_call_chunks", None)
                    if tool_call_chunks:
                        # è§„åˆ’é˜¶æ®µï¼šLLM åœ¨å†³å®šè°ƒç”¨å“ªä¸ªå·¥å…·ï¼Œä¸æ¨é€å†…å®¹
                        continue

                    # æœ€ç»ˆå›ç­”é˜¶æ®µ token
                    if not final_answer_started and tool_call_count > 0:
                        # å·¥å…·è°ƒå®Œåçš„ç¬¬ä¸€ä¸ª tokenï¼ŒåŠ æ¢è¡Œåˆ†éš”
                        final_answer_started = True

                    full_response.append(content)
                    safe_token = content.replace("\n", "\\n")
                    yield f"data: {safe_token}\n\n"

            # â”€â”€ LangGraph å…œåº•ï¼šè‹¥æ— æ–‡å­—è¾“å‡ºï¼Œé™çº§ä¸ºç›´æ¥ LLM æµå¼å›ç­” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # åŸå› ï¼šLLM è§„åˆ’ token è¢«è¿‡æ»¤ã€recursion_limit è€—å°½ã€API è¶…æ—¶ç­‰å‡å¯å¯¼è‡´
            # full_response ä¸ºç©ºï¼Œå‰ç«¯ä¼šæ˜¾ç¤º"æœªèƒ½ç”Ÿæˆå›å¤"ã€‚é™çº§ç¡®ä¿ç”¨æˆ·å§‹ç»ˆæœ‰å›å¤ã€‚
            if not full_response:
                logger.warning(f"[stream][session={session_id}] LangGraph æ— æ–‡å­—è¾“å‡ºï¼Œé™çº§ä¸ºç›´æ¥ LLM å›ç­”")
                yield "event: tool_status\ndata: ğŸ’¬ æ•´ç†å›å¤ä¸­...\n\n"
                sys_msg = SystemMessage(content=SYSTEM_PROMPT)
                async for chunk in self.llm.astream([sys_msg] + messages):
                    content = getattr(chunk, "content", "") or ""
                    if content:
                        full_response.append(content)
                        safe = content.replace("\n", "\\n")
                        yield f"data: {safe}\n\n"

            # æ›´æ–°å¯¹è¯å†å²
            if full_response:
                complete_text = "".join(full_response).replace("\\n", "\n")
                messages.append(AIMessage(content=complete_text))
                self._sessions[session_id] = messages

            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"[stream] Agentæµå¼å¤„ç†å¤±è´¥: {e}", exc_info=True)
            err_safe = str(e).replace("\n", " ")
            # å¼‚å¸¸é™çº§ï¼šå°è¯•ç”¨ç›´æ¥ LLM å…œåº•ï¼Œå½»åº•å¤±è´¥æ‰è¿”å› error äº‹ä»¶
            try:
                logger.info(f"[stream][session={session_id}] å¼‚å¸¸åé™çº§ä¸ºç›´æ¥ LLM å…œåº•")
                yield "event: tool_status\ndata: ğŸ’¬ é‡æ–°ç”Ÿæˆå›å¤...\n\n"
                sys_msg = SystemMessage(content=SYSTEM_PROMPT)
                fallback_resp = []
                async for chunk in self.llm.astream([sys_msg] + messages):
                    content = getattr(chunk, "content", "") or ""
                    if content:
                        fallback_resp.append(content)
                        safe = content.replace("\n", "\\n")
                        yield f"data: {safe}\n\n"
                if fallback_resp:
                    messages.append(AIMessage(content="".join(fallback_resp)))
                    self._sessions[session_id] = messages
                    yield "data: [DONE]\n\n"
                    return
            except Exception as e2:
                logger.error(f"[stream] å…œåº• LLM ä¹Ÿå¤±è´¥: {e2}")
            yield f"event: error\ndata: æŠ±æ­‰ï¼Œå¤„ç†å‡ºé”™: {err_safe}\n\n"
            yield "data: [DONE]\n\n"

    def reset_memory(self, session_id: str = None):
        """
        é‡ç½®å¯¹è¯å†å²

        Args:
            session_id: æŒ‡å®šä¼šè¯ ID åˆ™åªé‡ç½®è¯¥ä¼šè¯ï¼›ä¸º None åˆ™æ¸…ç©ºæ‰€æœ‰ä¼šè¯
        """
        if session_id is not None:
            self._sessions.pop(session_id, None)
            logger.info(f"ä¼šè¯ {session_id} å†å²å·²æ¸…ç©º")
        else:
            self._sessions.clear()
            logger.info("æ‰€æœ‰ä¼šè¯å†å²å·²æ¸…ç©º")


def interactive_chat():
    """äº¤äº’å¼å¯¹è¯"""
    print("="*80)
    print("èŒä½æ¨èAgent - äº¤äº’å¼å¯¹è¯")
    print("="*80)
    print("\næ¬¢è¿ä½¿ç”¨æ™ºèƒ½èŒä½æ¨èç³»ç»Ÿï¼")
    print("\næˆ‘å¯ä»¥å¸®ä½ :")
    print("  1. æœç´¢ç›¸å…³å²—ä½")
    print("  2. åˆ†ææŠ€èƒ½å·®è·")
    print("  3. æ¨èåˆé€‚å²—ä½")
    print("  4. æŸ¥è¯¢æŠ€èƒ½ä¿¡æ¯")
    print("\nè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("è¾“å…¥ 'reset' é‡ç½®å¯¹è¯å†å²")
    print("\n" + "="*80 + "\n")
    
    try:
        # åˆå§‹åŒ–Agent
        agent = JobRecommendAgent()
        print("âœ… Agentåˆå§‹åŒ–æˆåŠŸï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†ã€‚\n")
        
        # å¯¹è¯å¾ªç¯
        while True:
            try:
                user_input = input("You: ").strip()
                
                if not user_input:
                    continue
                
                if user_input.lower() in ['exit', 'quit']:
                    print("\nå†è§ï¼ğŸ‘‹")
                    break
                
                if user_input.lower() == 'reset':
                    agent.reset_memory()
                    print("âœ… å¯¹è¯å†å²å·²é‡ç½®\n")
                    continue
                
                print()  # ç©ºè¡Œ
                response = agent.chat(user_input)
                print(f"\nAgent: {response}\n")
                print("-"*80 + "\n")
                
            except KeyboardInterrupt:
                print("\n\nå†è§ï¼ğŸ‘‹")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}\n")
                continue
    
    except Exception as e:
        print(f"\nâŒ Agentåˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("  1. config.yamlä¸­çš„API Keyæ˜¯å¦æ­£ç¡®")
        print("  2. å‘é‡æ•°æ®åº“æ˜¯å¦å·²åˆå§‹åŒ–")
        print("  3. Neo4jæ˜¯å¦æ­£åœ¨è¿è¡Œ")


def test_agent():
    """æµ‹è¯•AgentåŠŸèƒ½"""
    print("="*80)
    print("æµ‹è¯•èŒä½æ¨èAgent")
    print("="*80)
    
    try:
        # åˆå§‹åŒ–Agent
        print("\nã€åˆå§‹åŒ–Agentã€‘")
        agent = JobRecommendAgent()
        print("âœ… åˆå§‹åŒ–æˆåŠŸ\n")
        
        # æµ‹è¯•å¯¹è¯
        test_conversations = [
            "æˆ‘ä¼šPythonå’ŒDjangoï¼Œæ¨èä»€ä¹ˆå²—ä½?",
            "æˆ‘è¿˜éœ€è¦å­¦ä¹ ä»€ä¹ˆæŠ€èƒ½?",
            "åŒ—äº¬æœ‰å“ªäº›Pythonåç«¯çš„å²—ä½?"
        ]
        
        print("ã€æµ‹è¯•å¯¹è¯ã€‘")
        for i, user_input in enumerate(test_conversations, 1):
            print(f"\n--- å¯¹è¯ {i} ---")
            print(f"You: {user_input}")
            
            response = agent.chat(user_input)
            print(f"\nAgent: {response[:300]}...")
            
            if i < len(test_conversations):
                print("\n" + "-"*80)
        
        print("\n" + "="*80)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    import argparse
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    parser = argparse.ArgumentParser(description="èŒä½æ¨èAgent")
    parser.add_argument(
        '--test',
        action='store_true',
        help='è¿è¡Œæµ‹è¯•æ¨¡å¼'
    )
    
    args = parser.parse_args()
    
    if args.test:
        test_agent()
    else:
        interactive_chat()
