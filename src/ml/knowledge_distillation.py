"""
çŸ¥è¯†è’¸é¦æ¨¡å—
å°†Qwen3-7Bçš„çŸ¥è¯†è’¸é¦åˆ°è½»é‡çº§åˆ†ç±»å™¨

æ ¸å¿ƒæ€è·¯:
1. ç”¨Qwen3å¤„ç†1-2ä¸‡ä»£è¡¨æ€§æ ·æœ¬ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰
2. æå–ç‰¹å¾: JDå‘é‡ + è§„åˆ™ç‰¹å¾
3. è®­ç»ƒè½»é‡çº§åˆ†ç±»å™¨ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰: LightGBM/XGBoost
4. å­¦ç”Ÿæ¨¡å‹å¤„ç†å‰©ä½™48-49ä¸‡æ•°æ®

ä¼˜åŠ¿:
- é€Ÿåº¦æå‡100å€: 0.1ç§’ vs 10ç§’
- æˆæœ¬é™ä½99%: æœ¬åœ°æ¨ç† vs LLM API
- å‡†ç¡®ç‡ä¿æŒ85-90%: æ¥è¿‘æ•™å¸ˆæ¨¡å‹

æŠ€æœ¯äº®ç‚¹ï¼ˆ2026å¹´çƒ­é—¨ï¼‰:
- çŸ¥è¯†è’¸é¦ (Knowledge Distillation)
- å¤šæ ‡ç­¾åˆ†ç±» (Multi-label Classification)
- ç‰¹å¾å·¥ç¨‹
"""
import logging
import numpy as np
import pickle
from typing import List, Dict, Tuple
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class SkillDistillationModel:
    """æŠ€èƒ½æŠ½å–è’¸é¦æ¨¡å‹"""
    
    def __init__(
        self,
        encoder_model: str = "moka-ai/m3e-base",
        classifier_type: str = "lightgbm"
    ):
        """
        åˆå§‹åŒ–è’¸é¦æ¨¡å‹
        
        Args:
            encoder_model: å‘é‡åŒ–æ¨¡å‹
            classifier_type: åˆ†ç±»å™¨ç±»å‹ ("lightgbm", "xgboost", "random_forest")
        """
        logger.info("="*80)
        logger.info("ğŸ“ åˆå§‹åŒ–çŸ¥è¯†è’¸é¦æ¨¡å‹")
        logger.info("="*80)
        
        # åŠ è½½å‘é‡åŒ–æ¨¡å‹
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"â³ åŠ è½½å‘é‡åŒ–æ¨¡å‹: {encoder_model}")
            self.encoder = SentenceTransformer(encoder_model)
            logger.info("âœ… å‘é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ å‘é‡åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        self.encoder_model = encoder_model
        self.classifier_type = classifier_type
        self.classifier = None
        self.label_encoder = None
        self.skill_list = None  # æ‰€æœ‰å¯èƒ½çš„æŠ€èƒ½åˆ—è¡¨
        
        logger.info(f"âœ… è’¸é¦æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   åˆ†ç±»å™¨ç±»å‹: {classifier_type}")
        logger.info("="*80)
    
    def train(
        self,
        jobs: List[Dict],
        teacher_skill_key: str = 'llm_skills',
        test_size: float = 0.1,
        show_progress: bool = True
    ) -> Dict:
        """
        è®­ç»ƒè’¸é¦æ¨¡å‹
        
        Args:
            jobs: è®­ç»ƒæ•°æ®ï¼ˆåŒ…å«æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼‰
            teacher_skill_key: æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„å­—æ®µå
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("\n" + "="*80)
        logger.info("ğŸ“ å¼€å§‹è®­ç»ƒè’¸é¦æ¨¡å‹")
        logger.info("="*80)
        logger.info(f"è®­ç»ƒæ ·æœ¬: {len(jobs):,} æ¡")
        logger.info(f"æµ‹è¯•é›†æ¯”ä¾‹: {test_size*100:.0f}%")
        logger.info("-"*80)
        
        # 1. æ„å»ºæŠ€èƒ½è¯æ±‡è¡¨
        logger.info("\nğŸ“š [1/5] æ„å»ºæŠ€èƒ½è¯æ±‡è¡¨...")
        self._build_skill_vocabulary(jobs, teacher_skill_key)
        logger.info(f"âœ… æŠ€èƒ½è¯æ±‡è¡¨: {len(self.skill_list)} ä¸ªæŠ€èƒ½")
        
        # 2. æå–ç‰¹å¾
        logger.info("\nğŸ”§ [2/5] æå–ç‰¹å¾...")
        X, y = self._extract_features_and_labels(
            jobs, 
            teacher_skill_key,
            show_progress=show_progress
        )
        logger.info(f"âœ… ç‰¹å¾çŸ©é˜µ: {X.shape}")
        logger.info(f"âœ… æ ‡ç­¾çŸ©é˜µ: {y.shape}")
        
        # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        logger.info("\nâœ‚ï¸ [3/5] åˆ’åˆ†æ•°æ®é›†...")
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        logger.info(f"âœ… è®­ç»ƒé›†: {X_train.shape[0]:,} æ¡")
        logger.info(f"âœ… æµ‹è¯•é›†: {X_test.shape[0]:,} æ¡")
        
        # 4. è®­ç»ƒåˆ†ç±»å™¨
        logger.info(f"\nğŸ‹ï¸ [4/5] è®­ç»ƒ{self.classifier_type}åˆ†ç±»å™¨...")
        self._train_classifier(X_train, y_train)
        logger.info("âœ… åˆ†ç±»å™¨è®­ç»ƒå®Œæˆ")
        
        # 5. è¯„ä¼°
        logger.info("\nğŸ“Š [5/5] è¯„ä¼°æ¨¡å‹...")
        metrics = self._evaluate(X_test, y_test)
        
        logger.info("\n" + "="*80)
        logger.info("âœ… è®­ç»ƒå®Œæˆï¼")
        logger.info("="*80)
        self._print_metrics(metrics)
        
        return metrics
    
    def _build_skill_vocabulary(
        self,
        jobs: List[Dict],
        teacher_skill_key: str
    ):
        """æ„å»ºæŠ€èƒ½è¯æ±‡è¡¨"""
        all_skills = set()
        
        for job in jobs:
            skills = job.get(teacher_skill_key, [])
            if isinstance(skills, list):
                all_skills.update(skills)
        
        # æ’åºï¼ˆä¿è¯ä¸€è‡´æ€§ï¼‰
        self.skill_list = sorted(list(all_skills))
        
        # æ„å»ºæŠ€èƒ½åˆ°ç´¢å¼•çš„æ˜ å°„
        self.skill_to_idx = {skill: idx for idx, skill in enumerate(self.skill_list)}
    
    def _extract_features_and_labels(
        self,
        jobs: List[Dict],
        teacher_skill_key: str,
        show_progress: bool = True
    ) -> Tuple[np.ndarray, np.ndarray]:
        """æå–ç‰¹å¾å’Œæ ‡ç­¾"""
        from tqdm import tqdm
        
        features = []
        labels = []
        
        iterator = jobs
        if show_progress:
            iterator = tqdm(jobs, desc="æå–ç‰¹å¾")
        
        for job in iterator:
            # æå–ç‰¹å¾
            feature_vec = self._extract_single_feature(job)
            features.append(feature_vec)
            
            # æå–æ ‡ç­¾ï¼ˆå¤šæ ‡ç­¾ï¼‰
            label_vec = self._skills_to_multilabel(
                job.get(teacher_skill_key, [])
            )
            labels.append(label_vec)
        
        X = np.array(features)
        y = np.array(labels)
        
        return X, y
    
    def _extract_single_feature(self, job: Dict) -> np.ndarray:
        """æå–å•ä¸ªæ ·æœ¬çš„ç‰¹å¾"""
        features = []
        
        # 1. å‘é‡ç‰¹å¾ï¼ˆ768ç»´ï¼‰
        jd_text = self._extract_jd_text(job)
        embedding = self.encoder.encode(jd_text, show_progress_bar=False)
        features.extend(embedding.tolist())
        
        # 2. ç»Ÿè®¡ç‰¹å¾
        # æ–‡æœ¬é•¿åº¦
        text_length = len(jd_text)
        features.append(text_length / 1000)  # å½’ä¸€åŒ–
        
        # æ˜¾å¼æŠ€èƒ½æ•°
        explicit_skills = len(job.get('skills', []))
        features.append(explicit_skills / 20)  # å½’ä¸€åŒ–
        
        # è–ªèµ„ç‰¹å¾
        salary_min = job.get('salary_min', 0)
        salary_max = job.get('salary_max', 0)
        features.append(salary_min / 50)  # å½’ä¸€åŒ–åˆ°0-1
        features.append(salary_max / 50)
        
        # ç»éªŒè¦æ±‚
        experience = job.get('experience', 'ä¸é™')
        exp_value = self._parse_experience(experience)
        features.append(exp_value / 10)
        
        # å­¦å†è¦æ±‚
        education = job.get('education', 'ä¸é™')
        edu_value = self._parse_education(education)
        features.append(edu_value)
        
        # 3. åŸå¸‚ç‰¹å¾ï¼ˆone-hotç¼–ç ï¼‰
        city = job.get('city', 'æœªçŸ¥')
        city_features = self._encode_city(city)
        features.extend(city_features)
        
        return np.array(features)
    
    def _skills_to_multilabel(self, skills: List[str]) -> np.ndarray:
        """å°†æŠ€èƒ½åˆ—è¡¨è½¬æ¢ä¸ºå¤šæ ‡ç­¾å‘é‡"""
        label_vec = np.zeros(len(self.skill_list), dtype=np.float32)
        
        for skill in skills:
            if skill in self.skill_to_idx:
                idx = self.skill_to_idx[skill]
                label_vec[idx] = 1.0
        
        return label_vec
    
    def _train_classifier(self, X: np.ndarray, y: np.ndarray):
        """è®­ç»ƒåˆ†ç±»å™¨"""
        if self.classifier_type == "lightgbm":
            self._train_lightgbm(X, y)
        elif self.classifier_type == "xgboost":
            self._train_xgboost(X, y)
        elif self.classifier_type == "random_forest":
            self._train_random_forest(X, y)
        else:
            raise ValueError(f"æœªçŸ¥åˆ†ç±»å™¨: {self.classifier_type}")
    
    def _train_lightgbm(self, X: np.ndarray, y: np.ndarray):
        """è®­ç»ƒLightGBMï¼ˆå¤šæ ‡ç­¾ï¼‰"""
        try:
            import lightgbm as lgb
        except ImportError:
            logger.error("âŒ LightGBMæœªå®‰è£…")
            logger.error("è¯·è¿è¡Œ: pip install lightgbm")
            raise
        
        # å¯¹æ¯ä¸ªæŠ€èƒ½è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨
        self.classifier = []
        
        n_skills = y.shape[1]
        logger.info(f"   è®­ç»ƒ{n_skills}ä¸ªäºŒåˆ†ç±»å™¨...")
        
        from tqdm import tqdm
        for skill_idx in tqdm(range(n_skills), desc="LightGBM"):
            y_single = y[:, skill_idx]
            
            # è·³è¿‡æ²¡æœ‰æ­£æ ·æœ¬çš„æŠ€èƒ½
            if y_single.sum() < 5:
                self.classifier.append(None)
                continue
            
            # è®­ç»ƒ
            clf = lgb.LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                num_leaves=31,
                random_state=42,
                verbose=-1
            )
            clf.fit(X, y_single)
            self.classifier.append(clf)
    
    def _train_xgboost(self, X: np.ndarray, y: np.ndarray):
        """è®­ç»ƒXGBoostï¼ˆå¤šæ ‡ç­¾ï¼‰"""
        try:
            import xgboost as xgb
        except ImportError:
            logger.error("âŒ XGBoostæœªå®‰è£…")
            logger.error("è¯·è¿è¡Œ: pip install xgboost")
            raise
        
        self.classifier = []
        n_skills = y.shape[1]
        
        from tqdm import tqdm
        for skill_idx in tqdm(range(n_skills), desc="XGBoost"):
            y_single = y[:, skill_idx]
            
            if y_single.sum() < 5:
                self.classifier.append(None)
                continue
            
            clf = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                use_label_encoder=False,
                eval_metric='logloss'
            )
            clf.fit(X, y_single)
            self.classifier.append(clf)
    
    def _train_random_forest(self, X: np.ndarray, y: np.ndarray):
        """è®­ç»ƒéšæœºæ£®æ—"""
        from sklearn.ensemble import RandomForestClassifier
        
        self.classifier = []
        n_skills = y.shape[1]
        
        from tqdm import tqdm
        for skill_idx in tqdm(range(n_skills), desc="Random Forest"):
            y_single = y[:, skill_idx]
            
            if y_single.sum() < 5:
                self.classifier.append(None)
                continue
            
            clf = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            clf.fit(X, y_single)
            self.classifier.append(clf)
    
    def predict(self, jobs: List[Dict], threshold: float = 0.5) -> List[List[str]]:
        """
        é¢„æµ‹æŠ€èƒ½
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            threshold: é¢„æµ‹é˜ˆå€¼
            
        Returns:
            æŠ€èƒ½åˆ—è¡¨çš„åˆ—è¡¨
        """
        if self.classifier is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # æå–ç‰¹å¾
        features = []
        for job in jobs:
            feature_vec = self._extract_single_feature(job)
            features.append(feature_vec)
        
        X = np.array(features)
        
        # é¢„æµ‹
        y_pred = np.zeros((X.shape[0], len(self.skill_list)))
        
        for skill_idx, clf in enumerate(self.classifier):
            if clf is None:
                continue
            
            proba = clf.predict_proba(X)[:, 1]
            y_pred[:, skill_idx] = proba
        
        # è½¬æ¢ä¸ºæŠ€èƒ½åˆ—è¡¨
        all_skills = []
        for row in y_pred:
            skills = []
            for skill_idx, score in enumerate(row):
                if score >= threshold:
                    skills.append(self.skill_list[skill_idx])
            all_skills.append(skills)
        
        return all_skills
    
    def _evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        # é¢„æµ‹
        y_pred = np.zeros_like(y_test)
        
        for skill_idx, clf in enumerate(self.classifier):
            if clf is None:
                continue
            
            proba = clf.predict_proba(X_test)[:, 1]
            y_pred[:, skill_idx] = (proba >= 0.5).astype(float)
        
        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        precision = precision_score(y_test, y_pred, average='micro', zero_division=0)
        recall = recall_score(y_test, y_pred, average='micro', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='micro', zero_division=0)
        
        # æ ·æœ¬çº§å‡†ç¡®ç‡
        sample_accuracy = np.mean([
            len(set(np.where(y_test[i])[0]) & set(np.where(y_pred[i])[0])) /
            max(len(set(np.where(y_test[i])[0]) | set(np.where(y_pred[i])[0])), 1)
            for i in range(len(y_test))
        ])
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'sample_accuracy': sample_accuracy
        }
    
    def _print_metrics(self, metrics: Dict):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        logger.info("\nğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
        logger.info("-"*80)
        logger.info(f"  Precision (ç²¾ç¡®ç‡): {metrics['precision']:.4f}")
        logger.info(f"  Recall (å¬å›ç‡):    {metrics['recall']:.4f}")
        logger.info(f"  F1 Score:          {metrics['f1']:.4f}")
        logger.info(f"  Sample Accuracy:   {metrics['sample_accuracy']:.4f}")
        logger.info("-"*80)
    
    def save(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜åˆ†ç±»å™¨
        with open(save_path / 'classifier.pkl', 'wb') as f:
            pickle.dump(self.classifier, f)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'encoder_model': self.encoder_model,
            'classifier_type': self.classifier_type,
            'skill_list': self.skill_list,
            'skill_to_idx': self.skill_to_idx,
        }
        with open(save_path / 'metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load(self, load_path: str):
        """åŠ è½½æ¨¡å‹"""
        load_path = Path(load_path)
        
        # åŠ è½½åˆ†ç±»å™¨
        with open(load_path / 'classifier.pkl', 'rb') as f:
            self.classifier = pickle.load(f)
        
        # åŠ è½½å…ƒæ•°æ®
        with open(load_path / 'metadata.json', 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        self.encoder_model = metadata['encoder_model']
        self.classifier_type = metadata['classifier_type']
        self.skill_list = metadata['skill_list']
        self.skill_to_idx = metadata['skill_to_idx']
        
        logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½: {load_path}")
    
    # è¾…åŠ©å‡½æ•°
    def _extract_jd_text(self, job: Dict) -> str:
        """æå–JDæ–‡æœ¬"""
        if 'jd_text' in job:
            return job['jd_text'][:1000]
        
        parts = []
        if job.get('title'):
            parts.append(job['title'])
        if job.get('skills'):
            skills = job['skills']
            if isinstance(skills, list):
                parts.append(', '.join(skills[:10]))
        
        return ' '.join(parts)[:1000]
    
    def _parse_experience(self, exp_str: str) -> float:
        """è§£æç»éªŒè¦æ±‚"""
        import re
        match = re.search(r'(\d+)', exp_str)
        if match:
            return float(match.group(1))
        return 0.0
    
    def _parse_education(self, edu_str: str) -> float:
        """è§£æå­¦å†è¦æ±‚"""
        edu_map = {
            'ä¸é™': 0.0,
            'å¤§ä¸“': 0.3,
            'æœ¬ç§‘': 0.6,
            'ç¡•å£«': 0.8,
            'åšå£«': 1.0
        }
        for key, value in edu_map.items():
            if key in edu_str:
                return value
        return 0.0
    
    def _encode_city(self, city: str) -> List[float]:
        """åŸå¸‚one-hotç¼–ç """
        cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·', 'å¹¿å·', 'æˆéƒ½']
        encoding = [1.0 if city == c else 0.0 for c in cities]
        return encoding


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("çŸ¥è¯†è’¸é¦æ¨¡å— - ç¤ºä¾‹ä»£ç ")
    print("è¯·å‚è€ƒ scripts/enhance_with_qwen3.py ä¸­çš„å®Œæ•´ä½¿ç”¨ç¤ºä¾‹")
