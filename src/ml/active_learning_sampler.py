"""
ä¸»åŠ¨å­¦ä¹ é‡‡æ ·å™¨
åŸºäºèšç±»çš„æ™ºèƒ½é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºé™ä½LLMæˆæœ¬

æ ¸å¿ƒæ€è·¯:
1. å¯¹50ä¸‡JDè¿›è¡Œå‘é‡åŒ–ï¼ˆä½¿ç”¨m3e-baseï¼Œæœ¬åœ°è¿è¡Œï¼‰
2. K-Meansèšç±»ï¼Œå°†æ•°æ®åˆ†æˆNä¸ªç°‡
3. ä»æ¯ä¸ªç°‡é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬ï¼ˆæœ€é è¿‘ç°‡ä¸­å¿ƒçš„æ ·æœ¬ï¼‰
4. LLMåªå¤„ç†è¿™äº›ä»£è¡¨æ€§æ ·æœ¬ï¼ˆ1-2ä¸‡æ¡ï¼‰
5. ç”¨LLMç»“æœè®­ç»ƒè’¸é¦æ¨¡å‹ï¼Œå¤„ç†å‰©ä½™æ•°æ®

ä¼˜åŠ¿:
- æˆæœ¬é™ä½: åªéœ€å¤„ç†2-5%çš„æ•°æ®
- å‡†ç¡®ç‡ä¿æŒ: ä»£è¡¨æ€§é‡‡æ ·ä¿è¯è¦†ç›–åº¦
- å¯æ‰©å±•: æ”¯æŒå¤šç§é‡‡æ ·ç­–ç•¥

æŠ€æœ¯äº®ç‚¹ï¼ˆ2026å¹´ä¸»æµï¼‰:
- ä¸»åŠ¨å­¦ä¹  (Active Learning)
- å‘é‡åŒ– + èšç±»
- æˆæœ¬ä¼˜åŒ–
"""
import logging
import numpy as np
from typing import List, Dict, Tuple
from pathlib import Path
import sys

logger = logging.getLogger(__name__)


class ActiveLearningSampler:
    """ä¸»åŠ¨å­¦ä¹ é‡‡æ ·å™¨"""
    
    def __init__(self, embedding_model: str = "moka-ai/m3e-base"):
        """
        åˆå§‹åŒ–é‡‡æ ·å™¨
        
        Args:
            embedding_model: å‘é‡åŒ–æ¨¡å‹åç§°
        """
        logger.info("="*80)
        logger.info("ğŸ¯ åˆå§‹åŒ–ä¸»åŠ¨å­¦ä¹ é‡‡æ ·å™¨")
        logger.info("="*80)
        
        # åŠ è½½å‘é‡åŒ–æ¨¡å‹
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"â³ åŠ è½½å‘é‡åŒ–æ¨¡å‹: {embedding_model}")
            self.encoder = SentenceTransformer(embedding_model)
            logger.info("âœ… å‘é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ")
        except ImportError:
            logger.error("âŒ sentence-transformersæœªå®‰è£…")
            logger.error("è¯·è¿è¡Œ: pip install sentence-transformers")
            raise
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        self.embedding_model = embedding_model
        logger.info("âœ… é‡‡æ ·å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info("="*80)
    
    def intelligent_sample(
        self,
        jobs: List[Dict],
        target_count: int = 10000,
        strategy: str = "cluster",
        show_progress: bool = True
    ) -> Tuple[List[Dict], np.ndarray]:
        """
        æ™ºèƒ½é‡‡æ ·ï¼ˆåŸºäºèšç±»ï¼‰
        
        Args:
            jobs: æ‰€æœ‰å²—ä½æ•°æ®
            target_count: ç›®æ ‡é‡‡æ ·æ•°é‡
            strategy: é‡‡æ ·ç­–ç•¥ ("cluster"=èšç±»é‡‡æ ·, "diverse"=å¤šæ ·æ€§é‡‡æ ·)
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            (é‡‡æ ·çš„å²—ä½åˆ—è¡¨, èšç±»æ ‡ç­¾æ•°ç»„)
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ¯ å¼€å§‹æ™ºèƒ½é‡‡æ ·")
        logger.info(f"{'='*80}")
        logger.info(f"æ€»æ•°æ®é‡: {len(jobs):,} æ¡")
        logger.info(f"ç›®æ ‡é‡‡æ ·: {target_count:,} æ¡ ({target_count/len(jobs)*100:.2f}%)")
        logger.info(f"é‡‡æ ·ç­–ç•¥: {strategy}")
        logger.info("-"*80)
        
        # 1. æå–JDæ–‡æœ¬
        logger.info("ğŸ“ [1/4] æå–JDæ–‡æœ¬...")
        jd_texts = []
        for job in jobs:
            text = self._extract_jd_text(job)
            jd_texts.append(text)
        logger.info(f"âœ… æå–å®Œæˆ: {len(jd_texts)} æ¡")
        
        # 2. å‘é‡åŒ–
        logger.info("\nğŸ”¢ [2/4] å‘é‡åŒ–JDæ–‡æœ¬...")
        logger.info(f"   ä½¿ç”¨æ¨¡å‹: {self.embedding_model}")
        embeddings = self.encoder.encode(
            jd_texts,
            show_progress_bar=show_progress,
            batch_size=64,
            normalize_embeddings=True
        )
        logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ: shape={embeddings.shape}")
        
        # 3. èšç±»
        logger.info("\nğŸ² [3/4] K-Meansèšç±»...")
        
        if strategy == "cluster":
            sampled_jobs, labels = self._cluster_sampling(
                jobs, embeddings, target_count
            )
        elif strategy == "diverse":
            sampled_jobs, labels = self._diversity_sampling(
                jobs, embeddings, target_count
            )
        else:
            raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")
        
        # 4. ç»Ÿè®¡åˆ†æ
        logger.info(f"\nğŸ“Š [4/4] é‡‡æ ·ç»Ÿè®¡")
        logger.info("-"*80)
        self._print_sampling_stats(jobs, sampled_jobs, labels)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… æ™ºèƒ½é‡‡æ ·å®Œæˆï¼")
        logger.info(f"{'='*80}\n")
        
        return sampled_jobs, labels
    
    def _cluster_sampling(
        self,
        jobs: List[Dict],
        embeddings: np.ndarray,
        target_count: int
    ) -> Tuple[List[Dict], np.ndarray]:
        """åŸºäºèšç±»çš„é‡‡æ ·"""
        from sklearn.cluster import KMeans
        
        # è®¡ç®—èšç±»æ•°é‡ï¼ˆæ¯ä¸ªç°‡å¹³å‡10-20ä¸ªæ ·æœ¬ï¼‰
        n_clusters = max(target_count // 15, 100)
        n_clusters = min(n_clusters, len(jobs) // 5)  # ç¡®ä¿æ¯ä¸ªç°‡è‡³å°‘5ä¸ªæ ·æœ¬
        
        logger.info(f"   èšç±»æ•°é‡: {n_clusters}")
        logger.info(f"   ç›®æ ‡: æ¯ç°‡é‡‡æ · ~{target_count/n_clusters:.0f} ä¸ª")
        
        # K-Meansèšç±»
        kmeans = KMeans(
            n_clusters=n_clusters,
            random_state=42,
            n_init=10,
            max_iter=300
        )
        labels = kmeans.fit_predict(embeddings)
        logger.info(f"âœ… èšç±»å®Œæˆ")
        
        # ä»æ¯ä¸ªç°‡ä¸­é€‰æ‹©ä»£è¡¨æ ·æœ¬
        sampled_jobs = []
        samples_per_cluster = target_count // n_clusters + 1
        
        for cluster_id in range(n_clusters):
            # æ‰¾åˆ°å±äºè¯¥ç°‡çš„æ‰€æœ‰æ ·æœ¬
            cluster_indices = np.where(labels == cluster_id)[0]
            
            if len(cluster_indices) == 0:
                continue
            
            # è®¡ç®—åˆ°ç°‡ä¸­å¿ƒçš„è·ç¦»
            cluster_embeddings = embeddings[cluster_indices]
            cluster_center = kmeans.cluster_centers_[cluster_id]
            
            distances = np.linalg.norm(
                cluster_embeddings - cluster_center,
                axis=1
            )
            
            # é€‰æ‹©æœ€é è¿‘ç°‡ä¸­å¿ƒçš„Nä¸ªæ ·æœ¬
            n_samples = min(samples_per_cluster, len(cluster_indices))
            closest_indices = np.argsort(distances)[:n_samples]
            
            # æ·»åŠ åˆ°é‡‡æ ·åˆ—è¡¨
            for idx in closest_indices:
                original_idx = cluster_indices[idx]
                sampled_jobs.append(jobs[original_idx])
                
                if len(sampled_jobs) >= target_count:
                    break
            
            if len(sampled_jobs) >= target_count:
                break
        
        logger.info(f"âœ… é‡‡æ ·å®Œæˆ: {len(sampled_jobs)} æ¡")
        
        return sampled_jobs, labels
    
    def _diversity_sampling(
        self,
        jobs: List[Dict],
        embeddings: np.ndarray,
        target_count: int
    ) -> Tuple[List[Dict], np.ndarray]:
        """åŸºäºå¤šæ ·æ€§çš„é‡‡æ ·ï¼ˆGreedyæœ€è¿œç‚¹é‡‡æ ·ï¼‰"""
        logger.info("   ä½¿ç”¨è´ªå¿ƒæœ€è¿œç‚¹é‡‡æ ·...")
        
        sampled_indices = []
        remaining_indices = set(range(len(jobs)))
        
        # éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªç‚¹
        first_idx = np.random.randint(0, len(jobs))
        sampled_indices.append(first_idx)
        remaining_indices.remove(first_idx)
        
        # è´ªå¿ƒé€‰æ‹©æœ€è¿œç‚¹
        for _ in range(target_count - 1):
            if not remaining_indices:
                break
            
            # è®¡ç®—æ¯ä¸ªæœªé‡‡æ ·ç‚¹åˆ°å·²é‡‡æ ·ç‚¹çš„æœ€å°è·ç¦»
            remaining = list(remaining_indices)
            remaining_embeddings = embeddings[remaining]
            sampled_embeddings = embeddings[sampled_indices]
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            distances = np.linalg.norm(
                remaining_embeddings[:, np.newaxis] - sampled_embeddings,
                axis=2
            )
            min_distances = distances.min(axis=1)
            
            # é€‰æ‹©æœ€è¿œçš„ç‚¹
            farthest_idx = remaining[np.argmax(min_distances)]
            sampled_indices.append(farthest_idx)
            remaining_indices.remove(farthest_idx)
        
        sampled_jobs = [jobs[i] for i in sampled_indices]
        labels = np.zeros(len(jobs), dtype=int)  # å¤šæ ·æ€§é‡‡æ ·ä¸äº§ç”Ÿç°‡æ ‡ç­¾
        
        logger.info(f"âœ… å¤šæ ·æ€§é‡‡æ ·å®Œæˆ: {len(sampled_jobs)} æ¡")
        
        return sampled_jobs, labels
    
    def _extract_jd_text(self, job: Dict) -> str:
        """ä»å²—ä½æ•°æ®æå–JDæ–‡æœ¬"""
        # ä¼˜å…ˆä½¿ç”¨jd_text
        if 'jd_text' in job and job['jd_text']:
            return job['jd_text'][:1000]  # é™åˆ¶é•¿åº¦
        
        # å¦åˆ™æ‹¼æ¥å…¶ä»–å­—æ®µ
        parts = []
        
        if job.get('title'):
            parts.append(f"å²—ä½: {job['title']}")
        
        if job.get('skills'):
            skills = job['skills']
            if isinstance(skills, list):
                parts.append(f"æŠ€èƒ½: {', '.join(skills[:20])}")
            else:
                parts.append(f"æŠ€èƒ½: {skills}")
        
        if job.get('description'):
            parts.append(f"æè¿°: {job['description'][:500]}")
        
        if job.get('welfare'):
            welfare = job['welfare']
            if isinstance(welfare, list):
                parts.append(f"ç¦åˆ©: {', '.join(welfare[:5])}")
        
        return "\n".join(parts)[:1000]
    
    def _print_sampling_stats(
        self,
        all_jobs: List[Dict],
        sampled_jobs: List[Dict],
        labels: np.ndarray
    ):
        """æ‰“å°é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯"""
        logger.info(f"æ€»æ•°æ®é‡: {len(all_jobs):,} æ¡")
        logger.info(f"é‡‡æ ·æ•°é‡: {len(sampled_jobs):,} æ¡")
        logger.info(f"é‡‡æ ·æ¯”ä¾‹: {len(sampled_jobs)/len(all_jobs)*100:.2f}%")
        
        # åˆ†æèšç±»åˆ†å¸ƒ
        if labels.max() > 0:
            unique_labels = np.unique(labels)
            logger.info(f"èšç±»æ•°é‡: {len(unique_labels)}")
            
            # ç»Ÿè®¡æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°
            cluster_sizes = [np.sum(labels == label) for label in unique_labels]
            logger.info(f"ç°‡å¤§å°: min={min(cluster_sizes)}, "
                       f"max={max(cluster_sizes)}, "
                       f"avg={np.mean(cluster_sizes):.0f}")
        
        # åˆ†æé‡‡æ ·è¦†ç›–åº¦
        sampled_job_ids = set(j.get('job_id') for j in sampled_jobs)
        logger.info(f"å”¯ä¸€job_id: {len(sampled_job_ids)}")
    
    def stratified_sample(
        self,
        jobs: List[Dict],
        target_count: int,
        stratify_by: str = 'city'
    ) -> List[Dict]:
        """
        åˆ†å±‚é‡‡æ ·ï¼ˆæŒ‰åŸå¸‚ã€è–ªèµ„ç­‰åˆ†å±‚ï¼‰
        
        Args:
            jobs: æ‰€æœ‰å²—ä½
            target_count: ç›®æ ‡æ•°é‡
            stratify_by: åˆ†å±‚å­—æ®µ ('city', 'salary_range', etc.)
            
        Returns:
            é‡‡æ ·çš„å²—ä½åˆ—è¡¨
        """
        from collections import defaultdict
        
        logger.info(f"ğŸ“Š åˆ†å±‚é‡‡æ · (æŒ‰{stratify_by})")
        
        # åˆ†ç»„
        groups = defaultdict(list)
        for job in jobs:
            key = job.get(stratify_by, 'unknown')
            groups[key].append(job)
        
        logger.info(f"   åˆ†å±‚æ•°: {len(groups)}")
        for key, items in groups.items():
            logger.info(f"   - {key}: {len(items)} æ¡")
        
        # æŒ‰æ¯”ä¾‹é‡‡æ ·
        sampled = []
        for key, items in groups.items():
            n_samples = int(len(items) / len(jobs) * target_count)
            n_samples = max(1, min(n_samples, len(items)))
            
            import random
            sampled.extend(random.sample(items, n_samples))
        
        logger.info(f"âœ… åˆ†å±‚é‡‡æ ·å®Œæˆ: {len(sampled)} æ¡")
        
        return sampled


# æµ‹è¯•ä»£ç 
def test_active_learning_sampler():
    """æµ‹è¯•ä¸»åŠ¨å­¦ä¹ é‡‡æ ·å™¨"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•ä¸»åŠ¨å­¦ä¹ é‡‡æ ·å™¨")
    print("="*80)
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        print("\nğŸ“ åˆ›å»ºæµ‹è¯•æ•°æ®...")
        test_jobs = []
        for i in range(1000):
            test_jobs.append({
                'job_id': f'job_{i}',
                'title': f'Pythonå¼€å‘å·¥ç¨‹å¸ˆ_{i%10}',
                'skills': ['Python', 'Django', 'MySQL'],
                'jd_text': f'è´Ÿè´£åç«¯å¼€å‘ï¼Œä½¿ç”¨Pythonå’ŒDjangoæ¡†æ¶ã€‚éœ€è¦{i%5}å¹´ç»éªŒã€‚',
                'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·'][i % 4]
            })
        print(f"âœ… åˆ›å»ºå®Œæˆ: {len(test_jobs)} æ¡")
        
        # åˆå§‹åŒ–é‡‡æ ·å™¨
        sampler = ActiveLearningSampler()
        
        # æµ‹è¯•1: èšç±»é‡‡æ ·
        print(f"\n{'='*80}")
        print("ã€æµ‹è¯•1: èšç±»é‡‡æ ·ã€‘")
        print("="*80)
        
        sampled, labels = sampler.intelligent_sample(
            test_jobs,
            target_count=100,
            strategy="cluster"
        )
        print(f"âœ… é‡‡æ ·å®Œæˆ: {len(sampled)} æ¡")
        
        # æµ‹è¯•2: åˆ†å±‚é‡‡æ ·
        print(f"\n{'='*80}")
        print("ã€æµ‹è¯•2: åˆ†å±‚é‡‡æ ·ã€‘")
        print("="*80)
        
        sampled_stratified = sampler.stratified_sample(
            test_jobs,
            target_count=100,
            stratify_by='city'
        )
        print(f"âœ… åˆ†å±‚é‡‡æ ·å®Œæˆ: {len(sampled_stratified)} æ¡")
        
        print(f"\n{'='*80}")
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    test_active_learning_sampler()
