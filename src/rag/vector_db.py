"""
å‘é‡æ•°æ®åº“ç®¡ç†
åŸºäºChromaDBå®ç°å²—ä½JDçš„å‘é‡åŒ–å­˜å‚¨å’Œæ£€ç´¢
"""
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
import yaml
import logging
from pathlib import Path
from tqdm import tqdm

logger = logging.getLogger(__name__)


class VectorDB:
    """ChromaDBå‘é‡æ•°æ®åº“å°è£…"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
        """
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        
        # åŠ è½½é…ç½®ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        config_file = project_root / config_path
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        self.vector_config = config['vector_db']
        self.embedding_config = config['embedding']
        
        # åˆå§‹åŒ–ChromaDBï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        persist_dir = self.vector_config['persist_directory']
        persist_dir_abs = project_root / persist_dir
        persist_dir_abs.mkdir(parents=True, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=str(persist_dir_abs))
        
        # åŠ è½½Embeddingæ¨¡å‹
        logger.info(f"åŠ è½½Embeddingæ¨¡å‹: {self.embedding_config['model_name']}")
        
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        model_path = self.embedding_config.get('model_path')
        if model_path:
            model_path_abs = project_root / model_path
            if model_path_abs.exists():
                logger.info(f"ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {model_path_abs}")
                self.model = SentenceTransformer(str(model_path_abs))
            else:
                logger.warning(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {model_path_abs}")
                logger.info(f"ä»HuggingFaceä¸‹è½½æ¨¡å‹: {self.embedding_config['model_name']}")
                self.model = SentenceTransformer(self.embedding_config['model_name'])
        else:
            logger.info(f"ä»HuggingFaceä¸‹è½½æ¨¡å‹: {self.embedding_config['model_name']}")
            self.model = SentenceTransformer(self.embedding_config['model_name'])
        
        # åˆ›å»ºæˆ–è·å–collectionï¼ˆä½¿ç”¨cosineè·ç¦»ï¼Œç›¸ä¼¼åº¦èŒƒå›´0~1ï¼Œæ›´ç›´è§‚ï¼‰
        collection_name = self.vector_config['collection_name']
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={
                "description": "æ‹›è˜å²—ä½JDå‘é‡",
                "hnsw:space": "cosine"   # cosineè·ç¦»ï¼š1-similarityï¼Œå€¼åŸŸ[0,2]ï¼Œå¯¹åº”similarity[-1,1]
            }
        )
        
        logger.info(f"å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  Collection: {collection_name}")
        logger.info(f"  å­˜å‚¨è·¯å¾„: {persist_dir}")
        logger.info(f"  å½“å‰æ–‡æ¡£æ•°: {self.collection.count()}")
    
    def encode(self, texts: List[str], batch_size: Optional[int] = None) -> List[List[float]]:
        """
        æ–‡æœ¬å‘é‡åŒ–
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        if batch_size is None:
            batch_size = self.embedding_config.get('batch_size', 32)
        
        # ä½¿ç”¨æ¨¡å‹çš„encodeæ–¹æ³•ï¼Œè‡ªåŠ¨æ‰¹å¤„ç†
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=False,
            convert_to_tensor=False
        )
        
        return embeddings.tolist()
    
    def add_jobs(self, jobs: List[Dict], batch_size: int = 50, show_progress: bool = True):
        """
        æ‰¹é‡æ·»åŠ å²—ä½
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        """
        logger.info(f"å¼€å§‹æ·»åŠ å²—ä½åˆ°å‘é‡æ•°æ®åº“ï¼Œå…± {len(jobs)} æ¡")
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        iterator = tqdm(range(0, len(jobs), batch_size), desc="å‘é‡åŒ–") if show_progress else range(0, len(jobs), batch_size)
        
        for i in iterator:
            batch = jobs[i:i+batch_size]
            
            # æ„å»ºæ–‡æ¡£ï¼ˆç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬ï¼‰
            documents = []
            metadatas = []
            ids = []
            
            # æ‰¹æ¬¡å†…æŒ‰ job_id å»é‡ï¼Œé¿å… ChromaDB upsert æŠ¥é‡å¤ ID é”™è¯¯
            seen_ids = set()
            deduped_batch = []
            for job in batch:
                jid = job.get('job_id', '')
                if jid and jid not in seen_ids:
                    seen_ids.add(jid)
                    deduped_batch.append(job)
            batch = deduped_batch

            for job in batch:
                # åˆå¹¶å¤šä¸ªå­—æ®µä½œä¸ºæ£€ç´¢æ–‡æœ¬
                doc = self._build_document(job)
                documents.append(doc)
                
                # å…ƒæ•°æ®ï¼ˆç”¨äºè¿‡æ»¤å’Œè¿”å›ï¼‰
                skills_list = job.get('skills', [])
                metadatas.append({
                    'job_id': job['job_id'],
                    'title': job['title'],
                    'city': job.get('city', ''),
                    'company': job.get('company', ''),
                    'salary_min': str(job.get('salary_min', 0)),
                    'salary_max': str(job.get('salary_max', 0)),
                    'experience': job.get('experience', ''),
                    'education': job.get('education', ''),
                    'industry': job.get('company_industry', ''),
                    'company_size': job.get('company_size', ''),
                    'skills_count': str(len(skills_list)),
                    # å­˜å‚¨æŠ€èƒ½åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼Œæœ€å¤š15ä¸ªï¼‰ï¼Œä¾›æœç´¢ç»“æœç›´æ¥è¿”å›
                    'skills': ','.join(skills_list[:15]) if skills_list else '',
                })
                
                ids.append(job['job_id'])
            
            # å‘é‡åŒ–
            try:
                embeddings = self.encode(documents, batch_size=len(documents))
                
                # æ·»åŠ åˆ°ChromaDBï¼ˆupsertï¼šå·²å­˜åœ¨åˆ™è¦†ç›–ï¼Œä¸å­˜åœ¨åˆ™æ–°å¢ï¼Œé¿å…é‡å¤æŠ¥é”™ï¼‰
                self.collection.upsert(
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {i//batch_size} æ·»åŠ å¤±è´¥: {e}")
                continue
        
        logger.info(f"æ·»åŠ å®Œæˆï¼å½“å‰æ€»æ–‡æ¡£æ•°: {self.collection.count()}")
    
    def _build_document(self, job: Dict) -> str:
        """
        æ„å»ºç”¨äºæ£€ç´¢çš„æ–‡æ¡£æ–‡æœ¬ã€‚

        ä½¿ç”¨ç»“æ„åŒ–å­—æ®µæ‹¼å‡ºè¯­ä¹‰ä¸°å¯Œçš„æè¿°ï¼Œè®©å‘é‡ç¼–ç èƒ½åŒæ—¶æ•è·
        å²—ä½ç±»å‹ã€æŠ€èƒ½è¦æ±‚ã€åŸå¸‚ã€ç»éªŒã€è–ªèµ„ç­‰ç»´åº¦ï¼Œæå‡æœç´¢è´¨é‡ã€‚
        ä¸ä¾èµ– JD è¯¦æƒ…æ–‡æœ¬ï¼ˆè´¨é‡å‚å·®ä¸é½ï¼‰ã€‚
        """
        title    = job.get('title', '')
        skills   = job.get('skills', [])
        city     = job.get('city', '')
        company  = job.get('company', '')
        exp      = job.get('experience', '')
        edu      = job.get('education', '')
        sal_min  = job.get('salary_min', 0)
        sal_max  = job.get('salary_max', 0)

        industry = job.get('company_industry', '')
        welfare  = job.get('welfare', [])

        parts = []

        if title:
            parts.append(f"å²—ä½ï¼š{title}")

        if skills:
            # æŠ€èƒ½é‡å¤ä¸€æ¬¡å¢å¼ºæƒé‡ï¼ˆå°trickï¼Œå¯¹çŸ­æ–‡æœ¬å‘é‡åŒ–æœ‰å¸®åŠ©ï¼‰
            skills_str = ' '.join(skills)
            parts.append(f"æŠ€èƒ½è¦æ±‚ï¼š{skills_str}")
            parts.append(skills_str)   # æƒé‡åŠ å€

        if city:
            parts.append(f"å·¥ä½œåŸå¸‚ï¼š{city}")

        if exp:
            parts.append(f"å·¥ä½œç»éªŒï¼š{exp}")

        if edu:
            parts.append(f"å­¦å†è¦æ±‚ï¼š{edu}")

        if sal_min and sal_max and (int(float(sal_min)) > 0 or int(float(sal_max)) > 0):
            parts.append(f"è–ªèµ„èŒƒå›´ï¼š{sal_min}-{sal_max}K")

        if industry:
            parts.append(f"è¡Œä¸šï¼š{industry}")

        if company:
            parts.append(f"å…¬å¸ï¼š{company}")

        if welfare and isinstance(welfare, list):
            parts.append(f"ç¦åˆ©ï¼š{' '.join(welfare[:5])}")   # æœ€å¤šå–5æ¡ï¼Œé¿å…å™ªéŸ³

        return ' | '.join(filter(None, parts))
    
    def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        filters: Optional[Dict] = None
    ) -> Dict:
        """
        è¯­ä¹‰æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›TOP Kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶ï¼Œä¾‹å¦‚: {"city": "åŒ—äº¬"}
            
        Returns:
            {
                'ids': [[...]],
                'distances': [[...]],
                'metadatas': [[...]],
                'documents': [[...]]
            }
        """
        # å‘é‡åŒ–æŸ¥è¯¢
        query_embedding = self.encode([query])[0]
        
        # ChromaDB åº•å±‚ SQLite æœ‰å˜é‡æ•°é‡é™åˆ¶ï¼Œn_results ä¸èƒ½è¿‡å¤§
        # å®é™…ä¸Šå‘é‡æœç´¢è¿”å›è¶…è¿‡ 500 æ¡åç›¸ä¼¼åº¦å·²å¾ˆä½ï¼Œæ²¡æœ‰æ„ä¹‰
        MAX_RESULTS = 500
        total = self.collection.count()
        if total == 0:
            return {'ids': [[]], 'distances': [[]], 'metadatas': [[]], 'documents': [[]]}
        n = min(top_k or MAX_RESULTS, MAX_RESULTS, total)

        # æ£€ç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n,
            where=filters
        )
        
        return results
    
    def search_by_skills(
        self,
        skills: List[str],
        top_k: int = 10,
        filters: Optional[Dict] = None
    ) -> Dict:
        """
        åŸºäºæŠ€èƒ½åˆ—è¡¨æœç´¢
        
        Args:
            skills: æŠ€èƒ½åˆ—è¡¨
            top_k: è¿”å›TOP Kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æœç´¢ç»“æœ
        """
        query = "éœ€è¦ä»¥ä¸‹æŠ€èƒ½: " + ", ".join(skills)
        return self.search(query, top_k, filters)
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': self.collection.count(),
            'embedding_dim': len(self.model.encode(["test"])[0]),
            'model_name': self.embedding_config['model_name']
        }
    
    def clear(self):
        """æ¸…ç©º collectionï¼ˆåˆ é™¤åé‡å»ºï¼Œé¿å… ChromaDB get() é»˜è®¤ 100 æ¡é™åˆ¶å¯¼è‡´æœªå®Œå…¨æ¸…ç©ºï¼‰"""
        logger.warning("æ­£åœ¨æ¸…ç©ºå‘é‡æ•°æ®åº“...")
        count = self.collection.count()
        collection_name = self.vector_config['collection_name']
        self.client.delete_collection(collection_name)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={
                "description": "æ‹›è˜å²—ä½JDå‘é‡",
                "hnsw:space": "cosine"
            }
        )
        logger.info(f"å·²æ¸…ç©º {count} ä¸ªæ–‡æ¡£ï¼ˆcollection å·²é‡å»ºï¼‰")


# æµ‹è¯•ä»£ç 
def test_vector_db():
    """æµ‹è¯•å‘é‡æ•°æ®åº“"""
    import json
    
    print("="*80)
    print("æµ‹è¯•å‘é‡æ•°æ®åº“")
    print("="*80)
    
    # åˆå§‹åŒ–
    print("\nã€åˆå§‹åŒ–å‘é‡æ•°æ®åº“ã€‘")
    db = VectorDB()
    print(f"âœ… åˆå§‹åŒ–æˆåŠŸ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = db.get_stats()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ–‡æ¡£æ€»æ•°: {stats['total_documents']}")
    print(f"  å‘é‡ç»´åº¦: {stats['embedding_dim']}")
    print(f"  æ¨¡å‹: {stats['model_name']}")
    
    # å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œæ·»åŠ æµ‹è¯•æ•°æ®
    if stats['total_documents'] == 0:
        print("\nã€æ·»åŠ æµ‹è¯•æ•°æ®ã€‘")
        test_jobs = [
            {
                'job_id': 'test_001',
                'title': 'Pythonåç«¯å¼€å‘å·¥ç¨‹å¸ˆ',
                'skills': ['Python', 'Django', 'MySQL', 'Redis'],
                'city': 'åŒ—äº¬',
                'company': 'æµ‹è¯•å…¬å¸A',
                'salary_min': 15,
                'salary_max': 25,
                'jd_text': 'è´Ÿè´£åç«¯æœåŠ¡å¼€å‘ï¼Œä½¿ç”¨Pythonå’ŒDjangoæ¡†æ¶ï¼Œç†Ÿæ‚‰MySQLå’ŒRedis'
            },
            {
                'job_id': 'test_002',
                'title': 'Javaé«˜çº§å¼€å‘å·¥ç¨‹å¸ˆ',
                'skills': ['Java', 'Spring Boot', 'MySQL', 'Redis'],
                'city': 'ä¸Šæµ·',
                'company': 'æµ‹è¯•å…¬å¸B',
                'salary_min': 20,
                'salary_max': 35,
                'jd_text': 'è´Ÿè´£Javaåç«¯å¼€å‘ï¼Œç²¾é€šSpring Bootï¼Œç†Ÿæ‚‰å¾®æœåŠ¡æ¶æ„'
            }
        ]
        
        db.add_jobs(test_jobs, show_progress=False)
        print(f"âœ… æ·»åŠ äº† {len(test_jobs)} æ¡æµ‹è¯•æ•°æ®")
    
    # æµ‹è¯•æœç´¢
    print("\nã€æµ‹è¯•è¯­ä¹‰æœç´¢ã€‘")
    queries = [
        "Pythonåç«¯å¼€å‘ï¼Œç†Ÿæ‚‰Django",
        "Javaå¾®æœåŠ¡å¼€å‘",
        "å‰ç«¯Reactå¼€å‘"
    ]
    
    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = db.search(query, top_k=2)
        
        if results['metadatas'] and results['metadatas'][0]:
            print(f"æ‰¾åˆ° {len(results['metadatas'][0])} ä¸ªç»“æœ:")
            for i, meta in enumerate(results['metadatas'][0]):
                distance = results['distances'][0][i]
                similarity = 1 / (1 + max(0, distance))  # è·ç¦»è½¬ç›¸ä¼¼åº¦ï¼Œå…¼å®¹L2/cosine
                print(f"  {i+1}. {meta['title']} - {meta['city']} (ç›¸ä¼¼åº¦: {similarity:.2f})")
        else:
            print("  æœªæ‰¾åˆ°ç»“æœ")
    
    print("\n" + "="*80)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    import sys
    sys.path.append(str(Path(__file__).parent.parent.parent))
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    test_vector_db()
