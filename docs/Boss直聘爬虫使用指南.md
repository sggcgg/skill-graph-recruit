# Boss直聘爬虫 - 完整使用指南

> **最新版本：v1.2** | **更新时间：2026-01-23**

## 📋 目录

- [快速开始](#快速开始)
- [功能特点](#功能特点)
- [使用方法](#使用方法)
- [测试模式](#测试模式) 🧪
- [断点续传](#断点续传)
- [数据说明](#数据说明)
- [常见问题](#常见问题)
- [最佳实践](#最佳实践)

---

## 🚀 快速开始

### 一分钟启动

```bash
# 1. 进入项目目录
cd d:\PycharmProjects\skill-graph-recruit

# 2. 运行爬虫
python src/crawler/boss_listdata.py

# 3. 按提示操作（选择模式 → 选择城市 → 扫码登录）
```

### 简单示例（正式抓取）

```
请选择模式 (1/2/3): 1                  ← 选择单城市模式（推荐）
请选择要抓取的城市 (1-7): 1             ← 选择北京
确认开始抓取? (yes/no): yes            ← 确认
→ 扫码登录（60秒内）→ 自动开始抓取
```

**完成！** 程序会自动抓取北京的所有关键词数据（约4-5小时）。

### 快速测试（2分钟）🧪

**首次使用建议先测试：**

```
请选择模式 (1/2/3): 3                  ← 选择测试模式
选择测试城市 (1-6，默认1-北京): ↵       ← 直接回车
开始测试? (yes/no): ↵                  ← 直接回车
→ 扫码登录 → 2-3分钟后看到测试结果
```

**测试特点：** 只抓取2个关键词，只下滑5次，不保存数据，快速验证功能是否正常。

---

## ✨ 功能特点

### 1. 多城市支持

- ✅ 支持6个城市：北京、上海、广州、深圳、杭州、成都
- ✅ 99个技能关键词自动抓取
- ✅ 预计数据量：**10.7万条**（去重后）

### 2. 断点续传

- ✅ 中途中断？下次自动继续
- ✅ 进度实时保存
- ✅ 网络断开、验证码、手动停止都能恢复

### 3. 智能反检测

- ✅ 模拟真人浏览行为
- ✅ 随机延迟、变速滚动
- ✅ 鼠标移动、偶尔回看
- ✅ 降低风控风险

### 4. 数据完整

- ✅ 自动解析15+字段
- ✅ 处理空值和异常
- ✅ 自动去重
- ✅ JSON格式保存

---

## 📖 使用方法

### 方案1：分批抓取（推荐）

**优点：** 安全、可控、中断影响小

**步骤：** 每天抓取1个城市，共6天完成

| 天数 | 城市 | 选择编号 | 任务数 | 耗时 | 开始时间 | 预计完成 |
|------|------|---------|--------|------|---------|---------|
| Day 1 | 北京 | 1 | 99 | **7-8小时** | 9:00 AM | 4:00-5:00 PM |
| Day 2 | 上海 | 2 | 99 | **7-8小时** | 9:00 AM | 4:00-5:00 PM |
| Day 3 | 广州 | 3 | 99 | **7-8小时** | 9:00 AM | 4:00-5:00 PM |
| Day 4 | 深圳 | 4 | 99 | **7-8小时** | 9:00 AM | 4:00-5:00 PM |
| Day 5 | 杭州 | 5 | 99 | **7-8小时** | 9:00 AM | 4:00-5:00 PM |
| Day 6 | 成都 | 6 | 99 | **7-8小时** | 9:00 AM | 4:00-5:00 PM |

> **注**：v1.3版本下滑次数从55次增加到90次，数据更全面但耗时增加。详见 [`docs/爬虫耗时估算.md`](./爬虫耗时估算.md)

**每天操作：**

```bash
python src/crawler/boss_listdata.py
请选择模式 (1/2): 1
请选择要抓取的城市 (1-7): 1    ← Day1选1，Day2选2，依此类推
确认开始抓取? (yes/no): yes
```

### 方案2：一次抓取全部（不推荐）

```bash
python src/crawler/boss_listdata.py
请选择模式 (1/2): 2              ← 选择全部城市模式
确认开始抓取? (yes/no): yes
```

**风险：** 需连续运行20-30小时，中断损失大

---

## 🧪 测试模式

### 功能简介

测试模式允许您**快速验证爬虫功能**，抓取少量数据但不保存，用于：

- ✅ 验证爬虫是否正常工作
- ✅ 检查数据解析是否正确
- ✅ 测试网络监听是否有效
- ✅ 快速查看数据格式和统计

### 使用方法

**启动测试：**

```bash
python src/crawler/boss_listdata.py
```

**选择测试模式：**

```
抓取模式选择
======================================================================
1. 单城市模式（推荐）：每次抓取1个城市，约4-5小时
2. 全部城市模式：一次性抓取所有城市，约20-30小时
3. 🧪 测试模式：快速测试少量数据（不保存），约2-3分钟
======================================================================

请选择模式 (1/2/3): 3    ← 输入3
```

**选择测试城市：**

```
可选测试城市：
  1. 北京
  2. 上海
  3. 广州
  4. 深圳
  5. 杭州
  6. 成都

选择测试城市 (1-6，默认1-北京): ↵    ← 直接回车（默认北京）
```

**确认测试：**

```
🧪 测试配置
======================================================================
测试城市: 北京
测试关键词: Python开发, Java开发
下滑次数: 5次/关键词
预计数据: 约60-100条（不保存）
预计耗时: 2-3分钟
======================================================================

开始测试? (yes/no): ↵    ← 直接回车
```

### 测试输出示例

```
🧪 测试模式已启用
⚠️  提醒：测试数据不会保存到文件

✓ 浏览器初始化成功
请在 60 秒内完成扫码登录!

======================================================================
任务进度: 1/2 (已跳过: 0)
城市: 北京 [1/1]
关键词: Python开发 [1/2]
======================================================================
访问URL: https://www.zhipin.com/web/geek/jobs?city=101010100&query=Python开发
✓ 监听已启动
🧪 测试模式：下滑 5 次（正常模式55次）
✓ 成功捕获 3 个数据包
🧪 测试模式：已解析 42 条数据（不保存）

============================================================
数据统计 - 北京 - Python开发
============================================================
职位数量: 42
公司数量: 35
技能种类: 28
平均薪资: 25.3K
============================================================

⏰ 等待 6.72 秒后继续...

======================================================================
任务进度: 2/2 (已跳过: 0)
城市: 北京 [1/1]
关键词: Java开发 [2/2]
======================================================================
...（省略Java开发的输出）

======================================================================
🎉 测试完成！
======================================================================
提示：
  - 如果数据正常，可以使用模式1或2进行正式抓取
  - 正式抓取会保存数据到 data/raw/boss_jobs/
======================================================================
```

### 测试模式特点

| 特性 | 测试模式 | 正式模式 |
|------|---------|---------|
| **城市数量** | 1个 | 1-6个 |
| **关键词数量** | 2个（固定：Python开发、Java开发） | 99个 |
| **下滑次数** | 5次/关键词 | 55次/关键词 |
| **数据量** | 约60-100条 | 约29,700条/城市 |
| **保存数据** | ❌ 不保存 | ✅ 保存到JSON |
| **断点续传** | ❌ 禁用 | ✅ 启用 |
| **耗时** | 2-3分钟 | 4-5小时/城市 |

### 测试检查清单

测试成功的标志：

- [ ] 扫码登录成功
- [ ] 浏览器正常打开页面
- [ ] 网络监听成功捕获数据包（3-5个/关键词）
- [ ] 数据解析成功（30-50条/关键词）
- [ ] 显示数据统计（职位数、公司数、技能数、平均薪资）
- [ ] 没有报错或异常

### 使用建议

1. **首次使用**：先运行测试模式，确保功能正常
2. **调试问题**：遇到问题时使用测试模式快速定位
3. **验证修改**：修改代码后使用测试模式验证
4. **正式抓取前**：建议先测试一次，确保账号、网络正常

---

## 🔄 断点续传

### 工作原理

程序会自动保存进度到 `data/crawler_progress.json`：

```json
{
  "completed_cities": ["北京", "上海"],
  "current_city": "广州",
  "completed_keywords": ["Docker", "Kubernetes", ...],
  "last_update": "2026-01-23 15:30:45"
}
```

### 使用场景

#### 场景1：中途中断

```
第50个关键词时按 Ctrl+C 中断
→ 进度已保存（前49个）

重新运行程序
→ 自动跳过前49个
→ 从第50个继续
```

#### 场景2：网络断开

```
抓取到第30个时网络断开
→ 进度已自动保存

网络恢复后重新运行
→ 自动跳过前30个
→ 从第31个继续
```

#### 场景3：遇到验证码

```
遇到验证码，按 Ctrl+C 暂停
→ 等待4小时后重新运行
→ 自动从中断处继续
```

#### 场景4：分多天抓取

```
Day 1: 抓取北京 → 完成 ✓
Day 2: 抓取上海 → 自动跳过北京 ✓
Day 3: 抓取广州 → 自动跳过北京、上海 ✓
```

### 操作指南

**继续上次进度：**

```
检测到上次中断的进度：
  - 已完成城市: 北京
  - 当前城市: 上海
  - 已完成关键词: 49个
  
是否从上次中断的地方继续？(yes/no，输入'reset'重新开始): yes  ← 输入yes
```

**重新开始：**

```
是否从上次中断的地方继续？(yes/no，输入'reset'重新开始): reset  ← 输入reset
✓ 已重置进度，将从头开始抓取
```

### 查看进度

**方法1：查看进度文件**

```bash
cat data/crawler_progress.json
```

**方法2：统计已抓取文件**

```powershell
# Windows PowerShell
(Get-ChildItem data/raw/boss_jobs/*.json).Count
```

**方法3：Python脚本**

```python
import json

with open('data/crawler_progress.json', 'r', encoding='utf-8') as f:
    progress = json.load(f)

print(f"已完成城市: {progress['completed_cities']}")
print(f"已完成关键词: {len(progress['completed_keywords'])}/99")
```

---

## 📊 数据说明

### 数据位置

**新版存储方案（v1.3+）：按城市保存 + 自动去重**

```
data/raw/
├── boss_北京.json        # 北京所有职位数据（去重后）
├── boss_上海.json        # 上海所有职位数据（去重后）
├── boss_广州.json        # 广州所有职位数据（去重后）
├── boss_深圳.json        # 深圳所有职位数据（去重后）
├── boss_杭州.json        # 杭州所有职位数据（去重后）
└── boss_成都.json        # 成都所有职位数据（去重后）
```

**优点**：
- ✅ 一个城市一个文件，清晰明了
- ✅ 自动去重，无重复数据
- ✅ 追加模式，断点续爬安全
- ✅ 方便后续导入Neo4j

### 数据格式

每个JSON文件包含该城市所有职位（已去重）：

```json
[
  {
    "job_id": "72d68b4a2747677d03x609-1EVtR",
    "title": "Docker运维工程师",
    "company": "字节跳动",
    "city": "北京",
    "district": "海淀区",
    "business_district": "中关村",
    "salary_min": 25,
    "salary_max": 40,
    "salary_text": "25-40K",
    "experience": "3-5年",
    "education": "本科",
    "skills": ["Docker", "Kubernetes", "Linux", "Shell"],
    "company_size": "10000人以上",
    "company_industry": "互联网",
    "company_stage": "已上市",
    "boss_name": "张先生",
    "boss_title": "HRBP",
    "welfare": ["五险一金", "带薪年假", "股票期权"],
    "publish_date": "2026-01-23",
    "source": "boss直聘"
  }
]
```

### 数据量统计

| 指标 | 数值 |
|------|------|
| 城市数量 | 6个 |
| 关键词数量 | 99个 |
| 总任务数 | 594个 |
| 去重后数据量 | 约106,920条（自动去重） |
| 文件数量 | **6个JSON文件**（按城市保存） |
| 单城市数据量 | 约17,820条/城市 |
| 数据大小 | 约500MB-1GB |

---

## ❓ 常见问题

### 数据存储相关

#### Q1: 为什么改成按城市保存？（v1.3新特性）

**A:** 新版存储方案更科学：

**旧方案问题**：
- ❌ 594个文件（每个关键词一个）
- ❌ 大量重复数据（同一职位匹配多个关键词）
- ❌ 后续去重麻烦

**新方案优势**：
- ✅ 只有6个文件（每个城市一个）
- ✅ 实时自动去重
- ✅ 追加模式，断点续爬安全
- ✅ 直接导入Neo4j，无需额外处理

#### Q2: 中途断点续爬会丢数据吗？

**A:** **不会！** 新方案采用追加模式：
1. 每抓完一个关键词，立即追加到城市文件
2. 追加前自动检查去重
3. 中断后重启，自动跳过已完成的关键词
4. 已保存的数据永久保留

**示例**：
```
抓取进度：北京 - 第50个关键词时中断
→ 前49个关键词的数据已保存到 boss_北京.json
→ 重新运行后，自动跳过前49个，从第50个继续
→ 第50个的新数据会追加到文件末尾（自动去重）
```

### 测试模式相关

#### Q3: 测试数据会保存吗？

**A:** 不会！测试模式只解析数据并显示统计，不会保存到文件。

#### Q4: 测试模式可以自定义关键词吗？

**A:** 当前版本固定为 `Python开发` 和 `Java开发`。如需自定义，可以修改代码：

```python
# boss_listdata.py 约第1365行
test_keywords = ['Python开发', 'Java开发']  # 修改为您想测试的关键词
```

#### Q5: 为什么只下滑5次？

**A:** 为了快速测试。正式抓取下滑55次（约300条数据），测试只需5次（约30-50条）即可验证功能。

#### Q6: 测试失败怎么办？

**A:** 检查：
1. 网络连接是否正常
2. 是否在60秒内完成扫码登录
3. 查看日志文件：`src/crawler/logs/boss_spider_*.log`

### 正式抓取相关

#### Q5: 扫码登录超时怎么办？

**A:** 默认等待60秒，如果超时：
- 重新运行程序
- 提前准备好微信扫码

#### Q6: 中途按Ctrl+C会丢失数据吗？

**A:** 不会！
- 已抓取的数据已保存
- 进度已自动记录
- 下次运行会自动跳过已完成任务

#### Q7: 如何查看实时进度？

**A:** 3种方法：

1. **查看控制台输出**

```
任务进度: 50/99 (已跳过: 0)
城市: 北京 [1/1]
关键词: MySQL [50/99]
```

2. **查看日志文件**

```powershell
Get-Content src/crawler/logs/boss_spider_*.log -Tail 50 -Wait
```

3. **统计已抓取文件**

```powershell
(Get-ChildItem data/raw/boss_jobs/boss_北京_*.json).Count
```

#### Q8: 会被风控吗？

**A:** 已做多重优化，风控风险较低：

| 优化措施 | 说明 |
|---------|------|
| 变速滚动 | 前期快、中期慢、后期快 |
| 随机延迟 | 5-10秒任务间隔 + 每10个任务长休息 |
| 鼠标模拟 | 随机移动鼠标 |
| 偶尔回看 | 随机向上滚动 |
| 自然编码 | 让浏览器自动处理URL |

**建议：**
- 分6天抓取（每天1个城市）
- 工作日白天抓取（9:00-18:00）
- 遇到验证码立即暂停4小时

#### Q9: 数据会重复吗？

**A:** **不会！已实现实时去重**：
- ✅ 每个城市文件内基于 `job_id` 自动去重
- ✅ 追加新数据前自动检查是否已存在
- ✅ 不同关键词抓到同一职位会自动跳过
- ✅ 无需后续手动去重

#### Q10: 如何知道某个城市抓取完成？

**A:** 看到以下提示：

```
✓ 城市 北京 完成
✓ 进度已保存: 北京 - 城市完成
✓ 已释放城市 北京 的内存缓存
```

**检查数据文件：**

```powershell
# 查看城市文件是否存在
ls data/raw/boss_北京.json

# 查看数据量（应该约17,820条）
(Get-Content data/raw/boss_北京.json | ConvertFrom-Json).Count
```

#### Q11: 可以修改抓取城市吗？

**A:** 可以！修改代码中的城市配置：

```python
# boss_listdata.py 第24-31行
self.cities = {
    '北京': '101010100',
    '上海': '101020100',
    '武汉': '101200100',  # 添加新城市
    # ... 其他城市
}
```

#### Q12: 如何调整风控参数？

**A:** 修改代码中的配置（第29-34行）：

```python
# 风控配置
self.task_interval_min = 5      # 任务间最小间隔（秒）
self.task_interval_max = 10     # 任务间最大间隔（秒）
self.long_break_interval = 10   # 每N个任务长休息一次
self.long_break_min = 15        # 长休息最小时间（秒）
self.long_break_max = 25        # 长休息最大时间（秒）
```

**建议：** 保持默认值，除非频繁遇到验证码

---

## 🎯 最佳实践

### 执行计划

**建议分6天完成，降低风控风险：**

```
Day 1 (周一): 北京   - 4-5小时 ✅
Day 2 (周二): 上海   - 4-5小时 ✅
Day 3 (周三): 广州   - 4-5小时 ✅
Day 4 (周四): 深圳   - 4-5小时 ✅
Day 5 (周五): 杭州   - 4-5小时 ✅
Day 6 (周六): 成都   - 4-5小时 ✅

总计：24-30小时，分6天完成
```

### 时间建议

**最佳抓取时间：**
- ✅ 工作日白天（9:00-18:00）：模拟正常用户
- ✅ 周末下午（14:00-19:00）：访问量适中
- ⚠️ 避免深夜（00:00-06:00）：容易被识别

### 备份策略

**每天备份：**

```powershell
# 备份进度文件
Copy-Item data/crawler_progress.json data/crawler_progress_backup_20260123.json

# 备份数据目录
Compress-Archive -Path data/raw/boss_jobs/ -DestinationPath data_backup_20260123.zip
```

### 监控清单

**抓取前：**
- [ ] 关闭电脑自动休眠
- [ ] 确保网络稳定
- [ ] 手机准备好扫码
- [ ] 确认磁盘空间充足（>5GB）

**抓取中：**
- [ ] 不要操作浏览器
- [ ] 定期查看日志
- [ ] 关注验证码提示

**抓取后：**
- [ ] 检查文件数量（每个城市99个）
- [ ] 备份数据
- [ ] 查看日志是否有错误

---

## 📈 执行流程示例

### 完整示例：抓取北京

```bash
$ python src/crawler/boss_listdata.py
```

**输出：**

```
======================================================================
Boss直聘批量爬虫 - 多城市多关键词版本
======================================================================
城市数量: 6
关键词总数: 99
总任务数: 6 × 99 = 594

未找到进度文件，将从头开始抓取

======================================================================
抓取模式选择
======================================================================
1. 单城市模式（推荐）：每次抓取1个城市，约4-5小时
2. 全部城市模式：一次性抓取所有城市，约20-30小时
======================================================================

请选择模式 (1/2): 1    ← 输入1

可选城市：
  1. 北京 (101010100)
  2. 上海 (101020100)
  3. 广州 (101280100)
  4. 深圳 (101280600)
  5. 杭州 (101210100)
  6. 成都 (101270100)
  7. 全部城市（一次性抓取）

请选择要抓取的城市 (1-7): 1    ← 输入1

======================================================================
已选择：北京
======================================================================
关键词数量: 99
任务数: 99
预计数据量: 29,700 条（去重后 17,820 条）
预计耗时: 4-5 小时
======================================================================

确认开始抓取? (yes/no): yes    ← 输入yes

✓ 浏览器初始化成功
请在 60 秒内完成扫码登录!

======================================================================
任务进度: 1/99 (已跳过: 0)
城市: 北京 [1/1]
关键词: Docker [1/99]
======================================================================
访问URL: https://www.zhipin.com/web/geek/jobs?city=101010100&query=Docker
✓ 监听已启动
模拟浏览页面 3.24 秒...
开始下滑页面触发更多数据加载（共55次）...
✓ 第 1/55 次下滑完成
✓ 第 2/55 次下滑完成
...
✓ 页面下滑完成
✓ 成功捕获 15 个数据包
✓ 成功解析 285/300 个职位
✓ 数据已保存: data/raw/boss_jobs/boss_北京_Docker_20260123_150530.json
✓ 进度已保存: 北京 - Docker

============================================================
数据统计 - 北京 - Docker
============================================================
职位数量: 285
公司数量: 156
技能种类: 42
平均薪资: 32.5K
============================================================

⏰ 等待 7.35 秒后继续...

======================================================================
任务进度: 2/99 (已跳过: 0)
城市: 北京 [1/1]
关键词: Kubernetes [2/99]
======================================================================
...

（4-5小时后）

======================================================================
任务进度: 99/99 (已跳过: 0)
城市: 北京 [1/1]
关键词: 性能测试 [99/99]
======================================================================
...
✓ 城市 北京 完成
✓ 进度已保存: 北京 - 城市完成

======================================================================
✓✓✓ 所有任务处理完成！✓✓✓
  - 处理城市: 1个
  - 处理关键词: 99个
  - 总任务数: 99个
  - 实际执行: 99个
======================================================================
✓ 所有任务完成，进度文件已清除
```

---

## 🔧 高级用法

### 禁用断点续传

如果不需要断点续传功能，修改代码第38行：

```python
self.enable_resume = False  # 禁用断点续传
```

### 手动编辑进度

如果需要跳过某个损坏的任务：

```json
{
  "completed_cities": ["北京"],
  "current_city": "上海",
  "completed_keywords": [
    "Docker",
    "Kubernetes",
    "MySQL"  // 添加这个会跳过MySQL
  ],
  "last_update": "2026-01-23 15:30:45"
}
```

### 清除特定城市进度

如果某个城市数据有问题，想重新抓取：

```python
import json

# 读取进度
with open('data/crawler_progress.json', 'r', encoding='utf-8') as f:
    progress = json.load(f)

# 移除北京
if '北京' in progress['completed_cities']:
    progress['completed_cities'].remove('北京')

# 保存
with open('data/crawler_progress.json', 'w', encoding='utf-8') as f:
    json.dump(progress, f, ensure_ascii=False, indent=2)
```

---

## 📝 下一步

数据抓取完成后，按以下顺序进行：

1. **数据清洗** → `python src/preprocessor/data_cleaner.py`
2. **技能提取** → `python src/graph_builder/skill_extractor.py`
3. **导入Neo4j** → `python src/graph_builder/graph_constructor.py`
4. **启动API** → `python src/api/main.py`

详细步骤见 [`docs/完整实施指南.md`](./完整实施指南.md)

---

## 📞 问题反馈

如遇到问题，请提供：
1. 日志文件（`src/crawler/logs/*.log`）
2. 错误截图
3. 运行环境（操作系统、Python版本）

---

**祝您抓取顺利！🎉**

**版本：** v1.2  
**更新时间：** 2026-01-23
