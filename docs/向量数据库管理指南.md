# 📦 向量数据库管理指南

> **更新时间**: 2026年2月11日  
> **目标**: 管理9万初始数据 + 30万增量数据

---

## 🎯 场景说明

您的项目数据规模：
- ✅ **已完成**: 9万条数据（已清洗）
- 🔜 **计划**: 再抓取30万条数据
- 📊 **总计**: 约40万条招聘数据

**核心问题**：如何高效管理向量数据库的初始化和增量更新？

---

## 📋 两种方案对比

| 对比项 | 方案A：全量重建 | 方案B：增量更新 ⭐ |
|--------|-----------------|-------------------|
| **脚本** | `init_vector_db.py` | `update_vector_db.py` |
| **操作** | 清空并重新创建 | 只添加新数据 |
| **耗时** | 40分钟（40万条） | 30分钟（30万新数据） |
| **风险** | 清空已有数据 | 保留已有数据 |
| **适用场景** | 首次初始化、模型更换 | 数据增量更新 |
| **推荐度** | ⭐⭐ | ⭐⭐⭐⭐⭐ |

**结论**：✅ **推荐使用方案B（增量更新）**

---

## 🚀 完整操作流程

### 阶段1：初始化（9万条数据）✅ 已完成

```bash
# 首次初始化向量数据库
python scripts/init_vector_db.py
```

**预期输出**：
```
✅ 向量数据库初始化完成！
📊 统计信息:
  总文档数: 69,205
  向量维度: 768
  模型: moka-ai/m3e-base
```

---

### 阶段2：增量更新（30万条新数据）🔜 待执行

#### 步骤1：抓取新数据

```bash
# 继续抓取数据（例如新城市、新岗位）
python src/crawler/boss_listdata.py --city 武汉
python src/crawler/boss_listdata.py --city 西安
# ... 抓取更多数据
```

新数据会保存到：
- `data/raw/boss_武汉_raw.json`
- `data/raw/boss_西安_raw.json`
- ...

#### 步骤2：清洗新数据

```bash
# 清洗新抓取的数据
python src/data_processing/data_cleaner.py
```

清洗后数据在：
- `data/cleaned/boss_武汉_cleaned.json`
- `data/cleaned/boss_西安_cleaned.json`
- ...

#### 步骤3：（可选）LLM增强

```bash
# 对新数据进行LLM增强
python scripts/enhance_with_qwen3.py
```

增强后数据在：
- `data/enhanced/boss_武汉_enhanced.json`
- `data/enhanced/boss_西安_enhanced.json`
- ...

#### 步骤4：增量更新向量数据库 ⭐ 核心步骤

```bash
# 增量添加清洗数据（推荐）
python scripts/update_vector_db.py

# 或者增量添加LLM增强数据（效果更好）
python scripts/update_vector_db.py --source enhanced
```

**预期输出**：
```
📦 增量更新向量数据库
============================================================

【步骤1: 连接向量数据库】
✅ 数据库连接成功
  当前文档数: 69,205

【步骤2: 加载数据】
找到 8 个数据文件
总计加载 300,000 条岗位数据

【步骤3: 检查重复】
数据库中已有 69,205 条数据
总数据: 300,000 条
已存在: 0 条
新数据: 300,000 条

将添加 300,000 条新数据
预计耗时: 300.0 分钟
预计占用空间: 3000.0 MB

确认继续？(y/n): y

【步骤4: 向量化并添加】
向量化: 100%|██████████| 6000/6000 [30:00<00:00, 3.33it/s]

✅ 向量数据库更新完成！
📊 更新统计:
  更新前: 69,205 条
  更新后: 369,205 条
  新增: 300,000 条
```

---

## 🔧 高级用法

### 用法1：只更新特定数据

如果只想添加某个城市的数据：

```bash
# 方法1：移动其他文件到临时目录
mkdir data/cleaned/temp
mv data/cleaned/boss_北京_cleaned.json data/cleaned/temp/
mv data/cleaned/boss_上海_cleaned.json data/cleaned/temp/

# 只保留要添加的文件
ls data/cleaned/  # 只有 boss_武汉_cleaned.json

# 执行增量更新
python scripts/update_vector_db.py

# 恢复文件
mv data/cleaned/temp/* data/cleaned/
rmdir data/cleaned/temp
```

### 用法2：强制更新已有数据

如果修改了现有数据，需要重新向量化：

```bash
# 强制更新所有数据（包括已有数据）
python scripts/update_vector_db.py --force-update
```

### 用法3：完全重建（谨慎使用）

如果更换了Embedding模型，需要重建：

```bash
# 清空并重建整个数据库
python scripts/init_vector_db.py --force
```

---

## 📊 数据去重机制

**自动去重**：
- ✅ 基于 `job_id` 自动去重
- ✅ 如果 `job_id` 已存在，会**更新**而不是重复添加
- ✅ 默认行为：跳过已存在的数据（`--skip-duplicates`）

**手动控制**：
```bash
# 跳过重复数据（默认）
python scripts/update_vector_db.py

# 不跳过重复数据（会更新已有数据）
python scripts/update_vector_db.py --no-skip-duplicates

# 强制更新所有数据
python scripts/update_vector_db.py --force-update
```

---

## 🎯 推荐的完整工作流程

### 首次运行（9万条）

```bash
# 1. 抓取数据
python src/crawler/boss_listdata.py

# 2. 清洗数据
python src/data_processing/data_cleaner.py

# 3. （可选）LLM增强
python scripts/enhance_with_qwen3.py

# 4. 初始化向量数据库
python scripts/init_vector_db.py

# 5. 初始化Neo4j图数据库
python scripts/reimport_neo4j.py
```

### 后续更新（30万条）

```bash
# 1. 抓取新数据
python src/crawler/boss_listdata.py --city 武汉
python src/crawler/boss_listdata.py --city 西安
# ... 更多城市

# 2. 清洗新数据
python src/data_processing/data_cleaner.py

# 3. （可选）LLM增强新数据
python scripts/enhance_with_qwen3.py

# 4. 增量更新向量数据库 ⭐
python scripts/update_vector_db.py --source enhanced

# 5. 增量更新Neo4j（需要修改 reimport_neo4j.py 支持增量）
python scripts/reimport_neo4j.py  # 目前会清空，需要改进
```

---

## ⚠️ 注意事项

### 1. Neo4j增量更新问题

**当前问题**：
- ❌ `scripts/reimport_neo4j.py` 目前会清空数据库
- ❌ 不支持增量添加

**解决方案**（待实现）：
- 需要创建 `scripts/update_neo4j.py`
- 类似向量数据库的增量逻辑
- 基于 `job_id` 去重

### 2. 内存管理

处理30万数据需要注意：
- 建议分批处理（每次5-10万）
- 监控内存使用：`htop` 或 `nvidia-smi`
- 如果内存不足，降低 `batch_size`

```python
# 在 config.yaml 中调整
vector_batch_size: 50  # 默认50，内存不足可降低到32
```

### 3. 磁盘空间

估算空间需求：
- 9万条数据：约900MB
- 30万条数据：约3GB
- 总计：约4GB

确保 `data/vector_db/` 目录有足够空间。

### 4. job_id 唯一性

**关键**：确保每条数据的 `job_id` 是唯一的！

检查方法：
```python
import json

with open('data/cleaned/boss_上海_cleaned.json', 'r', encoding='utf-8') as f:
    jobs = json.load(f)

job_ids = [j['job_id'] for j in jobs]
duplicates = len(job_ids) - len(set(job_ids))

print(f"总数据: {len(job_ids)}")
print(f"唯一ID: {len(set(job_ids))}")
print(f"重复: {duplicates}")
```

---

## 🔍 故障排查

### 问题1：内存不足

**症状**：
```
MemoryError: Unable to allocate array
```

**解决**：
```bash
# 1. 降低batch_size
python scripts/update_vector_db.py  # 修改脚本中的batch_size=50 → 32

# 2. 分批处理
# 每次只处理部分文件
```

### 问题2：重复数据

**症状**：
```
新增: 0 条
```

**原因**：所有数据已存在

**解决**：
```bash
# 强制更新
python scripts/update_vector_db.py --force-update
```

### 问题3：模型未下载

**症状**：
```
OSError: Can't load model for 'moka-ai/m3e-base'
```

**解决**：
```bash
# 下载模型
python scripts/download_m3e_model.py
```

---

## 📈 性能优化建议

### 1. 使用GPU加速

如果有GPU，在 `config.yaml` 中配置：

```yaml
embedding:
  model_name: "moka-ai/m3e-base"
  device: "cuda"  # 使用GPU，速度提升5-10倍
  batch_size: 128  # GPU可以用更大batch
```

### 2. 并行处理

对于超大数据量，可以并行处理多个城市：

```bash
# 终端1
python scripts/update_vector_db.py &  # 处理前半部分

# 终端2
python scripts/update_vector_db.py &  # 处理后半部分
```

### 3. 定期清理

删除不再需要的旧数据：

```python
# 删除特定城市的数据
from src.rag.vector_db import VectorDB

db = VectorDB()
# 注意：ChromaDB暂不支持按metadata批量删除
# 需要逐条删除或重建
```

---

## 📝 脚本对比总结

| 脚本 | 用途 | 何时使用 | 耗时估算 |
|------|------|---------|---------|
| `init_vector_db.py` | 初始化 | 首次运行、模型更换 | 约0.1min/千条 |
| `update_vector_db.py` | 增量更新 | 添加新数据 ⭐ | 约0.1min/千条 |

**推荐流程**：
1. 首次：`init_vector_db.py` → 9万条
2. 后续：`update_vector_db.py` → 每次新抓取数据后运行

---

## ✅ 检查清单

**在运行增量更新前，确保：**

- [ ] ✅ 已完成首次初始化（`init_vector_db.py`）
- [ ] ✅ 新数据已清洗（`data/cleaned/` 中有新文件）
- [ ] ✅ （可选）新数据已LLM增强（`data/enhanced/`）
- [ ] ✅ 磁盘空间充足（至少3GB）
- [ ] ✅ 内存充足（至少8GB可用）
- [ ] ✅ 每条数据有唯一的 `job_id`

**运行后验证：**

- [ ] ✅ 检查文档总数是否正确
- [ ] ✅ 测试搜索功能
- [ ] ✅ 检查 `data/vector_db/` 目录大小

---

## 🎉 总结

**您的场景最佳方案**：

```bash
# 当前（9万条）
python scripts/init_vector_db.py  # ✅ 已完成

# 未来（30万条新数据）
python scripts/update_vector_db.py  # 🔜 使用这个！
```

**核心优势**：
- ✅ 不需要重新处理9万条旧数据
- ✅ 只处理30万条新数据
- ✅ 节省约40%的时间（12分钟 vs 40分钟）
- ✅ 自动去重，数据安全

---

**文档更新时间**：2026年2月11日  
**适用版本**：ChromaDB 0.4+, Sentence-Transformers 2.2+
